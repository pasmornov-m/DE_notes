# QUESTIONS

---

## **1. Хранилища данных (DWH, Data Lake, архитектура)**

* [Какие подходы построения хранилища данных тебе известны?](#подходы-построения-хранилища-данных)
* [Чем Data Lake отличается от DWH?](#чем-data-lake-отличается-от-dwh)
* [В чем разница подходов Кимболла и Инмона?](#разница-между-подходами-кимболла-и-инмона)
* [Расскажи, что знаешь про SCD?](#scd-slowly-changing-dimensions)
* [Как сформировать процесс SCD2 для вставки, изменения, удаления?](#как-реализовать-scd-type-2-на-примере-клиентов)
* [Как правильно работать с таблицами, если 1я — это просто справочник (например, пользователи) построенный по SCD2, а 2я — это покупки пользователей, и необходимо найти все покупки пользователя с актуальными данными на день покупки.](#как-правильно-работать-с-таблицами-если-1я--это-просто-справочник-например-пользователи-построенный-по-scd2-а-2я--это-покупки-пользователей-и-необходимо-найти-все-покупки-пользователя-с-актуальными-данными-на-день-покупки)
* [Расскажи какие слои есть в хранилище данных?](#слои-в-хранилище-данных)
* [Чем в хранилище ODS слой отличается от DDS слоя?](#слои-в-хранилище-данных)
* [Чем колоночные БД отличаются от строковых?](#чем-колоночные-бд-отличаются-от-строковых)

---

## **2. HDFS, Hadoop, Spark, Hive, Oozie, YARN**

* Что такое HDFS? Из чего она состоит и как работает?
* Какие проблемы бывают с HDFS?
* Что такое перекос данных?
* Как работает HSFD? Для чего нужна NameNode, Secondary NameNode? Нам необходимо считать текстовый файл из HDFS, объясни, что будет происходить?
* Что такое фактор репликации в HDFS и для чего он нужен?
* Что такое Hadoop и из каких компонентов он состоит?
* Что такое YARN?
* Для чего нужен Apache Oozie?
* Что такое Hive и объясни, как он работает с данными?
* Что такое партиционирование и что оно из себя представляет в Hadoop?
* Что такое Spark? Для чего она нужна?
* Что делает Shuffle в Spark? Между чем передаются данные?
* Объясни парадигму MapReduce и почему Spark пришел ей на замену?
* Какие виды exchange (motion) в Spark?
* Как передать UDF?
* Расскажите про Job stage task в Spark.
* Что такое catalyst?

---

## **3. ClickHouse**

* Какие движки ClickHouse вы знаете?
* Какие особенности у движка ReplicatingMergeTree?
* Представь у тебя есть PGSQL и ClickHouse, как бы ты загружал данные из PGSQL в ClickHouse?
* Тебе необходимо из источника отправлять данные в нейронку каждые 10 минут, после чего результат записывать в ClickHouse, как ты это сделаешь? Опиши весь процесс.
* Как создать распределенную таблицу в Clickhouse?
* Какие движки в ClickHouse ты знаешь? И в чем их различия?
* Как оптимизировать запросы в Clickhouse?
* Как в Clickhouse устроена операция UPDATE?

---

## **4. Greenplum**

* В чем различие между GreenPlum и HDFS?
* Как происходит оптимизация запросов в GreenPlum?
* Для каких целей преднозначен Clickhouse и GreenPlum?

---

## **5. SQL, оконные функции, индексы, joins, СТЕ и др.**

* [Что такое оконные функции?](#что-такое-оконные-функции)
* [Как задать границы окна?](#как-задать-границы-окна)
* [В чем будет разница вывода, если я напишу агрегирующую оконную функцию по сумме с сортировкой и без неё?](#в-чем-будет-разница-вывода-если-я-напишу-агрегирующую-оконную-функцию-по-сумме-с-сортировкой-и-без-неё)
* [Чем отличаются оконные функции от агрегирующих в SQL?](#чем-отличаются-оконные-функции-от-агрегирующих-в-sql)
* [Можно ли использовать несколько агрегационных функций в select?](#можно-ли-использовать-несколько-агрегационных-функций-в-select)
* [Чем отличается DENSE_RANK от RANK?](#чем-отличается-dense_rank-от-rank)
* [Представим что DENSE_RANK не существует, как сделать её функционал с помощью ROW_NUMBER?](#как-получить-dense_rank-если-он-недоступен-вариант-через-distinct--row_number)
* [Как RANK() работает с NULL?](#как-rank-работает-с-null)
* [У вас есть поле с datetime, а вам надо сделать фильтр по дате без учета времени - перечислите возможные способы решения проблемы.](#у-вас-есть-поле-с-datetime-а-вам-надо-сделать-фильтр-по-дате-без-учета-времени---перечислите-возможные-способы-решения-проблемы)
* [Перечислите логические и физические джойны и алгоритмическую сложность физических.](#перечислите-логические-и-физические-джойны-и-алгоритмическую-сложность-физических)
* [Что делает утилита PGTune?](#что-делает-утилита-pgtune)
* [Что такое нормализация?](#что-такое-нормализация)
* [Какие типы индексов бывают?](#какие-типы-индексов-бывают)
* [Чем отличается кластеризованный индекс от некластеризованного?](#чем-отличается-кластеризованный-индекс-от-некластеризованного)
* [Сколько у таблицы может быть кластеризованных индексов?](#отличия-поведение-и-последствия)
* [Чем отличаются типы данных JSON и JSONb?](#чем-отличаются-типы-данных-json-и-jsonb)
* [Можно ли строить индекс по JSON полям?](#можно-ли-строить-индекс-по-json-полям)
* [Есть ли ограничения на создание партицированной таблицы?](#есть-ли-ограничения-на-создание-партицированной-таблицы)
* [Чем отличаются материализованное и нематериализованное представления?](#чем-отличаются-материализованное-и-нематериализованное-представления)
* [Можно ли читать данные из материализованного представления, когда выполняется команда REFRESH?](#можно-ли-читать-данные-из-материализованного-представления-когда-выполняется-команда-refresh)
* [Как удалить дубликаты из таблицы?](#как-удалить-дубликаты-из-таблицы)
* [Какой объявить СТЕ? Можно ли в одной таблице применить несколько СТЕ?](#какой-объявить-сте-можно-ли-в-одной-таблице-применить-несколько-сте)
* [Как оптимизируется запрос?](#как-оптимизируется-запрос)
* [Что будете делать, если в плане запроса увидели Nested Loop?](#что-будете-делать-если-в-плане-запроса-увидели-nested-loop)

---

## **6. PostgreSQL (PGSQL)**

* Как выдаются права доступа в PostgreSQL?
* Как устроена система транзакций в PSQL?
* Какие блокировки существуют?

---

## **7. Apache NiFi**

* Какие процессоры использовали в NiFi?
* Настраивали ли схемы, если да, то в каких модулях?
* Как считать данные из каталога?
* Зачем при считывании CSV файлов данные переводили в AVRO формат?
* В случае сбоя одного сервера с NiFi — как его перезапустить?
* Чем Атрибут отличается от Контекста?
* Теряет ли данные NiFi, если произошел сбой программы?
* Расскажи о логировании в NiFi?
* Какой модуль в NiFi используется для JOLT преобразований?
* NiFi работает в кластере и считываем данные из Kafka, один из серверов сгорает, и мы теряем данные. Как повторно обработать потерянные данные?

---

## **8. Apache Airflow**

* Таска в AirFlow упала с ошибкой, как сделать так, чтобы несмотря на ошибку, следующая таска запустилась?
* Как в AirFlow в зависимости от условия, продолжить обработку по нужной ветке ДАГа?
* Что такое Dataset в Airflow?
* Что представляет из себя Sensor в Airflow?
* Как передавать данные между задачами в Airflow? (ответа xcom не достаточно)

---

## **9. Python: функции, декораторы, ООП и др.**

* [Лямбда функция (что это, зачем, где использовать)](#лямбда-функция-что-это-зачем-где-использовать)
* [В чем разница "==" и "is"?](#в-чем-разница--и-is)
* [В чем разница между `func` и `func()`?](#в-чем-разница-между-func-и-func)
* [Назовите изменяемые и неизменяемые объекты (типы).](#назовите-изменяемые-и-неизменяемые-объекты-типы)
* [Декораторы (что, зачем нужно, как влияет на структуру) + написать свой пример](#декораторы-что-зачем-нужно-как-влияет-на-структуру--написать-свой-пример)
* [Можно ли на одну функцию нацепить несколько декораторов и как они будут считываться?](#можно-ли-на-одну-функцию-нацепить-несколько-декораторов-и-как-они-будут-считываться)
* [Что такое декоратор Шредингера?](#что-такое-декоратор-шредингера)
* [Генератор (что, зачем нужно) + написать свой пример](#генератор-что-зачем-нужно--написать-свой-пример)
* [Как рассчитывается сложность алгоритма? на примере list, tuple](#как-рассчитывается-сложность-алгоритма-на-примере-list-tuple)
* [Как передаются аргументы в функцию?](#как-передаются-аргументы-в-функцию)
* [Функция, которая используется в качестве аргумента, может использовать свои аргументы?](#функция-которая-используется-в-качестве-аргумента-может-использовать-свои-аргументы)
* [Зачем прописывать тип входящих или выходящих данных в функцию?](#зачем-прописывать-тип-входящих-или-выходящих-данных-в-функцию)
* [Какая типизация используется в Python?](#зачем-прописывать-тип-входящих-или-выходящих-данных-в-функцию)
* [Что представляет из себя тип данных Int в Python?](#что-представляет-из-себя-тип-данных-int-в-python)
* [Можно ли в функции Python в качестве аргумента использовать функцию? Если да, то как называется такая функция?](#можно-ли-в-функции-python-в-качестве-аргумента-использовать-функцию-если-да-то-как-называется-такая-функция)
* [Назовите парадигмы ООП?](#назовите-парадигмы-ооп)
* [Self (что это, для чего нужен, как и где использовать)](#self-что-это-для-чего-нужен-как-и-где-использовать)
* [Что такое super() и зачем нужен?](#что-такое-super-и-зачем-нужен)
* [Расскажи порядок разрешения методов?](#расскажи-порядок-разрешения-методов)
* [Что такое class methods / static methods?](#что-такое-class-methods--static-methods)
* [Что такое итерация?](#что-такое-итерация)
* [Какие типы данных могут быть ключами словаря?](#какие-типы-данных-могут-быть-ключами-словаря)
* [Может ли изменяться порядок ключей в словаре?](#может-ли-изменяться-порядок-ключей-в-словаре)
* [Какая алгоритмическая сложность у получения значения по ключу из словаря?](#какая-алгоритмическая-сложность-у-получения-значения-по-ключу-из-словаря)
* [Кортеж может быть ключом словаря?](#кортеж-может-быть-ключом-словаря)
* [Какие магические методы должны быть реализованы в в классе, чтоб его можно было использовать в качестве ключа словаря?](#какие-магические-методы-должны-быть-реализованы-в-в-классе-чтоб-его-можно-было-использовать-в-качестве-ключа-словаря)
* [Что такое контекстный менеджер?](#что-такое-контекстный-менеджер)
* [Как реализовать контекстный менеджер? Если ответите через класс, то попросят назвать и другие варианты.](#что-такое-контекстный-менеджер)
* [Что такое GIL?](#что-такое-gil)
* [Чем модуль отличается от пакета?](#чем-модуль-отличается-от-пакета)

---

# ANSWERS

---

**1. Хранилища данных (DWH, Data Lake, архитектура)**

---

## Подходы построения хранилища данных

### 1. Top-down (по Инмону)

#### Суть:

* Подход предполагает **централизованную архитектуру**, при которой в первую очередь создаётся **Enterprise Data Warehouse (EDW)** — единое корпоративное хранилище данных.
* Это хранилище содержит **нормализованные** данные (обычно 3NF), структурированные по предметным областям.

#### Особенности:

* EDW служит **единственным источником правды (Single Source of Truth)**.
* Из EDW создаются **витрины данных (Data Marts)**, уже в денормализованной форме — под нужды конкретных бизнес-пользователей или аналитиков.
* Основной акцент — **качество, консистентность, историчность**.

#### Плюсы:

* Централизованное управление метаданными и качеством данных.
* Удобно масштабировать и сопровождать на уровне всей организации.

#### Минусы:

* Высокая **стоимость и длительность внедрения**.
* Сложность в адаптации к изменяющимся бизнес-требованиям.

---

### 2. Bottom-up (по Кимболлу)

#### Суть:

* Построение хранилища начинается **с витрин данных (Data Marts)**, создаваемых для отдельных бизнес-процессов.
* Позже эти витрины объединяются в **логическое DWH**.

#### Особенности:

* Используется **денормализованная схема**, чаще всего **звезда** (star schema) или **снежинка** (snowflake).
* Данные моделируются вокруг **факт-таблиц** и **измерений**.
* Популярен благодаря своей **простоте и быстрому Time-to-Market**.

#### Плюсы:

* Быстрое получение бизнес-результатов.
* Относительно просто обучить и подключить конечных пользователей.
* Хорошая производительность при аналитических запросах.

#### Минусы:

* При масштабировании и объединении множества витрин возможно **дублирование логики**, **расхождение метрик**.
* Нет единого централизованного источника правды.

---

### 3. Data Vault

#### Суть:

* **Гибридный подход**, сочетающий достоинства Инмона и Кимболла.
* Разделяет структуру на **Hub (ключи сущностей)**, **Link (связи между сущностями)** и **Satellite (атрибуты, историчность)**.

#### Особенности:

* Поддерживает **историчность**, **аудит**, **многоверсионность**.
* Хорошо подходит для **Agile и DevOps** сред.
* Логика бизнес-преобразования вынесена за пределы core-структуры — в **Data Marts**.

#### Плюсы:

* Легко масштабируется и адаптируется под изменения схем источников.
* Строго отделяет бизнес-логику от данных.
* Хорошо подходит для **Big Data и распределённых систем**.

#### Минусы:

* Более **сложная модель**, требует грамотной ETL-реализации.
* Сложность в прямой аналитике без промежуточной агрегации.

---

### 4. Data Lake

#### Суть:

* Хранилище **сырого или полуобработанного** контента в виде файлов, таблиц, изображений и пр.
* Как правило, работает на базе **объектного хранилища**: S3, HDFS, Azure Blob.

#### Особенности:

* Используется в основном для **Big Data** и **Data Science** задач.
* Структура данных может быть **semi-structured** или **unstructured** (JSON, Parquet, Avro и пр.).
* Отложенная обработка (ELT, а не ETL).

#### Плюсы:

* Дешёвое масштабируемое хранилище.
* Гибкость в использовании — можно применять машинное обучение, потоковую обработку и пр.
* Подходит для хранения **огромных объёмов** разнотипных данных.

#### Минусы:

* Отсутствие схемы ведёт к **хаосу и "data swamp"**, если не настроены правила и метаданные.
* Сложнее обеспечить консистентность и управление качеством данных.

---

### 5. Lakehouse

#### Суть:

* Современный гибрид **Data Lake + Data Warehouse**.
* Использует движки вроде **Delta Lake**, **Apache Iceberg**, **Apache Hudi**, которые дают поддержку **ACID**, **time-travel**, **схем**, **индексов** и пр. поверх Data Lake.

#### Особенности:

* Хранение осуществляется в файловой системе, но с возможностями реляционной обработки.
* Работает с теми же инструментами, что и Data Lake (Spark, Presto, Dremio и др.).

#### Плюсы:

* Объединяет гибкость Data Lake с управляемостью DWH.
* Позволяет строить BI-отчёты и Data Science на одних и тех же данных.
* Хорошая производительность и контроль данных.

#### Минусы:

* Пока что менее зрелая технология, требует интеграции нескольких компонентов.
* Не всегда просто настраивается без облачных платформ (Databricks, Snowflake).

---

### 6. Lambda и Kappa архитектуры

#### Lambda Architecture:

* Объединяет **batch processing** (например, Hadoop/Spark) и **stream processing** (Kafka/Storm/Flink).
* Данные сначала обрабатываются в реальном времени (speed layer), а потом — партиями для точности (batch layer).
* Используется при требовании **быстрых и точных данных одновременно**.

#### Kappa Architecture:

* Упрощённая архитектура, в которой **все данные обрабатываются как поток**.
* Нет разделения на batch и stream — единый pipeline.

#### Плюсы:

* Lambda: высокая точность и скорость.
* Kappa: простота архитектуры, лучше подходит для событийных систем.

#### Минусы:

* Lambda: высокая сложность поддержки двух параллельных путей обработки.
* Kappa: сложнее корректировать ошибки в исторических данных.

---

**Заключение**:

Инмон и Кимболл — классические для BI, Data Vault — для гибкости и историчности, Data Lake и Lakehouse — для современных Big Data и ML-задач.

---

## Чем Data Lake отличается от DWH?

### 1. **Тип хранимых данных**

* **DWH (Data Warehouse):**

  * Хранит **структурированные данные** из различных источников, которые предварительно очищаются и трансформируются.
  * Используются реляционные базы данных, таблицы с жёстко заданными схемами.

* **Data Lake:**

  * Может хранить **любой тип данных**: структурированные (таблицы), полуструктурированные (JSON, XML), неструктурированные (изображения, видео, логи).
  * Данные загружаются "как есть", без строгой предварительной обработки (raw format).

---

### 2. **Схема и структура хранения**

* **DWH:**

  * Использует подход **schema-on-write** — данные приводятся к чёткой структуре **до** записи в хранилище.
  * Модель хранения разрабатывается заранее (звезда, снежинка, 3NF).

* **Data Lake:**

  * Использует подход **schema-on-read** — данные приводятся к нужной структуре **только во время чтения**.
  * Возможна работа с данными без заранее заданной схемы.

---

### 3. **Назначение**

* **DWH:**

  * Предназначено для **аналитики и бизнес-отчётности**.
  * Чётко определённые источники данных, высокая точность и надёжность.

* **Data Lake:**

  * Используется для **анализа больших объёмов разнородных данных**, в том числе для **Data Science**, **машинного обучения**, **потоковой обработки**.
  * Часто служит как единое хранилище "сырых" данных.

---

### 4. **Процесс загрузки данных**

* **DWH:**

  * Применяется классическая **ETL (Extract → Transform → Load)** схема: сначала данные очищаются и трансформируются, потом загружаются в хранилище.
* **Data Lake:**

  * Применяется **ELT (Extract → Load → Transform)**: данные сначала загружаются в lake, а потом обрабатываются по мере необходимости.

---

### 5. **Хранилище и технологии**

* **DWH:**

  * Обычно реализовано на **реляционных базах данных** (PostgreSQL, Oracle, Greenplum, Snowflake, MS SQL).
  * Поддерживает SQL-запросы и индексацию.

* **Data Lake:**

  * Строится на **объектных хранилищах** (Amazon S3, HDFS, Azure Blob).
  * Поддерживает работу с файлами (Parquet, ORC, Avro) и распределённую обработку (Spark, Presto, Flink).

---

### 6. **Гибкость и масштабируемость**

* **DWH:**

  * Ограничен типом данных и объёмами. Масштабируемость требует вертикального роста (более мощное железо).
  * Высокие требования к качеству и консистентности.

* **Data Lake:**

  * Гибок, масштабируется горизонтально. Подходит для хранения **петабайтов данных**.
  * Часто используется в **облачных инфраструктурах**.

---

### 7. **Стоимость**

* **DWH:**

  * Дороже в разработке и сопровождении, так как требует проектирования схем, ETL-процессов, инфраструктуры.

* **Data Lake:**

  * Относительно дешевле, особенно при использовании облачных решений. Не требует сложной подготовки данных перед загрузкой.


Оба подхода могут использоваться **совместно**, например: данные сначала собираются в Data Lake, затем после очистки и агрегации загружаются в DWH для бизнес-анализа. Такой гибридный подход особенно популярен в больших компаниях.

---

## Разница между подходами Кимболла и Инмона

Заключается в архитектуре построения хранилищ данных, способе организации данных, приоритетах и применении нормализации. Ниже подробно раскрыты основные различия между двумя подходами.

---

### 1. **Общий подход к построению хранилища**

**Инмон**:

* Считается «отцом корпоративного хранилища данных (EDW)».
* Подход **top-down** — сначала проектируется **централизованное хранилище**, потом на его основе строятся витрины данных для отдельных бизнес-подразделений.
* Основное внимание уделяется **централизованности и согласованности** данных.

**Кимболл**:

* Подход **bottom-up** — сначала создаются **витрины данных (Data Marts)** под конкретные бизнес-задачи, которые затем объединяются в единое логическое хранилище.
* Основной приоритет — **быстрое удовлетворение потребностей бизнеса**, простота реализации.

---

### 2. **Моделирование данных**

**Инмон**:

* Используется **нормализованная структура**, чаще всего **третья нормальная форма (3NF)**.
* Данные хранятся в виде **предметно-ориентированных таблиц**, связанных друг с другом через ключи.
* Цель — устранение избыточности, повышение целостности данных.

**Кимболл**:

* Используются **денормализованные структуры**, в частности **звёздная схема (star schema)** или **снежинка (snowflake schema)**.
* Основные сущности: **факт-таблицы** (события, числовые показатели) и **измерения** (атрибуты сущностей).
* Цель — обеспечить удобство и производительность аналитических запросов.

---

### 3. **Порядок загрузки и обработки данных**

**Инмон**:

* Используется классическая схема **ETL (Extract – Transform – Load)**.
* Данные сначала приводятся к строгой структуре, очищаются, нормализуются, а затем загружаются в хранилище.
* Требует тщательной подготовки и согласования схем.

**Кимболл**:

* Может использовать **ETL** или **ELT**, но основная задача — быстро и удобно представить данные бизнес-пользователям.
* Трансформации выполняются так, чтобы обеспечить удобство построения отчётов и анализа.

---

### 4. **Инфраструктура и масштабируемость**

**Инмон**:

* Предполагает создание **единого корпоративного хранилища**, которое служит универсальным источником данных для всех подразделений.
* Хорошо подходит для **больших организаций** с высокими требованиями к качеству данных и контролю.

**Кимболл**:

* Строится из **наборов независимых витрин**, каждая из которых разрабатывается быстро под конкретную задачу.
* Легче начать внедрение в условиях ограниченных ресурсов или при необходимости быстрого результата.

---

### 5. **Гибкость и сопровождение**

**Инмон**:

* **Менее гибкий**: изменение структуры требует серьёзных доработок в централизованной модели.
* Зато обеспечивает **долгосрочную устойчивость**, согласованность и прозрачность структуры данных.

**Кимболл**:

* **Более гибкий**: легко создавать и модифицировать витрины, адаптируя под новые задачи.
* При масштабировании и объединении витрин может возникнуть **дублирование данных и логики**.

---

### 6. **Пользователи и цели**

**Инмон**:

* Ориентирован на **ИТ-отделы и архитекторов**, строится с учётом стратегических целей и корпоративных требований.
* Часто используется в системах, где важна **аудитность, безопасность, соответствие нормативам**.

**Кимболл**:

* Ориентирован на **бизнес-пользователей и аналитиков**, которым нужны простые отчёты и быстрая аналитика.
* Хорошо подходит для **BI-инструментов** и визуализации данных.

---

### Заключение

Подход Инмона подходит для создания масштабируемой, надёжной архитектуры, с акцентом на качество и контроль данных. Подход Кимболла — для быстрого внедрения аналитики и адаптивности к изменяющимся требованиям бизнеса.

На практике часто применяются **гибридные подходы**, где корпоративное хранилище проектируется по Инмону, а витрины — по Кимболлу. Это позволяет сочетать централизованность с удобством анализа.

---

## SCD (Slowly Changing Dimensions)

### Что такое SCD?

**SCD (медленно изменяющиеся измерения)** — это справочные таблицы (например, клиенты, продукты, сотрудники), данные в которых изменяются **редко**, но такие изменения необходимо **хранить** и учитывать в аналитике. Например, клиент сменил адрес или должность, и важно понимать, как его поведение или метрики менялись до и после этого события.

---

### Основные типы SCD

#### **SCD Type 0 — неизменяемые измерения**

* Данные в таблице **никогда не изменяются** после загрузки.
* Используется для атрибутов, которые не должны пересматриваться (например, дата рождения).
* Любое изменение источника игнорируется.

**Применение:** паспортные данные, пол, дата рождения.

---

#### **SCD Type 1 — перезапись (overwrite)**

* Изменения **перезаписываются**: старое значение теряется, хранится только актуальное.
* Простой в реализации, но не даёт возможности проанализировать прошлое значение.

**Пример:** клиент сменил город — в таблице просто обновляется значение поля `city`.

**Плюсы:** простая реализация, экономит место.
**Минусы:** невозможно восстановить историю изменений.

---

#### **SCD Type 2 — хранение полной истории изменений**

* Каждое изменение приводит к **созданию новой строки** в таблице.
* Используются специальные поля:

  * `valid_from` / `valid_to` (период действия записи),
  * `is_current` (флаг актуальности),
  * `version` (опционально).

**Пример:** при смене адреса у клиента будет две строки: одна с прошлым адресом, другая — с новым, и только одна из них будет помечена как текущая.

**Плюсы:** можно проводить анализ в ретроспективе, отслеживать, когда и какие изменения происходили.
**Минусы:** большее потребление памяти, усложнённая логика работы.

---

#### **SCD Type 3 — хранение части истории**

* Хранится только **одно предыдущее значение** вместе с текущим (например, `previous_city`, `current_city`).
* Подходит, когда не требуется глубокая история изменений.

**Пример:** для клиента можно хранить текущий и предыдущий адрес, но не более.

**Плюсы:** экономия места.
**Минусы:** ограниченная аналитическая ценность, невозможно отследить более двух состояний.

---

### Другие типы (используемые реже):

#### SCD Type 4 — журнал изменений (Change History Table)

* История хранится **в отдельной таблице**, а основная справочная содержит только актуальные данные.
* Используется для минимизации нагрузки на основную таблицу и для удобной работы с историей.

#### SCD Type 6 — гибрид 1+2+3

* Сочетает сразу несколько стратегий:

  * перезапись текущих значений (как Type 1),
  * хранение истории в виде новых строк (как Type 2),
  * сохранение предыдущего значения в колонке (как Type 3).
* Применяется в системах с высокими требованиями к аналитике.

---

### Зачем нужен SCD?

* Для корректного анализа данных во времени.
* Чтобы избежать искажений в отчётах при изменении справочной информации.
* Для соответствия требованиям аудита и нормативного учёта (например, в банковской или медицинской сфере).

---

### Технические аспекты:

* Важно использовать **бизнес-ключи**, а не surrogate-ключи (primary key), для определения изменений.
* Для Type 2 часто добавляют поля:

  * `surrogate_key` (уникальный ID строки),
  * `valid_from`, `valid_to`,
  * `version`,
  * `is_current`.


---

### Как реализовать SCD Type 2 (на примере клиентов):

Обработка включает три сценария: **вставка новой записи**, **изменение существующей записи** и **обработка удаления**.

#### 1. **Вставка новой записи (insert)**

Если в целевой таблице нет строки с таким `business_key` (например, `customer_id`):

* Вставить новую строку с:

  * `valid_from = now()`
  * `valid_to = NULL`
  * `is_current = true`
  * другими полями, соответствующими входным данным.

---

#### 2. **Изменение существующей записи (update)**

Если запись с `business_key` существует и флаг `is_current = true`, нужно проверить: изменились ли **отслеживаемые** атрибуты (например, адрес, должность и т.д.):

* **Если атрибуты изменились:**

  * Завершить старую запись:

    * `valid_to = now()`
    * `is_current = false`
  * Вставить новую строку:

    * `valid_from = now()`
    * `valid_to = NULL`
    * `is_current = true`
    * с обновлёнными значениями полей.

* **Если изменений нет:**

  * Ничего не делать (данные актуальны, история не нарушается).

---

#### 3. **Удаление записи (delete / логическое удаление)**

Удаления в SCD2 обычно не означают физическое удаление строки, а оформляются как **завершение действия записи**:

* Найти текущую активную запись (`is_current = true`);
* Обновить:

  * `valid_to = now()`
  * `is_current = false`
* (Опционально) добавить логическое поле `is_deleted = true` в новую запись, если требуется отражение удаления.

**Важно:** Если необходимо вести учёт «удалённых» записей (например, клиент ушёл), можно создать новую строку с тем же `business_key`, но с флагом `is_deleted = true`, чтобы сохранялась история.

---

### Как правильно работать с таблицами, если 1я — это просто справочник (например, пользователи) построенный по SCD2, а 2я — это покупки пользователей, и необходимо найти все покупки пользователя с актуальными данными на день покупки.

#### Условия задачи

* **Таблица `users_scd2`** — справочник пользователей, ведётся по SCD Type 2, содержит:

  * `user_id` — бизнес-ключ (natural key);
  * `valid_from` и `valid_to` — диапазон действия версии;
  * `is_current` — флаг актуальности;
  * другие атрибуты (например, `user_city`, `user_status` и т.д.).

* **Таблица `purchases`** — факт-покупки:

  * `user_id` — внешний ключ на пользователя;
  * `purchase_date` — дата покупки;
  * другие атрибуты (сумма, товар и т.д.).

---

#### Цель

Найти все покупки пользователей, при этом — к каждой покупке прикрепить актуальную на **дату покупки** версию пользователя из таблицы `users_scd2`.

---

#### Правильный подход

Для этого выполняется **темпоральное соединение (range join)** по следующему условию:

```sql
purchases.user_id = users_scd2.user_id
AND purchases.purchase_date >= users_scd2.valid_from
AND (purchases.purchase_date < users_scd2.valid_to OR users_scd2.valid_to IS NULL)
```

---

## Пример SQL-запроса

```sql
SELECT
    p.purchase_id,
    p.user_id,
    p.purchase_date,
    p.amount,
    u.user_city,
    u.user_status
FROM
    purchases p
JOIN
    users_scd2 u
    ON p.user_id = u.user_id
   AND p.purchase_date >= u.valid_from
   AND (p.purchase_date < u.valid_to OR u.valid_to IS NULL);
```

---

## Объяснение условий

* `p.user_id = u.user_id` — обычное соединение по бизнес-ключу;
* `p.purchase_date >= u.valid_from` — дата покупки должна быть позже начала действия версии;
* `p.purchase_date < u.valid_to OR u.valid_to IS NULL` — покупка произошла до конца действия версии, или же версия ещё актуальна (в этом случае `valid_to` = `NULL`).

Таким образом, к каждой покупке будет привязана именно **та версия пользователя**, которая была действующей в момент совершения этой покупки.

---

## Слои в хранилище данных

Хранилище данных (Data Warehouse, DWH) строится по **многоуровневой архитектуре**, где каждый слой выполняет свою специфическую задачу и служит промежуточным этапом обработки данных. Это позволяет обеспечить надёжность, масштабируемость и упрощает сопровождение системы.

---

### 1. **Staging Area (или Raw/Buffer Layer)**

**Назначение:** временное хранилище "сырых" данных, загружаемых из источников.
**Характеристики:**

* Данные поступают в том виде, в котором они есть в источниках (CRM, ERP, API и др.);
* Минимальная или отсутствующая обработка;
* Часто хранится только за короткий период времени;
* Используется для анализа отклонений, аудита и повторной загрузки данных при сбоях.

**Примеры данных:** полные выгрузки из таблиц, лог-файлы, JSON от API, бинарные события и т.п.

---

### 2. **ODS (Operational Data Store)**

**ODS (Операционное хранилище данных)** — это слой, предназначенный для хранения операционных (транзакционных) данных, собранных из различных источников. Этот слой обычно:

**Назначение:**

* Быстрое получение оперативной информации (почти в реальном времени);
* Агрегация и нормализация данных;
* Источник для построения витрин или отчётов в текущем моменте времени.

**Характеристики:**

* Структура ближе к нормализованной форме (3NF);
* Обычно хранится только актуальная информация (без истории);
* Может использоваться в операционных отчётах и дэшбордах.

**Пример:** таблица клиентов из разных систем объединяется по бизнес-ключу, чтобы сформировать единое представление "Клиенты".

---

### 3. **DDS (Data Distribution Store / Data Delivery Store / Data Data Store)**

**DDS** — это аналитический слой хранилища, часто служащий **ядром корпоративного DWH**, и включает в себя **измерения (dimensions)** и **факты (facts)**.

**Назначение:**

* Хранение **историзированных** и **обогащённых** данных;
* Формирование единого аналитического контекста;
* Источник для построения витрин данных, BI-отчётов, аналитических панелей.

**Характеристики:**

* Чаще всего денормализованная структура (звезда или снежинка);
* Используются методы SCD (slowly changing dimensions) для ведения истории;
* Обрабатываются бизнес-правила, трансформации, расчёты.

**Подсистемы DDS:**

* **Факт-таблицы (Fact Tables):** содержат метрики, суммы, счета, транзакции и пр.;
* **Измерения (Dimension Tables):** описательные справочники — клиенты, продукты, время, регионы.

---

### 4. **Data Marts (Витрины данных)**

**Назначение:**

* Предоставление данных для конкретных бизнес-пользователей или подразделений;
* Оптимизированы под отчётность и аналитические запросы.

**Характеристики:**

* Могут быть логическими (в рамках DDS) или физическими (отдельные базы/таблицы);
* Настроены под конкретные задачи: отчёт по продажам, маркетинг, логистика;
* Часто имеют упрощённую модель и агрегированные данные.

---

### 5. **Presentation Layer (слой представления)**

**Назначение:**

* Предоставление доступа к данным конечным пользователям, BI-системам, API и другим внешним компонентам.

**Формы представления:**

* SQL-витрины (views, materialized views);
* OLAP-кубы (например, с использованием ClickHouse, SSAS, Vertica и т.д.);
* Доступ через BI-инструменты (Power BI, Tableau, Looker и пр.);
* REST API или GraphQL интерфейсы для интеграции с другими системами.

---

### 6. **Metadata Layer (слой метаданных)**

**Назначение:**

* Хранение описания данных: источники, типы, владельцы, трансформации, история изменений.

**Используется для:**

* Каталогизации и управления качеством данных;
* Поддержки lineage и data governance;
* Интеграции с Data Catalog системами (например, Apache Atlas, DataHub).

---

### Сводная схема слоёв DWH

```plaintext
[ Источники данных ]
        │
        ▼
[ Staging Area (Raw) ]
        │
        ▼
[ ODS — нормализованные и очищенные данные ]
        │
        ▼
[ DDS — аналитическое ядро (факты + измерения) ]
        │
        ▼
[ Data Marts / BI витрины ]
        │
        ▼
[ Presentation Layer — отчёты, BI, API ]
```

---

### Заключение

Хранилище данных строится по многоуровневой архитектуре, где каждый слой играет важную роль:

* **Staging** — буферизация и приём данных;
* **ODS** — нормализация и консолидация оперативной информации;
* **DDS** — ядро аналитики с историей изменений и бизнес-логикой;
* **Data Marts** — ориентация на конкретных потребителей данных;
* **Presentation** — доступ к данным в удобной форме.

---

## Чем колоночные БД отличаются от строковых?

### 1. Физическое хранение данных

* **Строковые базы данных** (row-oriented) хранят данные построчно.
  Каждая строка таблицы записывается целиком подряд, то есть все значения всех столбцов для одной строки находятся вместе.

* **Колоночные базы данных** (column-oriented) хранят данные поколоночно.
  Значения каждого столбца записываются отдельно друг от друга, то есть все значения одного столбца находятся вместе, потом идут значения следующего столбца и так далее.

---

### 2. Как это влияет на операции чтения и записи

#### Строковые базы

* Оптимальны для операций, когда нужны **все данные одной строки**. Например, при обработке транзакций, когда читается или обновляется целая запись.
* При запросе к одному или нескольким столбцам приходится читать всю строку полностью, включая ненужные данные.

#### Колоночные базы

* Оптимальны для аналитических запросов, когда требуется обработать **ограниченное число столбцов, но много строк**. Например, агрегатные функции, фильтрация по одному столбцу.
* Чтение только нужных столбцов уменьшает объем данных, которые необходимо обработать, ускоряет запросы.

---

### 3. Сжатие данных

* В колонковых базах данные одного столбца имеют однородный тип и схожие значения, что облегчает эффективное сжатие. Это уменьшает объем хранимых данных и ускоряет ввод-вывод.
* В строковых базах сжатие менее эффективно, так как в строке смешиваются разные типы данных.

---

### 4. Производительность запросов

* Колоночные базы показывают высокую производительность при выполнении запросов типа OLAP (аналитика), агрегации, построении отчетов.
* Строковые базы лучше подходят для OLTP (оперативной обработки транзакций), где важна скорость вставки, обновления и выборки отдельных записей.

---

### 5. Пример систем

* **Строковые СУБД:** PostgreSQL, MySQL, Oracle, SQL Server — классические реляционные СУБД.
* **Колоночные СУБД:** ClickHouse, Amazon Redshift, Google BigQuery, Apache Cassandra (частично), Vertica — ориентированы на аналитику и хранение больших объемов данных.

---

### 6. Применение

* **Строковые БД** — системы учета, CRM, банковские приложения, где данные часто обновляются и важна целостность строк.
* **Колоночные БД** — системы бизнес-аналитики, хранилища данных, где обрабатываются большие объемы и нужны быстрые агрегаты.

---

### 7. Ограничения

* Колоночные базы обычно хуже подходят для частых операций вставки и обновления, так как данные распределены по столбцам, и обновление может требовать изменения нескольких мест.
* Строковые базы менее эффективны для обработки большого количества аналитических запросов по отдельным столбцам.

---

### Итог

**Основное отличие** — способ хранения данных:

* Строковые базы хранят строки целиком, удобны для транзакционных операций.
* Колоночные базы хранят данные по столбцам, эффективны для аналитических запросов и работы с большими объемами данных.

---

**2. HDFS, Hadoop, Spark, Hive, Oozie, YARN**

---

**5. SQL, оконные функции, индексы, joins, СТЕ и др.**

---

## Что такое оконные функции?

**Оконные функции** в PostgreSQL — это функции, которые позволяют выполнять вычисления над набором строк, связанных с текущей строкой, без группировки данных в одну строку (как это делает `GROUP BY`).
В отличие от агрегатных функций, оконные функции **не сворачивают** набор строк, а добавляют результат вычисления как дополнительный столбец к каждой строке.

---

### Основная идея

Оконная функция «смотрит» на определённое **окно строк** (window frame), которое задаётся с помощью предложения `OVER (...)`.
Это окно определяется с помощью:

1. **PARTITION BY** — делит результат на группы (разделы, партиции).
2. **ORDER BY** — задаёт порядок обработки строк внутри каждой группы.
3. **ROWS / RANGE / GROUPS** — уточняет границы окна относительно текущей строки.

---

### Примеры оконных функций

#### 1. Агрегаты как оконные функции

Можно использовать привычные агрегатные функции (`SUM`, `AVG`, `COUNT`, `MIN`, `MAX`) в оконном контексте.

```sql
SELECT
    department,
    employee,
    salary,
    SUM(salary) OVER (PARTITION BY department) AS total_salary_in_dept
FROM employees;
```

**Что делает:**
Для каждой строки считает сумму `salary` по всем сотрудникам в том же `department`, не сворачивая строки в одну.

---

#### 2. Ранжирующие функции

* `ROW_NUMBER()` — порядковый номер строки в окне.
* `RANK()` — присваивает ранги с пропусками при совпадениях.
* `DENSE_RANK()` — присваивает ранги без пропусков.
* `NTILE(n)` — разбивает строки на `n` групп.

```sql
SELECT
    department,
    employee,
    salary,
    RANK() OVER (PARTITION BY department ORDER BY salary DESC) AS rank_in_dept
FROM employees;
```

---

#### 3. Функции смещения

* `LAG(column, offset)` — значение из предыдущей строки.
* `LEAD(column, offset)` — значение из следующей строки.
* `FIRST_VALUE(column)` — первое значение в окне.
* `LAST_VALUE(column)` — последнее значение в окне.
* `NTH_VALUE(column, n)` — n-ное значение в окне.

```sql
SELECT
    employee,
    salary,
    LAG(salary, 1) OVER (ORDER BY salary) AS prev_salary,
    LEAD(salary, 1) OVER (ORDER BY salary) AS next_salary
FROM employees;
```

---

### Пример с `ROWS BETWEEN`

```sql
SELECT
    employee,
    salary,
    AVG(salary) OVER (
        ORDER BY salary
        ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING
    ) AS moving_avg
FROM employees;
```

**Что делает:**
Считает среднюю зарплату для текущей строки, включая одну предыдущую и одну следующую.

---

### Отличие от агрегатных функций

| Характеристика          | Агрегатные функции       | Оконные функции                            |
| ----------------------- | ------------------------ | ------------------------------------------ |
| Кол-во строк на выходе  | Меньше или равно входным | Ровно столько же, сколько входных          |
| `GROUP BY`              | Требуется                | Не требуется                               |
| Гибкость работы с окном | Нет                      | Есть (`PARTITION`, `ORDER`, `ROWS` и т.д.) |

---

## Как задать границы окна?

Ниже — подробное объяснение того, **как задать границы окна** в оконной функции PostgreSQL: синтаксис, типы фреймов, семантика границ, поведение по умолчанию, практические примеры и рекомендации.

---

### 1. Общая структура окна

Оконная функция записывается как `функция() OVER (...)`. Внутри `OVER` можно задать:

* `PARTITION BY ...` — разделение на группы (партиции);
* `ORDER BY ...` — порядок в каждой партиции;
* опциональную **clause фрейма**: `ROWS`, `RANGE` или `GROUPS` + границы.

Синтаксис:

```sql
<window_func> OVER (
    [ PARTITION BY expr [, ...] ]
    [ ORDER BY expr [ ASC | DESC ] [, ...] ]
    [ { ROWS | RANGE | GROUPS }
      BETWEEN <frame_start> AND <frame_end>
    ]
)
```

---

### 2. Типы фреймов и их семантика

#### ROWS

Интерпретирует границы **в терминах строк (физических смещений)** относительно текущей строки. Каждая позиция — это конкретная строка (включая саму текущую).

* `ROWS BETWEEN 2 PRECEDING AND CURRENT ROW` — текущая строка и две предыдущие строки.
* `ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING` — фиксированное скользящее окно из трёх строк (предыдущая, текущая, следующая).

**Поведение:** детерминировано: `CURRENT ROW` — только текущая строка; предшествование/следование — ровно N строк.

#### RANGE

Интерпретирует границы **на основе значений ORDER BY** (логически). Включаются все строки, у которых значение выражения `ORDER BY` попадает в указанный диапазон относительно текущей строки. Для `CURRENT ROW` и при наличии равных значений (`peers`) RANGE включает **все peer-строки** (с тем же значением ORDER BY).

* `RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW` — от начала партиции до всех peer-строк текущей строки.
* `RANGE BETWEEN INTERVAL '7 days' PRECEDING AND CURRENT ROW` — для `ORDER BY date` включает строки с датой в интервале `[current_date - 7d, current_date]`.

**Поведение:** полезен для value-based окон (например, «последние 7 дней»). Для нечисловых/несовместимых типов некоторые варианты PRECEDING/FOLLOWING запрещены.

#### GROUPS

Работает на уровне **групп peer-строк** (группы, у которых одинаковые значения ORDER BY). Граница `1 PRECEDING` — это предыдущая группа peer-строк, и т.д.

**Полезно**, когда нужно оперировать «группами равных ORDER BY значений», а не отдельными строками.

---

### 3. Варианты границ (frame boundaries)

Границы можно задавать следующими способами:

* `UNBOUNDED PRECEDING` — от начала партиции.
* `UNBOUNDED FOLLOWING` — до конца партиции.
* `CURRENT ROW` — текущая строка (ролевая трактовка зависит от `ROWS/RANGE/GROUPS`).
* `n PRECEDING` — n строк (для `ROWS`) или n групп (для `GROUPS`), либо значение/интервал (для `RANGE`, если тип ORDER BY позволяет).
* `n FOLLOWING` — симметрично.

Примеры границ:

```sql
ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
ROWS BETWEEN 2 PRECEDING AND 1 FOLLOWING
RANGE BETWEEN INTERVAL '7 days' PRECEDING AND CURRENT ROW
GROUPS BETWEEN 1 PRECEDING AND 1 FOLLOWING
```

---

### 4. Семантика `CURRENT ROW` в разных режимах

* `ROWS CURRENT ROW` — только физическая текущая строка.
* `RANGE CURRENT ROW` — **все peer-строки**, у которых значение(я) в `ORDER BY` равно текущему значению(ям).
* `GROUPS CURRENT ROW` — целая группа peer-строк, рассматриваемая как единица.

---

### 5. Поведение по умолчанию

* Если **`ORDER BY` отсутствует** в `OVER(...)`, то фрейм по умолчанию — **вся партиция**: эквивалент `ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING`. Результат агрегата одинаков для всех строк партиции.
* Если **`ORDER BY` есть**, но **фрейм не указан**, PostgreSQL (и стандарт SQL) использует по умолчанию:
  `RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW`.
  Это приводит к поведению «накопления» (running total), причём `RANGE` учитывает всех peer-строк текущей строки.

Рекомендуется **явно указывать** `ROWS`/`RANGE` при необходимости детерминированного поведения, чтобы избежать подвохов с peer-строками.

---

### 6. Практические примеры

#### 6.1. Накопительная сумма (running total) по дате

```sql
SELECT
  account_id,
  tx_date,
  amount,
  SUM(amount) OVER (
    PARTITION BY account_id
    ORDER BY tx_date
    ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
  ) AS running_total
FROM transactions;
```

Здесь используем `ROWS` чтобы включать только физические предшествующие строки — детерминированное поведение.

#### 6.2. Скользящая средняя по 3 последним строкам

```sql
AVG(value) OVER (
  ORDER BY ts
  ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
) AS moving_avg_3
```

#### 6.3. Сумма за последние 7 дней (value-based)

```sql
SUM(sales) OVER (
  ORDER BY sale_date
  RANGE BETWEEN INTERVAL '6 days' PRECEDING AND CURRENT ROW
) AS sum_last_7_days
```

`RANGE` учитывает временной интервал относительно значения `sale_date`.

#### 6.4. Ранжирование и учёт равных значений (peers)

```sql
RANK() OVER (PARTITION BY dept ORDER BY salary DESC)
```

`RANK()` и `DENSE_RANK()` не требуют явного фрейма — они основаны на `ORDER BY` и peer-группах.

---

### 7. Отличия `ROWS` vs `RANGE` (важно понимать)

* `ROWS` — смещение по количеству строк; предсказуемо для фиксированных размеров окон.
* `RANGE` — смещение по значению ORDER BY; при наличии равных значений (`peers`) включаются все строки с тем же ORDER BY-значением.
* `RANGE` удобен для «диапазонов по значению» (напр., последние N единиц времени), `ROWS` — для «N предыдущих записей».

Пример с одинаковыми `ORDER BY` значениями:

* С `ORDER BY score` и двумя строками с `score = 100`:

  * `ROWS BETWEEN 1 PRECEDING AND CURRENT ROW` для второй из этих двух строк включает именно предыдущую строку (одну).
  * `RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW` включает **обе** строки (peer-строки).

---

### 8. Производительность и рекомендации

* `ROWS` обычно быстрее и проще для оптимизатора, особенно для небольших фиксированных окон.
* `RANGE` может быть дороже, потому что требует сравнения значений и обработки peer-групп; для интервалов по времени может потребоваться дополнительная сортировка/сканирование.
* Всегда **явно указывайте фрейм**, если вам важно точное поведение (особенно отличия `CURRENT ROW`).
* Если вы хотите фиксированное количество предыдущих записей — используйте `ROWS`.
* Если хотите «последние X единиц времени» — используйте `RANGE` с `INTERVAL`.

---

### 9. Частые ошибки/подводные камни

* Ожидание, что `RANGE` с `1 PRECEDING` означает «одну строку» — неверно; это «значение ORDER BY минус 1» (для числовых), а `CURRENT ROW` включает всех peers.
* Опускание `ORDER BY` при использовании PRECEDING/FOLLOWING не имеет смысла — PostgreSQL требует ORDER BY для таких смещений.
* При использовании `RANGE` с нечисловыми типами будьте внимательны: допустимость `n PRECEDING` зависит от типа ORDER BY (для дат — `INTERVAL`).

---

### 10. Короткая шпаргалка синтаксиса

```sql
-- От начала партиции до текущей строки
... OVER (PARTITION BY p ORDER BY o ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)

-- Фиксированное скользящее окно N строк
... OVER (ORDER BY ts ROWS BETWEEN 2 PRECEDING AND 1 FOLLOWING)

-- Value-based окно (например, последние 7 дней)
... OVER (ORDER BY created_at RANGE BETWEEN INTERVAL '6 days' PRECEDING AND CURRENT ROW)

-- Вся партиция (без ORDER BY)
... OVER (PARTITION BY p)
-- эквивалент ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING
```

---

## В чем будет разница вывода, если я напишу агрегирующую оконную функцию по сумме с сортировкой и без неё?

* **Без `ORDER BY`** (`SUM(...) OVER (PARTITION BY ...)` или просто `SUM(...) OVER ()`) окно по умолчанию — вся партиция (эквивалент `ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING`). В результате вы получите **одно и то же значение суммы для каждой строки партиции** (итог по партиции / по всей таблице).
* **С `ORDER BY`** (`SUM(...) OVER (PARTITION BY ... ORDER BY ...)`) поведение другое: по стандарту SQL (и в PostgreSQL) при наличии `ORDER BY` по умолчанию используется фрейм `RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW`. Это даёт **накопительную (running) сумму** — для каждой строки суммируются значения всех строк с порядковым (ORDER BY) значением меньше или равным текущему (с учётом peer-групп при `RANGE`).

Далее — подробно с иллюстрациями, нюансами и рекомендациями.

---

### Пример данных

```sql
CREATE TABLE sales(id int, sale_date date, amount int);

-- данные
id | sale_date  | amount
1  | 2024-01-01 | 10
2  | 2024-01-02 | 20
3  | 2024-01-02 |  5
4  | 2024-01-03 | 15
```

#### 1) Без `ORDER BY`

```sql
SELECT id, sale_date, amount,
       SUM(amount) OVER () AS total_all
FROM sales;
```

Результат:

```
id | sale_date  | amount | total_all
1  | 2024-01-01 | 10     | 50
2  | 2024-01-02 | 20     | 50
3  | 2024-01-02 | 5      | 50
4  | 2024-01-03 | 15     | 50
```

Здесь для каждой строки — общая сумма по всем строкам (вся таблица), т.к. окно — полная партиция.

Если добавить `PARTITION BY customer_id`, это будет итог по каждому customer.

### 2) С `ORDER BY` (по умолчанию `RANGE ... CURRENT ROW`)

```sql
SELECT id, sale_date, amount,
       SUM(amount) OVER (ORDER BY sale_date) AS running_sum_range
FROM sales;
```

Поскольку `ORDER BY sale_date` и по умолчанию используется `RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW`, для строк с одинаковой `sale_date` (peer-строки) `RANGE` включает **все** peer-строки. Поэтому результат:

```
id | sale_date  | amount | running_sum_range
1  | 2024-01-01 | 10     | 10
2  | 2024-01-02 | 20     | 35   -- здесь включены обе строки с 2024-01-02 (20+5) + 10
3  | 2024-01-02 | 5      | 35   -- та же сумма, т.к. это peer той же даты
4  | 2024-01-03 | 15     | 50
```

Если бы вы хотели, чтобы второму ряду с 2024-01-02 соответствовала сумма только предыдущих строк и собственной строки (включая предыдущую строку с 20, но не одновременно оба peer), нужно использовать `ROWS`:

```sql
SELECT id, sale_date, amount,
       SUM(amount) OVER (
         ORDER BY sale_date
         ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
       ) AS running_sum_rows
FROM sales;
```

Результат `running_sum_rows` будет:

```
id=1 -> 10
id=2 -> 30  -- 10 + 20  (только предыдущая физическая строка)
id=3 -> 35  -- 10 + 20 + 5
id=4 -> 50
```

---

### Важные нюансы и отличия

1. **Поведение по умолчанию**

   * Без `ORDER BY` — окно = вся партиция → одинаковый итог для каждой строки.
   * С `ORDER BY` и без явного `ROWS/RANGE/GROUPS` — по стандарту используется `RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW` → running total, но с поведением `RANGE` (учёт peer-строк).

2. **`RANGE` vs `ROWS`**

   * `ROWS` отсчитывает N физический строк относительно текущей строки.
   * `RANGE` отсчитывает по значению(ям) `ORDER BY` — включает все строки с равным значением (peers). При `ORDER BY date` `RANGE BETWEEN INTERVAL '6 days' PRECEDING AND CURRENT ROW` удобно для "последних 7 дней".
   * Если нужен предсказуемый «по одной физической строке» running total — используйте `ROWS ... CURRENT ROW`.

3. **Повторяющиеся (tie) значения в `ORDER BY`**

   * При `RANGE` все tie-строки получают одинаковое значение накопления для текущего value.
   * При `ROWS` поведение зависит от фактического порядка строк (внутреннего), и каждая физическая строка имеет своё накопление.

4. **Явно задаваемые фреймы**

   * Можно контролировать поведение полностью: `SUM(...) OVER (ORDER BY ts ROWS BETWEEN 2 PRECEDING AND CURRENT ROW)` — фиксированное окно из 3 строк.
   * Или `SUM(...) OVER (ORDER BY ts RANGE BETWEEN INTERVAL '6 days' PRECEDING AND CURRENT ROW)` — value-based временной интервал.

5. **Производительность**

   * `ORDER BY` требует сортировки внутри партиции (или использования индекса), что дороже, чем простая агрегация по партиции.
   * Использование больших окон и `RANGE` с интервалами может быть дороже по вычислениям.

6. **Неоднозначность при отсутствии `ORDER BY`**

   * Если нужен итог по партиции — используйте явно `SUM(...) OVER (PARTITION BY ...)`.
   * Если нужен running total — используйте `ORDER BY` + явный фрейм (`ROWS` для физического порядка или `RANGE` для value-based).

---

### Рекомендации / best practices

* Если вам нужно **итоговое значение партиции** — пишите `SUM(col) OVER (PARTITION BY ...)` (без `ORDER BY`).
* Если вам нужен **накопительный итог (running total)** — указывайте `ORDER BY` и **желательно** явно `ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW` для детерминированного поведения:

  ```sql
  SUM(amount) OVER (PARTITION BY user_id ORDER BY ts
                    ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)
  ```
* Если хотите value-based окно (например, сумма за последние 7 дней) — используйте `RANGE ... INTERVAL`.
* Будьте внимательны с tie-значениями в `ORDER BY`: `RANGE` включает peers, `ROWS` — оперирует физическими строками.

---

### Итог

* **Без `ORDER BY`** → результат — одна и та же сумма для каждой строки (итог по партиции).
* **С `ORDER BY`** → по умолчанию — running sum (`RANGE` до текущей строки), поведение может приводить к одинаковым значениям для peer-строк; для детерминизма и точного контроля используйте явный фрейм (`ROWS` или `RANGE ...`).

---

## Чем отличаются оконные функции от агрегирующих в SQL?

### Кратко — главное отличие

* **Агрегатные функции + GROUP BY** сворачивают (агрегируют) множество строк в **одну строку на группу** — уменьшают число строк результата.
* **Оконные функции** (через `... OVER (...)`) **не сворачивают строки**: они вычисляют значение на основе некоторого «окна строк» и **добавляют это значение как колонку к каждой исходной строке**; число строк остаётся тем же.

---

### Детали и примеры

Рассмотрим таблицу `sales`:

```
id | dept | amount | sale_date
---+------+--------+----------
1  | A    | 10     | 2024-01-01
2  | A    | 20     | 2024-01-02
3  | B    | 15     | 2024-01-01
4  | A    | 5      | 2024-01-03
```

#### 1) Агрегация с GROUP BY (итог по департаменту)

```sql
SELECT dept, SUM(amount) AS total
FROM sales
GROUP BY dept;
```

Результат:

```
dept | total
-----+------
A    | 35
B    | 15
```

Здесь 4 строки были сведены в 2 строки — одна строка на группу `dept`.

#### 2) Оконная функция — сумма по департаменту, без свёртки

```sql
SELECT id, dept, amount,
       SUM(amount) OVER (PARTITION BY dept) AS total_by_dept
FROM sales;
```

Результат:

```
id | dept | amount | total_by_dept
---+------+--------+--------------
1  | A    | 10     | 35
2  | A    | 20     | 35
4  | A    | 5      | 35
3  | B    | 15     | 15
```

Количество строк не изменилось; к каждой строке добавлено значение суммы по её департаменту.

---

### Что можно делать оконными, чего нельзя агрегатными (и наоборот)

#### Оконные функции умеют:

* Давать **контекст строки**: running totals (накопительная сумма), скользящие средние, значения соседних строк (`LAG`, `LEAD`), ранжирование (`ROW_NUMBER`, `RANK`, `PERCENT_RANK`) и т.д.
* Работать с **frame** (границами окна): `ROWS`, `RANGE`, `GROUPS` и `BETWEEN ... AND ...`.
* Возвращать значение для **каждой строки**, сохраняя детализацию.

#### Агрегатные функции (без OVER) умеют:

* Сводить набор строк в **резюме/итог** (GROUP BY): быстрый отчёт «одна строка — одна группа».
* Быть использованы в `HAVING` для фильтрации групп.
* Используются, когда нужен именно агрегатный отчёт, а не детализированная таблица с дополнительной колонкой.

---

### Semantics: ORDER BY, FRAME и peers

* Если вы пишете `SUM() OVER (ORDER BY ...)` — по умолчанию используется `RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW` → это **running total**, и при одинаковых значениях `ORDER BY` (peers) `RANGE` включает все peers одновременно (они получат одинаковую накопительную сумму).
* Если нужен детерминированный подсчёт по физическим строкам — используйте `ROWS ...` фреймы.

Агрегатная функция в GROUP BY не имеет такого понятия frame/peers — она просто суммирует всю группу.

---

### Правила применения в SQL-выражениях (о порядке вычислений)

* Обычная агрегатная функция (без `OVER`) применяется **вместе с `GROUP BY`** и даёт агрегированный набор; её нельзя использовать в `WHERE` (но можно в `HAVING`).
* Оконные функции вычисляются **после** обработки `FROM`, `WHERE`, `GROUP BY`, `HAVING` — то есть они видят уже сгруппированные/отфильтрованные данные. Поэтому:

  * Оконные функции **нельзя** использовать в `WHERE`, `GROUP BY`, `HAVING` (они вычисляются позже).
  * Оконные функции **можно** использовать в `SELECT` и `ORDER BY`.
* Это означает: если нужно сначала агрегировать, а потом применить окно к результатам (например, ранжировать группы по сумме), то можно либо агрегировать в подзапросе/CTE, либо агрегировать в SELECT и затем поверх результата применить оконную функцию.

Пример — ранжирование департаментов по сумме:

```sql
WITH dept_sum AS (
  SELECT dept, SUM(amount) AS total
  FROM sales
  GROUP BY dept
)
SELECT dept, total,
       RANK() OVER (ORDER BY total DESC) AS dept_rank
FROM dept_sum;
```

---

### Ограничения в синтаксисе

* `SUM(column)` без `OVER` — агрегат, требует GROUP BY если есть другие не-агрегированные колонки в SELECT.
* `SUM(column) OVER (...)` — оконная форма, может сосуществовать с другими колонками без GROUP BY.
* Нельзя использовать оконную функцию в `GROUP BY` или `HAVING`.
* Нельзя использовать агрегат (без OVER) в `ORDER BY` напрямую, если не сгруппировали (за исключением специфичных СУБД/диалектов).

---

### Производительность и оптимизация

* `GROUP BY` обычно реализуется через хеш-агрегацию или сортировку + агрегацию и эффективен для получения итогов.
* Оконные функции обычно требуют **сортировки** по `PARTITION BY`/`ORDER BY` (внутри партиций) и могут быть дороже для больших наборов данных, особенно для сложных окон (RANGE с интервалами, большие frame).
* В некоторых задачах можно выбрать: посчитать агрегат с `GROUP BY` в подзапросе (меньше данных для ранжирования) или использовать оконную функцию напрямую — зависит от цели (свертка vs добавление колонки) и объёма данных.

---

### Когда что использовать (рекомендации)

* Нужен **итоговый отчёт (свернутые строки)** → используйте `GROUP BY` и агрегаты.
* Нужна **детализированная таблица** с дополнительными агрегатными значениями на уровне строки (например, «сумма по клиенту» к каждой покупке) → используйте оконные функции `... OVER (PARTITION BY ...)`.
* Нужен **running total, lead/lag, ранжирование по группе** → оконные функции — прямой инструмент.
* Нужен ранг агрегированных групп — агрегируйте (`GROUP BY`), затем применяйте оконную функцию к результату (через CTE/подзапрос).

---

### Несколько практических примеров

1. Общая сумма по всем строкам (эквивалент GROUP BY по всей таблице):

```sql
SELECT id, amount, SUM(amount) OVER () AS total_all
FROM sales;
```

2. Накопительная сумма по времени (детерминированно по строкам):

```sql
SELECT id, sale_date, amount,
       SUM(amount) OVER (ORDER BY sale_date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS running_total
FROM sales;
```

3. Итого по группе (как в GROUP BY), но без свёртки:

```sql
SELECT id, dept, amount,
       SUM(amount) OVER (PARTITION BY dept) AS total_by_dept
FROM sales;
```

4. Итог по группе и ранжирование групп по сумме:

```sql
WITH dept_sum AS (
  SELECT dept, SUM(amount) AS total
  FROM sales
  GROUP BY dept
)
SELECT dept, total,
       RANK() OVER (ORDER BY total DESC) AS rank_by_total
FROM dept_sum;
```

---

### Вывод (сжатая шпаргалка)

* **GROUP BY + агрегаты** — сворачивают строки: нужен итог/агрегированная сводка. Возвращают по одной строке на группу.
* **Оконные функции (OVER)** — не сворачивают строки: дают дополнительную колонку с вычислением по окну/партиции/фрейму; подходят для running totals, рангов, сравнения со смежными строками.
* Оконные функции выполняются **после** агрегирования/фильтрации, поэтому иногда потребуется CTE/подзапрос, если нужно агрегировать, а затем применить дополнительные окна.

Если хотите, могу на ваших реальных данных подготовить примеры запросов (GROUP BY и эквивалент с оконной функцией) и показать разницу результатов и примерный план выполнения.

---

## Можно ли использовать несколько агрегационных функций в select?

Короткий ответ: да — в `SELECT` можно использовать **сколько угодно агрегатных функций** одновременно. Но при этом важно понимать правила их применения (контекст `GROUP BY`/`OVER`, `HAVING`/`WHERE`, семантику `DISTINCT` и т.п.). Ниже — подробное и практическое объяснение с примерами и рекомендациями.

Агрегатные функции (`COUNT`, `SUM`, `AVG`, `MIN`, `MAX`, `STRING_AGG`, `ARRAY_AGG`, `JSON_AGG`, `BOOL_AND`, `BOOL_OR` и т.д.) собирают группу строк в одно значение. В `SELECT` их можно комбинировать:

* без `GROUP BY` — агрегаты считаются по всей выборке и вернётся одна строка;
* с `GROUP BY` — агрегаты считаются по каждой группе, в результате — по одной строке на группу.

Важно: **нельзя** в `SELECT` одновременно выводить неагрегированные столбцы и агрегаты, если эти столбцы не указаны в `GROUP BY`. SQL выдаст ошибку.

---

## Чем отличается DENSE_RANK от RANK?

Ниже — подробное и чёткое объяснение, в чём именно различие между `RANK()` и `DENSE_RANK()` в SQL, с примерами, семантикой, вариантами применения и практическими советами.

---

### Коротко (суть)

* `RANK()` присваивает одинаковый ранг одинаковым (tie) значениям, **но оставляет разрывы (gaps)** в нумерации после tie.
* `DENSE_RANK()` тоже присваивает одинаковый ранг tie-значениям, **но нумерация идёт подряд без разрывов** (no gaps).

---

### Формулы (интуитивно)

* `RANK()` для строки = `1 + количество строк с более высоким (или меньшим, в зависимости от ORDER BY) порядковым положением`
  \=> если две строки tie на 1-м месте, следующая строка получит ранг 3.
* `DENSE_RANK()` для строки = `1 + количество **различных** значений ORDER BY, превосходящих текущий`
  \=> если две строки tie на 1-м месте, следующая получит ранг 2.

---

### Пример

Данные:

```
id | name | score
---+------+------
1  | A    | 100
2  | B    | 100
3  | C    | 90
4  | D    | 80
5  | E    | 80
```

Запрос:

```sql
SELECT id, name, score,
       RANK()       OVER (ORDER BY score DESC)     AS rnk,
       DENSE_RANK() OVER (ORDER BY score DESC)     AS drnk
FROM scores;
```

Результат:

```
id | name | score | rnk | drnk
---+------+-------+-----+-----
1  | A    | 100   | 1   | 1
2  | B    | 100   | 1   | 1
3  | C    | 90    | 3   | 2
4  | D    | 80    | 4   | 3
5  | E    | 80    | 4   | 3
```

Обратите внимание:

* Для `RANK()` после двух единиц идёт 3 → затем 4 (потому что 1,1 занимают 2 строк — следующая позиция = 1 + 2 = 3, далее 4).
* Для `DENSE_RANK()` ранги идут 1,1,2,3,3 — без пропусков между 1 и 2 и далее.

---

### Поведение при `PARTITION BY` и сложном `ORDER BY`

* Оба оператора — оконные функции. При использовании `PARTITION BY` ранжирование выполняется отдельно в каждой партиции.
* Tie определяется **по всем выражениям в `ORDER BY` внутри `OVER(...)`**; если `ORDER BY` не уникален, те строки считаются peers (равными) и получают одинаковый ранг.

---

### Как выбирать: когда `RANK()`, когда `DENSE_RANK()`

* Используйте `RANK()` когда важно, чтобы ранги отражали **положение в «последовательности» с учётом количества предыдущих элементов** — например, спортивные позиции, где при tie следующая позиция пропускается (двое заняли 1-е → никто не на 2-м, следующий — 3-й).
* Используйте `DENSE_RANK()` когда нужны **плотные, подряд идущие номера категорий/рангов** (например, ранжирование категорий по продажам, где важно иметь компактные номера для группировки/нулевого смещения).

Примеры:

* `RANK()` — конкурс со звёздами: два победителя = места 1 и 1, следующему участнику место 3.
* `DENSE_RANK()` — присвоение уровня класса (High/Medium/Low): если два элемента в High, следующий уровень — Medium с номером 2.

---

### Влияние на топ-N запросы

При отборе «топ-N» записей поведение разных функций даст разные результаты:

* `WHERE RANK() <= 3` может вернуть меньше или больше строк, чем ожидается, потому что `RANK()` создаёт разрывы (например, если есть tie на ранге 3, вернутся все строки с рангом 3).
* `WHERE DENSE_RANK() <= 3` вернёт все элементы, принадлежащие трём первым различным позициям по ORDER BY, без учёта разрывов.

Вывод: при выборе критерия для top-N учитывайте, нужны ли вам «gap-aware» позиции или плотные ранги.

---

### Как получить `DENSE_RANK`, если он недоступен (вариант через DISTINCT + ROW_NUMBER)

Если СУБД не поддерживает `DENSE_RANK()` (редко встречается в современных СУБД), можно:

1. Сформировать набор **distinct** значений по полю сортировки и пронумеровать их `ROW_NUMBER()` → это даст плотные ранги для значений;
2. Затем соединить (join) обратно с оригинальной таблицей по ключу сортировки.

Пример:

```sql
WITH distinct_scores AS (
  SELECT DISTINCT score
  FROM scores
  ORDER BY score DESC
),
dense AS (
  SELECT score,
         ROW_NUMBER() OVER (ORDER BY score DESC) AS dense_rank
  FROM distinct_scores
)
SELECT s.*, d.dense_rank
FROM scores s
JOIN dense d USING (score)
ORDER BY score DESC, id;
```

Этот паттерн эффективнее, чем correlated subquery с `COUNT(DISTINCT ...)`.

---

## Как RANK() работает с NULL?

В SQL функция `RANK()` при ранжировании учитывает строки с `NULL`-значениями так же, как и любая сортировка в базе данных: поведение зависит от настроек сортировки (`ORDER BY ... ASC|DESC`) и конкретной СУБД.

### Основные моменты

1. **NULL участвует в ранжировании**
   `NULL` не игнорируется — строка с `NULL` в колонке сортировки получит свой ранг.

2. **Место `NULL` зависит от порядка сортировки**

   * В большинстве СУБД (PostgreSQL, Oracle, SQL Server) при `ORDER BY ... ASC` `NULL` считается наименьшим значением и идёт в начале.
   * При `ORDER BY ... DESC` `NULL` идёт в конце (если явно не указано `NULLS FIRST` или `NULLS LAST`).

3. **`NULL` считается равным `NULL` при ранжировании**
   Все строки, у которых значение в сортируемом столбце `NULL`, будут считаться «tie» и получат одинаковый ранг.

4. **Разрывы в рангах**
   Как и всегда в `RANK()`, если есть несколько строк с одинаковым значением (в том числе `NULL`), они получают одинаковый ранг, а следующая группа значений получает ранг с учётом пропуска позиций.

---

### Пример (PostgreSQL)

```sql
CREATE TABLE test_rank (
    id    INT,
    score INT
);

INSERT INTO test_rank VALUES
(1, 100),
(2, 90),
(3, NULL),
(4, 90),
(5, NULL);

SELECT id, score,
       RANK() OVER (ORDER BY score DESC) AS rnk
FROM test_rank;
```

Результат:

```
id | score | rnk
---+-------+-----
1  | 100   | 1
2  | 90    | 2
4  | 90    | 2
3  | NULL  | 4
5  | NULL  | 4
```

Разбор:

* `100` → ранг 1.
* Два значения `90` → ранг 2 для обеих строк.
* `NULL`-значения сортируются последними (DESC) → ранг 4 для обеих строк.

---

## У вас есть поле с datetime, а вам надо сделать фильтр по дате без учета времени - перечислите возможные способы решения проблемы.

### Ключевая идея и рекомендация

Лучший и наиболее производительный подход — **использовать диапазон** (range) с полуоткрытой границей:

```sql
WHERE ts >= '2024-08-01'::date
  AND ts <  ('2024-08-01'::date + INTERVAL '1 day')
```

(в общем виде: `>= date AND < date + 1 day`).
Этот подход sargable, использует индекс по `ts` и не зависит от точности времени. Я буду к нему часто возвращаться как к рекомендуемому.

---

### 1) Привести значение к типу `date` и сравнить (удобно, но часто не сагабильно)

* PostgreSQL:

  ```sql
  WHERE ts::date = DATE '2024-08-01'
  -- или
  WHERE CAST(ts AS date) = DATE '2024-08-01'
  ```

**Плюсы:** читаемо, просто.
**Минусы:** функция применяется к столбцу → **индекс не используется**, если нет функционального/вычисляемого столбца или функционального индекса.

---

### 2) Диапазон (рекомендуется): от полуночи до полуночи следующего дня (sargable)

**Формула (универсальная):**

```sql
WHERE ts >= @date
  AND ts  < @date + INTERVAL '1 day'
```

Примеры:

* PostgreSQL:

  ```sql
  WHERE ts >= DATE '2024-08-01'
    AND ts <  DATE '2024-08-01' + INTERVAL '1 day';
  ```

**Плюсы:** использует индекс по `ts`, точен с точки зрения границ, не зависит от точности времени. **Всегда лучший выбор**, если у вас есть индекс на `ts`.
**Минусы:** нужно аккуратно вычислять верхнюю границу (используйте ` < next_day` вместо `<= 23:59:59`).

---

### 3) `DATE_TRUNC` / `TRUNC` (срез по дню) — схоже с приведением к дате

* PostgreSQL:

  ```sql
  WHERE date_trunc('day', ts) = '2024-08-01'::timestamp
  ```

**Плюсы:** удобно для выражения «обрезать время».
**Минусы:** функция на столбце → не использует обычный индекс (нужно функцияльный индекс, если хотите производительность).

---

### 4) `BETWEEN` с временными границами (менее предпочтителен)

```sql
WHERE ts BETWEEN '2024-08-01 00:00:00' AND '2024-08-01 23:59:59.999'
```

**Минусы:** риски с точностью (миллисекунды/микросекунды) и временем в разных типах, не рекомендуется. Лучше `>= start AND < next_day`.

---

### 5) Преобразование в строку / форматирование (не рекомендуемо для производительности)

```sql
WHERE TO_CHAR(ts, 'YYYY-MM-DD') = '2024-08-01'   -- Postgres
```

**Минусы:** полностью не сагабильно, медленно.

---

### 6) Использовать вычисляемый/персистентный столбец или функциональный индекс (оптимизация для часто выполняемых запросов)

Если вы часто фильтруете по дате без времени, создайте колонку с датой и индекс на ней:

* PostgreSQL (функциональный индекс):

  ```sql
  CREATE INDEX idx_table_date ON table ((ts::date));
  -- потом WHERE ts::date = '2024-08-01' будет использовать индекс
  ```

**Плюсы:** можно писать удобные выражения (`CAST(ts AS date) = ...`) и при этом индекс работает.
**Минусы:** дополнительное место/поддержка, возможная стоимость при вставках/обновлениях.

---

### 7) Временные зоны — вещь, о которой нельзя забывать

Если поле — `timestamp with time zone` (Postgres `timestamptz`) или БД хранит UTC, важно согласовать дату с нужной временной зоной:

* Postgres (пример: интерпретировать дату в локальной зоне и получить границы в UTC):

  ```sql
  -- границы в timestamptz для временной зоны Europe/Bucharest
  WHERE ts >= (DATE '2024-08-01'::timestamp AT TIME ZONE 'Europe/Bucharest')
    AND ts <  ((DATE '2024-08-01'::timestamp + INTERVAL '1 day') AT TIME ZONE 'Europe/Bucharest');
  ```

Пояснение: `timestamp AT TIME ZONE zone` переводит локальный timestamp в timestamptz (сдвиг в UTC), что позволяет корректно задать границы в момент времени независимо от хранения.

**Рекомендация:** всегда конвертируйте дату в нужные временные границы в том часовом поясе, в котором вы хотите фильтровать (лобби пользователей), особенно при `timestamptz`.

---

### 8) Отдельные сценарии и шаблоны

* **Фильтр за период (N дней):**

  ```sql
  WHERE ts >= @date_start
    AND ts <  @date_end_plus_1
  ```

* **Running queries с датой в явном виде (Postgres):**

  ```sql
  WHERE ts >= '2024-08-01'::date
    AND ts <  ('2024-08-01'::date + INTERVAL '1 day')
  ```

---

### 9) Сравнение подходов — кратко

* **Диапазон (`>= date AND < date+1`)** — лучший выбор: сагабилен, индексируем, точен. Рекомендуется в 99% случаев.
* **CAST/DATE()/TRUNC/DATE_TRUNC** — удобно, но **не использует обычный индекс**, если только не создан функциональный индекс/вычисляемая колонка.
* **TO_CHAR / форматирование** — медленно, не использует индекс; не рекомендуется.
* **BETWEEN с 23:59:59** — опасно из-за точности; лучше `< next_day`.
* **Функция на колонке + функциональный индекс / persisted column** — хорошее компромиссное решение, если запросы с приведением к дате часты.

---

## Перечислите логические и физические джойны и алгоритмическую сложность физических.

### Физические JOINы

Физический джойн — это алгоритм, который используется для выполнения операции объединения двух таблиц. Иными словами, это то, что происходит "под капотом", когда вы вызываете join в запросе

Основных алгоритмов всего 3: nested loops, merge join, hash join/hash match.

#### Nested loops

Принцип работы уже понятен из названия: каждый элемент внешнего цикла сравнивается с каждым элементом внутреннего.
Алгоритмическая сложность - O(n**2)

```sql
For Each value in pile1
    For Each value in pile2
        If pile1.value = pile2.value
        Return pile1.value, pile2.value
```
 
#### Merge join

Для этого алгоритма элементы уже должны быть отсортированы. Тут мы проходимся двумя указателями по элементам и сравниваем их. В конце проходим по оставшимся элементам.

Если не считать сортировку, алгоритмическая сложность - O(n).

```sql
get first row R1 from   input 1
get first row R2 from   input 2

while not at the end   of either input
      begin
          if R1 joins with R2
              begin
                  get next row R2 from input 2
                  return (R1, R2)
              end
          else if R1 < R2
              get next row R1 from input 1
          else
              get next row R2 from input 2
    end
```

#### Hash join

Вычисляем хэш для каждого элемента левой таблицы, затем вычисляем хэш у элементов правой таблицы и проверяем его наличие в левой.
Алгоритмическая сложность - O(n).

```sql
// Build phase
FOR each row in BuildTable DO
    Compute hash value for the join key
    Insert row into HashTable based on hash value
END FOR

// Probe phase
FOR each row in ProbeTable DO
    Compute hash value for the join key
    IF hash value exists in HashTable THEN
        Retrieve matching rows from HashTable
        FOR each matching row DO
            Combine rows from ProbeTable and BuildTable
            Add the combined row to the result set
        END FOR
    END IF
END FOR
```

---

#### Выбор физического JOIN

После оценки алгоритмической сложности физических джойнов можно прийти к выводу, что выбор hash join является оптимальным решением, однако это далеко не так. Как и во многом в программировании, всегда есть space–time trade-off (компромисс времени и памяти), и выбор оптимального джойна будет зависеть от входных данных.

С выбором джойна в большинстве случаев достаточно хорошо справляется оптимизатор, однако бывают ситуации, когда выбором джойна придется заниматься вам.

##### Условие соединения
Для equi-joins (равенство =, неравенство !=) и non-equi-joins (>, <, >=, <=). Для второго типа подойдет только nested loops.

##### Размер таблиц
Также, конечно, важен размер таблиц. Из-за необходимости многократно проходить по второй таблице в случае с nested loops будет велика цена I/O, в случае merge join будет дорогой сортировка, а в случае hash join может не хватить памяти для хеширования, и часть придется переносить на диск. Хешируется, кстати, меньшая таблица.

Если вы работаете с отсортированными данными, выиграет merge join, а с неотсортированными — hash join.

В случае, когда обе таблицы маленькие, эффективнее может быть nested loops, ведь с merge сортировка может вовсе не окупиться.

##### Индексы и дубликаты
В случае с неиндексированными данными лучше справятся merge и hash join, однако наличие большого количества дубликатов при выборе hash join может повлечь неправильное распределение данных и необходимость обработки коллизий.

### Логические JOINы

Основные логические виды JOIN:

1. **INNER JOIN**
   Возвращает только те строки, для которых условие соединения выполняется в обеих таблицах.
   Эквивалентно `JOIN` без указания типа (по умолчанию `INNER`).

2. **LEFT JOIN** или **LEFT OUTER JOIN**
   Возвращает все строки из левой таблицы и только совпадающие строки из правой.
   Для несовпавших строк правые колонки будут `NULL`.

3. **RIGHT JOIN** или **RIGHT OUTER JOIN**
   Возвращает все строки из правой таблицы и только совпадающие строки из левой.
   Для несовпавших строк левые колонки будут `NULL`.

4. **FULL JOIN** или **FULL OUTER JOIN**
   Возвращает все строки из обеих таблиц: совпавшие и несовпавшие.
   Для несовпавших значений с одной стороны — `NULL` в соответствующих колонках.

5. **CROSS JOIN**
   Декартово произведение — каждая строка левой таблицы соединяется с каждой строкой правой, без условия соединения.

---

## Что делает утилита PGTune?

Утилита **PGTune** предназначена для автоматической генерации рекомендованных значений параметров конфигурации PostgreSQL на основе характеристик аппаратного обеспечения и нагрузки сервера.

---

## Зачем нужна PGTune

Ручная настройка параметров — трудоёмкий и нетривиальный процесс, особенно для новичков. PGTune автоматизирует этот процесс, предлагая базовые настройки, которые улучшают производительность без глубокого погружения в тонкости настройки.

---

### Что именно делает PGTune

1. **Собирает информацию о сервере:**

   * Объём оперативной памяти (RAM).
   * Количество процессорных ядер (CPU).
   * Тип и скорость хранения данных (SSD/HDD).
   * Размер базы данных (опционально).
   * Тип рабочих нагрузок (OLTP, OLAP, смешанный).

2. **На основе этой информации рассчитывает оптимальные значения основных параметров PostgreSQL:**

   * `shared_buffers` — размер памяти, выделенной под кэширование страниц базы данных.
   * `work_mem` — размер памяти, выделяемой под операции сортировки и хеширования в запросах.
   * `maintenance_work_mem` — память для операций обслуживания (индексация, VACUUM).
   * `effective_cache_size` — оценка доступного объёма кэшированной ОС памяти, которую PostgreSQL учитывает при планировании запросов.
   * `checkpoint_segments` (в новых версиях — `max_wal_size`) — параметры, влияющие на частоту контрольных точек.
   * Другие параметры, связанные с логированием, параллелизмом, автовацуумом и т.д.

3. **Генерирует конфигурационный файл (обычно `postgresql.conf`) или часть настроек, которые можно применить в существующем файле конфигурации.**

---

### Преимущества использования PGTune

* Значительно упрощает стартовую настройку PostgreSQL.
* Позволяет адаптировать конфигурацию под аппаратные ресурсы без глубоких знаний.
* Помогает избежать типичных ошибок при настройке параметров.
* Может ускорить работу базы, уменьшить время отклика и повысить стабильность.

---

### Ограничения

* PGTune даёт только стартовые рекомендации, которые могут потребовать доработки под конкретные сценарии и нагрузки.
* Не заменяет полноценный аудит и тонкую настройку, особенно для больших и сложных систем.
* Иногда рекомендуемые значения могут быть консервативными или не учитывать все особенности приложений.

---

### Пример использования

```bash
pgtune -i /path/to/postgresql.conf -o /path/to/new_postgresql.conf -T oltp -M 16GB -c 8
```

Где:

* `-i` — исходный конфигурационный файл.
* `-o` — файл с новыми параметрами.
* `-T` — тип нагрузки (`oltp`, `olap`, `mixed`).
* `-M` — объём оперативной памяти.
* `-c` — число процессорных ядер.

---

## Что такое нормализация?

**Нормализация** — это процесс проектирования структуры реляционной базы данных с целью уменьшения избыточности данных и предотвращения аномалий при вставке/обновлении/удалении. Основная идея — разбить исходную таблицу на несколько взаимосвязанных таблиц так, чтобы каждая зависимость данных была представлена корректно и однозначно.

---

### Задачи и преимущества нормализации

* **Устранение избыточности** (redundancy) — одно и то же значение не хранится в нескольких местах без необходимости.
* **Предотвращение аномалий**:

  * **Insert anomaly** — невозможность вставить логически корректную запись без дополнительных данных.
  * **Update anomaly** — необходимость обновлять одно и то же значение в нескольких местах.
  * **Delete anomaly** — удаление строки может непреднамеренно удалить нужную информацию.
* **Повышение целостности данных** — проще обеспечить согласованность через ключи и ограничения.
* **Ясность семантики** — структура таблиц отражает бизнес-сущности и их отношения.

---

### Ключевое понятие: функциональная зависимость

Нормализация опирается на понятие **функциональной зависимости (FD)**: `A -> B` означает, что значение атрибута A однозначно определяет значение B. Нормальные формы формулируются через требования к функциональным зависимостям.

---

### Нормальные формы (основные)

#### 1NF (Первая нормальная форма)

* Все атрибуты атомарны (нет повторяющихся групп или массивов в одной колонке).
* Пример нарушения: поле `items = "item1,item2,item3"` в одной колонке — не 1NF.
* Устраняется: создаются отдельные строки/таблицы для элементов.

#### 2NF (Вторая нормальная форма)

* Таблица в 1NF.
* Нет **частичных зависимостей** от части составного ключа (только для таблиц с составным первичным ключом).
* Если есть составной ключ `(order_id, product_id)`, и поле `product_name` зависит только от `product_id`, то это частичная зависимость → информация о продукте выделяется в отдельную таблицу `products`.

#### 3NF (Третья нормальная форма)

* Таблица в 2NF.
* Нет **транзитивных зависимостей** `A -> B -> C`, когда неключевой атрибут зависит от другого неключевого атрибута.
* Если в таблице `orders` есть `customer_id` и `customer_address` (адрес зависит от customer\_id, а заказ зависит от customer\_id), `customer_address` следует вынести в таблицу `customers`.

#### BCNF (Бойс–Коддова нормальная форма)

* Более строгая версия 3NF: для любой FD `X -> Y` X должен быть суперключом.
* Устраняет случаи, которые проходят 3NF, но нарушают более строгие зависимости.

#### 4NF, 5NF и далее

* **4NF** — учитывает многозначные зависимости (multi-valued dependencies). Пример: сотрудник может иметь множество навыков и множество проектов — хранение в одной таблице без разбивки ведёт к дублированию комбинаций.
* **5NF** (Projection-Join NF) — про декомпозиции, которые можно восстановить только через корректные join'ы без потерь.
* На практике большинство OLTP систем нормализуют до **3NF или BCNF**; 4NF/5NF требуются редко.

---

### Пример: избыточная таблица → нормализация

Исходная (плохая) таблица:

```sql
orders_raw(order_id, order_date, customer_name, customer_email, product_id, product_name, qty, price)
```

Проблемы:

* Дублирование `customer_name/email` для каждого заказа клиента.
* Дублирование `product_name` для каждого заказа продукта.
* При изменении email надо обновлять много строк.

Нормализуем (пример):

```sql
CREATE TABLE customers (
  customer_id SERIAL PRIMARY KEY,
  name TEXT,
  email TEXT UNIQUE
);

CREATE TABLE products (
  product_id INT PRIMARY KEY,
  name TEXT,
  price NUMERIC
);

CREATE TABLE orders (
  order_id SERIAL PRIMARY KEY,
  order_date DATE,
  customer_id INT REFERENCES customers(customer_id)
);

CREATE TABLE order_items (
  order_id INT REFERENCES orders(order_id),
  product_id INT REFERENCES products(product_id),
  qty INT,
  price NUMERIC,
  PRIMARY KEY(order_id, product_id)
);
```

Теперь обновление e-mail делается в `customers` в одном месте; данные о продукте в `products` и т.д.

---

### Аномалии — иллюстрация проблемы

* **Update anomaly:** поменяли `product_name` в одной строке, забыли в другой → разные названия одного продукта.
* **Insert anomaly:** нельзя добавить продукт, пока нет заказа (если продукт умещён только в `orders_raw`).
* **Delete anomaly:** удалили последний заказ клиента → потеряли информацию о самом клиенте.

Нормализация предотвращает это.

---

### Недостатки нормализации и когда денормализовать

Нормализация улучшает целостность, но увеличивает количество JOIN'ов при чтении. В реальных системах бывает целесообразно **денормализовать** часть данных:

* OLTP (транзакционные системы): обычно нормализуют до 3NF/BCNF.
* OLAP / аналитика / BI: часто используют **денормализованные схемы** (звёздная/снежинка) или хранение в data lake/lakehouse для оптимизации чтения и агрегаций.
* Денормализация применяется ради производительности: копирование полей (denormalized columns), материализованные представления, предварительные агрегаты.

Денормализация — компромисс: быстрее чтение, сложнее поддерживать целостность; часто требует дополнительной логики при записи (триггеры, обновления ETL).

---

### Практические рекомендации

* Понимайте нагрузку: если большинство операций — записи/обновления, нормализация важна; если — чтение/аналитика, рассматривайте денормализацию и materialized views.
* Нормализуйте до уровня, обеспечивающего отсутствие основных аномалий (обычно 3NF или BCNF).
* Используйте индексы для ускорения JOIN'ов на полях-ссылках.
* Для часто выполняемых агрегаций используйте материализованные представления или предварительно агрегированные таблицы.
* При работе с временными данными и партиционированием учитывайте trade-offs между нормой и производительностью.

---

## Какие типы индексов бывают?

### Основные физические типы индексов в PostgreSQL

#### 1. B-tree (по умолчанию)

* **Назначение:** универсальный индекс для равенств и диапазонных запросов (`=`, `<`, `<=`, `>`, `>=`, `BETWEEN`, поиск по префиксу в упорядоченных данных).
* **Сложность:** логарифмическая `O(log n)` для поиска.
* **Плюсы:** быстрый, надёжен, поддерживает уникальные индексы, индекс-only scan (при выполнении условий).
* **Минусы:** неэффективен для полнотекстового поиска, массивов, JSONB или геоданных.
* **Пример:**

  ```sql
  CREATE INDEX idx_users_email ON users USING btree (email);
  ```
* **Типичные случаи:** первичные ключи, уникальные индексы, поиск по временам, сортировка/ORDER BY.

---

#### 2. Hash

* **Назначение:** оптимизирован для операций равенства (`=`).
* **Сложность:** амортизированное `O(1)` для поиска по ключу.
* **Плюсы:** быстрый для чистой эквивалентной выборки.
* **Минусы:** до недавних версий PostgreSQL хеш-индексы были небезопасны после сбоя; сейчас улучшены, но всё ещё реже используются; не поддерживают диапазоны, не являются универсальным решением.
* **Пример:**

  ```sql
  CREATE INDEX idx_hash ON t USING hash (col);
  ```
* **Рекомендация:** чаще использовать B-tree, если не доказана реальная выгода hash.

---

#### 3. GiST (Generalized Search Tree)

* **Назначение:** обобщённая структура для индексирования данных, где требуется произвольная логика поиска (например, геопространственные запросы — PostGIS).
* **Плюсы:** поддерживает многомерные и пространственные индексирования; гибкость (поддержка nearest neighbor, overlap, etc.).
* **Минусы:** сложнее, может уступать по скорости специализированным структурам; семантика зависит от operator class.
* **Типичные применения:** PostGIS (геоданные), индексирование `tsvector` (иногда), `range`-типов.
* **Пример:**

  ```sql
  CREATE INDEX idx_geom_gist ON geom_table USING gist (geom);
  ```

---

#### 4. SP-GiST (Space-partitioned GiST)

* **Назначение:** вариант GiST с разбиением пространства; подходит для сильно разреженных/структурированных пространств (quadtree, radix tree, tries).
* **Плюсы:** эффективен для некоторых типов данных и распределений; полезен для точечных пространственных данных, текстовых префиксов, IP-диапазонов.
* **Минусы:** меньше универсален; operator class важен.
* **Пример:**

  ```sql
  CREATE INDEX idx_spgist ON t USING spgist (ip_range);
  ```

---

#### 5. GIN (Generalized Inverted Index)

* **Назначение:** индекс «обратного вида» для быстрого поиска по множественным ключам в одной строке — массивы, JSONB, `tsvector` (full-text).
* **Плюсы:** очень эффективен для поиска по элементам массивов, containment (`@>`), полнотекстового поиска (при использовании `tsvector`), JSONB ключей/значений.
* **Минусы:** большие размеры индекса, дорогая вставка/обновление (много записей в индексе), первоначальная сборка медленнее.
* **Типичные случаи:** `jsonb @>`, `array @>`, `WHERE to_tsvector(col) @@ plainto_tsquery('...')`.
* **Пример:**

  ```sql
  CREATE INDEX idx_documents_gin ON documents USING gin (content_tsv);
  CREATE INDEX idx_jsonb_gin ON data USING gin (payload jsonb_path_ops);
  ```
* **Примечание:** pg\_trgm часто использует GIN/GiST для fast LIKE/ILIKE/`%foo%`.

---

#### 6. BRIN (Block Range Index)

* **Назначение:** компактный индекс для очень больших, «локально упорядоченных» таблиц (данные имеют корреляцию с физической записью, например `created_at`).
* **Плюсы:** маленький размер, очень быстрый для вставок; подходит для аналитических больших таблиц.
* **Минусы:** точность хуже, чем у B-tree; при плохой корреляции эффективность падает.
* **Типичные случаи:** огромные лог-таблицы, архивы, где данные физически упорядочены по времени или ключу.
* **Пример:**

  ```sql
  CREATE INDEX idx_large_brin ON events USING brin(event_ts);
  ```

---

### Логические / семантические и дополнительные виды индексов

Эти варианты — не отдельные физические структуры, а способы использования и модификации индекса.

#### 7. Уникальные индексы (UNIQUE)

* **Назначение:** гарантировать уникальность значения (SQL constraint).
* **Семантика:** фактически B-tree/другой индекс с ограничением уникальности.
* **Пример:**

  ```sql
  CREATE UNIQUE INDEX ux_users_email ON users (email);
  ```

#### 8. Много-колоночные (composite / multicolumn) индексы

* **Особенность:** порядок колонок важен; индекс эффективен для запросов, использующих **левую префиксную** часть списка колонок.
* **Пример:**

  ```sql
  CREATE INDEX idx_orders ON orders (customer_id, created_at DESC);
  ```

#### 9. Индексы на выражениях (functional / expression indexes)

* **Назначение:** индексировать результат выражения (например, `lower(email)`), полезно для поиска без учёта регистра.
* **Пример:**

  ```sql
  CREATE INDEX idx_users_lower_email ON users ((lower(email)));
  ```

#### 10. Частичные индексы (partial indexes)

* **Назначение:** индексировать только подмножество строк, задаваемое условием `WHERE`.
* **Плюсы:** меньший размер, быстрее обновление, эффективен при селективных условия.
* **Пример:**

  ```sql
  CREATE INDEX idx_active_users ON users (last_login) WHERE active = true;
  ```

#### 11. Covering / INCLUDE-индексы

* **PostgreSQL (начиная с 11):** `INCLUDE (col2, col3)` позволяет хранить в индексе дополнительные колонки, чтобы обеспечить **index-only scan** (они не участвуют в сортировке/поиске).
* **Пример:**

  ```sql
  CREATE INDEX idx_orders_on_user_date_include_total
    ON orders (user_id, created_at)
    INCLUDE (total_amount);
  ```

#### 12. Частично уникальные / conditional unique indexes

* Уникальность только для подмножества строк:

  ```sql
  CREATE UNIQUE INDEX ux_active_email ON users (email) WHERE active = true;
  ```

#### 13. Кластеризация (CLUSTER) и кластерный индекс

* **CLUSTER table USING idx:** физически реорганизует таблицу по индексу — улучшает локальность доступа, но **не поддерживается автоматически** при последующих вставках; нужно повторно кластеризовать.
* В PostgreSQL нет «кластерного индекса» как в некоторых СУБД, но CLUSTER даёт аналогичный эффект однократно.

#### 14. Партиционированные индексы

* В таблицах с партиционированием индексы могут создаваться на каждой партиции или как глобальные (в PostgreSQL глобальные индексы появились позже). Планирование индексации для партиций — отдельная тема.

---

### Примеры создания индексов (с синтаксисом)

```sql
-- B-tree
CREATE INDEX idx_btree ON t USING btree (col);

-- Multicolumn
CREATE INDEX idx_multi ON t (col1, col2);

-- Partial
CREATE INDEX idx_partial ON t (col) WHERE status = 'active';

-- Functional
CREATE INDEX idx_lower ON users ((lower(email)));

-- GIN for jsonb
CREATE INDEX idx_jsonb_gin ON table_name USING gin (data jsonb_path_ops);

-- GiST for geometry
CREATE INDEX idx_geom ON geom_table USING gist (geom);

-- BRIN for huge table
CREATE INDEX idx_brin ON huge_table USING brin(created_at);

-- INCLUDE (covering)
CREATE INDEX idx_inc ON orders (user_id) INCLUDE (total);
```

---

### Поведение, ограничения и производительность

* **Индексы ускоряют чтение, замедляют запись.** Каждый индекс — дополнительная работа при `INSERT`, `UPDATE`, `DELETE`. Чем больше индексов — тем дороже операции записи.
* **Использование индекса зависит от селективности.** Низкоселективные колонки (например, булевы) редко полезно индексировать сами по себе; лучше — частичный индекс.
* **Index-only scan:** возможен, если индекс содержит все необходимые столбцы и visibility map показывает, что строки видимы (требуется VACUUM для обновления visibility map).
* **Operator class и collations:** индексы зависят от operator class (напр., `text_pattern_ops` для LIKE) и от collations (сортировка/сравнение строк).
* **Индексы занимают пространство.** BRIN — очень компактный, GIN и GiST — большие.
* **Поддержка транзакционной целостности:** уникальные индексы, constraints опираются на индексы.
* **REINDEX, VACUUM, ANALYZE** — операции обслуживания; при проблемах с индексом используют `REINDEX`.
* **CREATE INDEX CONCURRENTLY** — создание индекса без блокировки записи, но дороже и длительнее.

---

### Практические рекомендации

1. **Выбирайте тип по задаче:**

   * B-tree — «по умолчанию» для большинства задач.
   * GIN — массивы, `jsonb`, `tsvector`.
   * GiST/SP-GiST — геоданные, специальные поиски.
   * BRIN — экстремально большие таблицы с физической корреляцией.
2. **Не индексируйте всё подряд.** Подбирайте индексы по реальным нагрузкам и `EXPLAIN ANALYZE`.
3. **Используйте частичные и функциональные индексы** для уменьшения размера и повышения селективности.
4. **Проверяйте индекс-only scans и visibility map**, вакуумируйте для поддержки index-only.
5. **Следите за стоимостью вставок/обновлений.** Для частых обновлений больших строких индексов разумно минимизировать их число.
6. **Тестируйте план выполнения** — не предполагайте, что индекс будет использован автоматически.

---

## Чем отличается кластеризованный индекс от некластеризованного?

### Чем отличается кластеризованный индекс от некластеризованного (с примерами для PostgreSQL)

**Кратко:**

* *Кластеризованный индекс* — это индекс, по которому **физически** упорядочена таблица (листья индекса соответствуют порядку записей в таблице). В СУБД вроде SQL Server это встроенная и поддерживаемая структура (каждый кластеризованный индекс — это физический порядок хранения).
* *Некластеризованный индекс* — это отдельная структура (обычно B-tree), содержащая ключи и указатели на физические строки (heap TID). Таблица хранится независимо от порядка индекса; индекс указывает, где найти строки в heap.

В PostgreSQL понятие «кластеризованный индекс» реализовано иначе: есть команда `CLUSTER`, которая **однократно** перестраивает (реорганизует) таблицу на диске в порядке, заданном индексом. PostgreSQL не поддерживает автоматически поддерживаемый кластеризованный индекс, как, например, SQL Server; физический порядок в Postgres не поддерживается и не поддерживается автоматически при последующих вставках/обновлениях.

---

### Как это устроено технически

**Некластеризованный индекс (Postgres по умолчанию)**

* Индекс хранит пары `(ключ, ctid)`, где `ctid` — физический идентификатор строки в heap (блок/смещение).
* По индексу ищется `ctid`, затем выполняется доступ к heap по этому `ctid`, чтобы получить полную строку.
* На таблице можно иметь много некластеризованных индексов; все они автоматически поддерживаются при DML.

**Кластеризация (функция в PostgreSQL)**

* Команда `CLUSTER table USING index` физически перезаписывает таблицу в порядке, соответствующем указанному индексу. После этого записи с похожими значениями ключа будут лежать рядом на диске.
* Это перестроение — одномоментная операция (требует блокировки) и **не поддерживается автоматически**: со временем при вставках/обновлениях порядок теряется, и нужно запускать `CLUSTER` повторно (или использовать внешние инструменты вроде `pg_repack`).

---

### Примеры в PostgreSQL

Создаём таблицу и индекс:

```sql
CREATE TABLE orders (
  id serial PRIMARY KEY,
  customer_id int,
  created_at timestamptz,
  total numeric
);

CREATE INDEX idx_orders_customer ON orders (customer_id);
```

Кластеризовать таблицу по этому индексу (физически упорядочить):

```sql
CLUSTER orders USING idx_orders_customer;
-- или задать индекс как дефолтный для дальнейшего CLUSTER:
ALTER TABLE orders CLUSTER ON idx_orders_customer;
```

Проверить, какой индекс помечен как использовавшийся для кластеризации:

```sql
SELECT i.relname AS index_name, ix.indisclustered
FROM pg_class t
JOIN pg_index ix ON t.oid = ix.indrelid
JOIN pg_class i ON i.oid = ix.indexrelid
WHERE t.relname = 'orders';
```

Важно: `CLUSTER` не создаёт новый индекс — он использует уже существующий индекс для определения порядка и перезаписывает таблицу. После интенсивных DML-операций порядок снова нарушится.

---

### Отличия: поведение и последствия

| Аспект                                         |                                                      Кластеризованный (физический порядок по индексу) | Некластеризованный (обычный индекс в Postgres)                          |
| ---------------------------------------------- | ----------------------------------------------------------------------------------------------------: | ----------------------------------------------------------------------- |
| Физический порядок строк                       |                                                                Да (только после выполнения `CLUSTER`) | Нет                                                                     |
| Поддержка порядка при DML                      |                            **Нет** автоматической поддержки в Postgres; требуется повторный `CLUSTER` | Не требуется                                                            |
| Количество «кластеров» на таблицу              |                                      Только один физический порядок — значит, единственный «кандидат» | Можно иметь сколько угодно индексов                                     |
| Производительность диапазонных запросов        |                   Может значительно улучшить локальность чтения и уменьшить число I/O для range scans | Индексные сканы возможны, но чтение heap по ctids может быть разбросано |
| Стоимость вставок/обновлений                   |                             потенциально выше, если требуется поддерживать физический порядок вручную | обычная накладная на поддержание индексов                               |
| Поддержание (recluster)                        |                                           Требует периодического выполнения `CLUSTER` или `pg_repack` | не требует                                                              |

---

### Когда кластеризация даёт преимущество

* Таблица **большая**, и запросы часто выполняют **диапазонные запросы** по колонке (например, `WHERE customer_id = X ORDER BY created_at` или `WHERE date BETWEEN ...`) — тогда физическая близость строк уменьшает количество страниц, которые нужно прочитать.
* Таблица **в основном читается**, вставки происходят в основном в конец (append-heavy), и порядок не быстро теряется.
* Когда вы проводите сканирование с `ORDER BY` и хотите уменьшить random I/O: после `CLUSTER` чтение страниц будет более последовательным.

---

### Ограничения и подводные камни

1. **В PostgreSQL только одно «фактическое» упорядочивание таблицы** — этот порядок не поддерживается автоматически при изменениях. Со временем кластеризация устаревает.
2. **CLUSTER требует эксклюзивной блокировки** таблицы (в старых версиях); выполнение `CLUSTER` может нарушить доступность. (Можно использовать `pg_repack` для онлайн-реорганизации.)
3. **INSERT** новых строк не будут автоматически вставляться в «средине» таблицы, чтобы поддерживать порядок; они будут добавляться туда, куда ОС выделит место -> порядок со временем деградирует.
4. **Одно решение для одного шаблона запросов:** если у вас есть несколько шаблонов запросов по разным колонкам, вы не сможете одновременно физически оптимизировать таблицу под все. Для разных шаблонов нужны разные индексы/партиционирование/материализованные представления.
5. **CLUSTER не заменяет индекс-only scan**: index-only scan в Postgres зависит от visibility map (VACUUM), а не от кластеризации.

---

### Практические советы (Postgres)

* Для таблиц с **временной локальностью** (лог-файлы, события по времени) рассмотрите **BRIN-индекс**. BRIN даёт компактный индекс и эффективно работает для физически упорядоченных (append-only) таблиц. BRIN часто предпочтительнее частых `CLUSTER`-операций.
* Если вы хотите поддерживать физический порядок “онлайново” — используйте `pg_repack` (расширение), оно выполняет перепаковку без длительной блокировки.
* Если запросы требуют покрытия нескольких колонок, рассмотрите **INCLUDE**-индексы (`CREATE INDEX ... INCLUDE (...)`), чтобы уменьшить нужду в доступе к heap (index-only scan). Это снижает выигрыш от кластеризации, поскольку многие данные уже внутри индекса.
* Для OLTP систем обычно достаточно хорошей схемы индексов (несколько некластеризованных индексов). Кластеризация имеет смысл для аналитических/архивных таблиц или крупных range-select таблиц.
* Планируйте: если вы решили `CLUSTER`-ить таблицу, автоматизируйте периодический `CLUSTER`/`pg_repack` или убедитесь, что вставки не разрушают порядок быстро.

---

### Примеры сценариев

**Сценарий 1 — аналитическая таблица с диапазонными запросами по времени:**
Таблица `events` хранит данные в порядке вставки (по `created_at`). Для быстрых выборок по диапазонам времени лучше:

* либо периодически `CLUSTER events USING idx_events_created_at;`
* либо создать `BRIN` индекс: `CREATE INDEX ON events USING brin(created_at);` (меньший индекс, хороший для огромных таблиц)

**Сценарий 2 — OLTP таблица с большим количеством мелких транзакций по разным колонкам:**
Кластеризация не помогает, так как вставки/обновления быстро разрушают порядок; лучше оптимизировать набор некластеризованных индексов и рассмотреть покрывающие индексы (`INCLUDE`) по горячим запросам.

---

### Как понять, стоит ли кластеризовать

1. Проанализируйте планы запросов с `EXPLAIN (ANALYZE, BUFFERS)` для типичных диапазонных запросов — смотрите количество heap page fetches и random reads.
2. Если индексный запрос много раз делает random heap fetches, а кластеризация может сделать эти fetches последовательными — вероятно, выигрыш есть.
3. Оцените частоту вставок/обновлений: если они высоки и быстро разрушают порядок — выигрыша мало, если не планируется частый `CLUSTER`/repack.
4. Рассмотрите альтернативы: BRIN, партиционирование, покрывающие индексы, материализованные представления.

---

### Вывод

* **Кластеризованный индекс** — в общем смысле означает физический порядок таблицы по индексному ключу. В PostgreSQL это достигается командой `CLUSTER`, но этот порядок **не поддерживается автоматически** и требует периодической переработки.
* **Некластеризованный индекс** — стандартный индекс, который всегда указывает на записи таблицы (ctid) и автоматически поддерживается при DML; в Postgres это основной рабочий механизм индексации.
* Выбор между ними — компромисс: кластеризация даёт выгоду для range-запросов с длительным периодом покоя, но имеет накладные расходы на поддержание; в большинстве OLTP-сцен набор некластеризованных индексов и/или BRIN/партиционирование — более практичен.

---

## Чем отличаются типы данных JSON и JSONb?

В PostgreSQL типы данных **JSON** и **JSONB** оба предназначены для хранения данных в формате JSON, но они имеют принципиальные различия в способе хранения, производительности и функциональности.

---

### 1. **Формат хранения**

* **JSON**
  Данные хранятся в виде **текста**, полностью повторяя исходную строку, включая пробелы, порядок ключей и форматирование.
  Пример:

  ```sql
  CREATE TABLE test_json (data JSON);
  INSERT INTO test_json VALUES ('{ "name": "Alex", "age": 30 }');
  ```

  Здесь PostgreSQL хранит строку именно так, как она была вставлена.

* **JSONB**
  Данные хранятся в **двоичном, разобранном и нормализованном виде**. При сохранении PostgreSQL удаляет лишние пробелы, сортирует ключи и преобразует формат в более компактный и быстрый для обработки.
  Пример:

  ```sql
  CREATE TABLE test_jsonb (data JSONB);
  INSERT INTO test_jsonb VALUES ('{ "name": "Alex", "age": 30 }');
  ```

  Фактически в памяти и на диске хранится уже оптимизированная структура, а не строка.

---

### 2. **Скорость чтения и обработки**

* **JSON**
  При каждом обращении PostgreSQL **парсит** текст заново, что замедляет операции фильтрации, поиска и извлечения значений.

* **JSONB**
  Так как данные уже разобраны и хранятся в структурированном виде, операции чтения, фильтрации и поиска значительно быстрее.

---

### 3. **Поддержка индексов**

* **JSON**
  Не поддерживает индексацию напрямую. Для поиска придётся каждый раз парсить всё содержимое.

* **JSONB**
  Поддерживает индексы типа **GIN** и **GiST**, что позволяет быстро искать по ключам и значениям.
  Пример индекса GIN:

  ```sql
  CREATE INDEX idx_data_gin ON test_jsonb USING gin (data jsonb_path_ops);
  SELECT * FROM test_jsonb WHERE data @> '{"age": 30}';
  ```

---

### 4. **Поддержка операций**

* **JSON**
  Поддерживает операции извлечения значений (`->`, `->>`), но **не поддерживает** модификацию (например, удаление или добавление ключей).

* **JSONB**
  Помимо извлечения, поддерживает модификацию структуры JSON: добавление, удаление ключей, обновление значений.
  Пример:

  ```sql
  UPDATE test_jsonb
  SET data = jsonb_set(data, '{age}', '35'::jsonb);
  ```

---

### 5. **Сохранение исходного формата**

* **JSON**
  Сохраняет **точно** тот формат, который был вставлен, включая пробелы и порядок ключей.

* **JSONB**
  При сохранении всегда сортирует ключи и убирает лишние пробелы — оригинальное форматирование не сохраняется.

---

### 6. **Размер на диске**

* **JSON**
  Может занимать меньше места, если данные маленькие и не используются для частых операций поиска.

* **JSONB**
  Может занимать немного больше места из-за дополнительной структуры данных, но этот минус компенсируется ускорением поиска.

---

### 7. **Когда использовать**

* **JSON** — если:

  * Нужно сохранить оригинальное форматирование JSON (например, для логов или экспорта).
  * Данные редко читаются и почти не фильтруются.
* **JSONB** — если:

  * Нужно часто фильтровать, сортировать или модифицировать JSON-данные.
  * Нужна индексация для ускорения запросов.

---

## Можно ли строить индекс по JSON полям?

Да — в PostgreSQL можно строить индексы по полям JSON/JSONB. На практике для `jsonb` это делает поиск по ключам/значениям и полнотекстовые/containment-запросы быстрыми. Если же по какой-то причине вы не хотите или не можете использовать индексы, ниже перечислены альтернативы и компромиссы.

### Детали (PostgreSQL)

* `json` — хранится как текст; индексирование по нему практически бесполезно (нужен парсинг).
* `jsonb` — хранится в бинарном разобранном виде и предназначен для эффективных поисков; по `jsonb` можно строить специализированные индексы (наиболее полезный вариант для запросов по содержимому JSON).

### Типы индексов, которые обычно применяют к JSONB (коротко)

* **GIN** (`USING gin (col)` или `USING gin (col jsonb_path_ops)`) — лучший выбор для containment-запросов `col @> '{"k":"v"}'`, поиска по массивам и полнотекстовых `tsvector`-подобных запросов.
* **GiST / SP-GiST** — для специальных задач (пространственных/частичных совпадений, триграм-поиска через расширения).
* **Функциональные индексы** — индекс на выражение, например `CREATE INDEX ON t ((data->>'user_id'))` для быстрого поиска по конкретному извлечённому полю.
* **Частичные индексы** — индексируются только строки, удовлетворяющие условию `WHERE` (полезно для селективных кейсов `WHERE data->>'status' = 'active'`).

### Примеры (PostgreSQL)

Функциональный индекс (индексируем конкретное поле):

```sql
CREATE INDEX idx_orders_userid ON orders ((data->>'user_id'));
-- запрос использующий индекс:
SELECT * FROM orders WHERE (data->>'user_id') = '123';
```

GIN для containment (`@>`):

```sql
CREATE INDEX idx_docs_data_gin ON documents USING gin (data jsonb_path_ops);
-- поиск всех документов, где в JSON есть {"type":"invoice"}
SELECT * FROM documents WHERE data @> '{"type":"invoice"}';
```

### Что даст индекс и когда он нужен

* Индекс значительно ускорит операции `WHERE data @> ...`, `data->>'x' = 'y'`, `jsonb_exists`, полнотекст/pg\_trgm-LIKE по значениям.
* Если таблица маленькая или запросы редкие — можно не индексировать. Но для больших таблиц и частых фильтров по JSON-полям индексы критичны для производительности.

### Если индексы *нельзя* использовать — альтернативы и компромиссы

1. **Вынести часто фильтруемые поля в реляционные колонки**

   * Самый надёжный и быстрый способ: при загрузке/ETL извлечь ключи в отдельные колонки (`user_id`, `status`, `created_at`) и индексировать их (или использовать их без индекса, если невелика нагрузка).
   * Пример: `ALTER TABLE events ADD COLUMN user_id text; UPDATE events SET user_id = data->>'user_id';`

2. **Вычисляемые/постоянные столбцы (generated/computed column)**

   * В PostgreSQL можно хранить выражение в колонке вручную при write (или с `GENERATED` в новых версиях) — затем фильтровать по ней. Это даёт тот же удобный интерфейс, но без раздувания JSON-индексов.

3. **Материализованные представления (materialized views)**

   * Сформировать таблицу/материализованное представление, содержащее извлечённые поля и/или агрегаты, и обновлять его по расписанию. Подходит для аналитики и дешёвых запросов чтения.
   * Минус — сложность синхронизации/обновления.

4. **Денормализация и хранение «горячих» полей в отдельной таблице**

   * Выделить часто используемые поля в отдельную таблицу (one-to-one) и поддерживать через триггеры/ETL.

5. **Кеширование на уровне приложения**

   * Кешировать результаты тяжёлых JSON-фильтров в оперативной памяти/redis; подходит для часто повторяющихся запросов с низкой долей изменений.

6. **Партиционирование таблицы по логическому признаку**

   * Если фильтрация часто по какому-то атрибуту (например, `type`), можно партиционировать таблицу по этому атрибуту, чтобы запросы сканировали меньший набор партиций. Но чтобы партиционирование работало, этот атрибут лучше вынести в колонку.

7. **Последовательный скан + более эффективные выражения**

   * Оптимизировать выражения (использовать `->>` вместо сложных JSON-функций), уменьшить выборку столбцов; однако последовательный скан по большой таблице будет медленным.

### Практические рекомендации

* Если вы планируете часто фильтровать/поисковать по содержимому JSON — используйте `jsonb` и создавайте GIN/функциональные индексы.
* Если индексы недопустимы (ограничения политики, инфраструктуры), лучше **проектировать схему так, чтобы ключевые для фильтрации поля хранились в колонках**, либо использовать materialized views/денормализацию.
* Для ad-hoc фильтров по редким ключам — последовательный скан может быть приемлем; для производственных запросов на больших объёмах — индекс или денормализация практически обязательны.

---

## Есть ли ограничения на создание партицированной таблицы?

Коротко: да — в PostgreSQL для декларативного партиционирования есть ряд ограничений и особенностей, которые важно учитывать при проектировании (ограничения по ключам/ограничениям, по индексам, по структуре партиций и по операциям сопровождения). Ниже — подробный перечень с объяснениями, примерами и практическими обходными путями.

---

### 1. Ограничения по ключам и ограничениям (UNIQUE / PRIMARY KEY / EXCLUDE / FOREIGN KEY)

* **UNIQUE / PRIMARY KEY**
  На партиционированной таблице уникальные ограничения и первичные ключи накладываются «по-разному»: *столбцы ограничения обязаны включать все колонки partition key*, а сами partition key не должны быть выражениями/вызовами функций. Иначе сервер не позволит создать такое ограничение. Это связано с тем, что реальное поддержание уникальности делается индексами на отдельных партициях, и пересекающую уникальность между партициями СУБД по умолчанию не обеспечивает.

  Пример-следствие: если вы хотите ссылочную целостность (FK) на partitioned table, то PK в целевой таблице обычно придется делать составным и включать в него ключ партиционирования (см. раздел «варианты» ниже).

* **EXCLUDE**
  EXCLUDE-ограничения тоже подчиняются ограничению: они *должны включать все колонки partition key*, и для этих колонок сравнение должно быть через равенство (не, например, через `&&`). Невозможно объявить произвольное меж-партиционное исключение.

* **FOREIGN KEYS**
  Поддержка ссылочных ограничений на и с партиционированными таблицами улучшалась с версиями (существенное изменение появилось в PostgreSQL 12), но фактически FK наследует ограничения UNIQUE/PK — т.е. чтобы корректно ссылаться на partitioned table, ключи/индексы должны удовлетворять правилам (часто нужно включать partition key). Кроме того, наличие FK делает некоторые операции с партициями (detach/attach) и удаление партиций более сложными.

---

### 2. Индексы и создание индексов — блокировки и «конкурентность»

* **CREATE INDEX ON partitioned\_table автоматически создаёт соответствующие индексы на каждой партиции**, но создание индекса таким образом **не поддерживает ключевое слово `CONCURRENTLY`** — это может привести к длительным блокировкам. Как обходной путь рекомендовано: создать «недействительный» индекс на `ONLY parent` (он создаётся, но помечается invalid), затем создать индексы `CONCURRENTLY` на каждой партиции и прикрепить их с помощью `ALTER INDEX ... ATTACH PARTITION`; после того, как все под-индексы присоединены, родительский индекс помечается валидным.

* **Нет «глобального» (single global) индекса**, охватывающего все партиции как единое целое — в PostgreSQL индексы по сути локальные для каждой партиции (родительский индекс «виртуален»). Это означает, что запросы, которые не используют партиционный ключ, могут оказаться менее оптимальными, т.к. planner не сможет опираться на один глобальный индекс.

---

### 3. Ограничения по структуре таблиц / колонок / наследованию

* **Партиции обязаны иметь точно тот же набор колонок, что и parent**; нельзя добавить колонки только в конкретную партицию, нельзя у партиции быть дополнительных колонок, отсутствующих у родителя. Также нельзя делать таблицу, которая одновременно наследует и от partitioned table, и от обычной таблицы — декларативные партиции не смешиваются с произвольной иерархией наследования.

* **CHECK и NOT NULL ограничения родителя наследуются всеми партициями**, и нельзя создавать `CHECK` с NO INHERIT для partitioned table. Невозможно снять NOT NULL на колонке партиции, если у родителя оно присутствует.

* **Нельзя «превратить» обычную таблицу в partitioned table (и обратно) напрямую** — нужно создать новую partitioned table и переносить/прикреплять данные/таблицы.

---

### 4. Триггеры / ON CONFLICT / поведение вставок/обновлений

* **`BEFORE ROW` триггер на INSERT не может менять конечную партицию**, то есть триггер не должен изменять значения так, чтобы итоговая строка уходила в другую партицию. (Row-routing выполняется внутренне и триггер не может перенаправлять.)

* **`INSERT ... ON CONFLICT` и `ON CONFLICT DO UPDATE` с партиционированными таблицами** имеют ограничения: например, `DO UPDATE` не поддерживает изменение partition key так, чтобы строка перешла в другую партицию (в этом случае будет ошибка). В целом поведение UPSERT на partitioned tables исторически было ограничено и постепенно улучшалось в новых версиях — но надо внимательно читать документацию для вашей версии.

---

### 5. Операции сопровождения: attach/detach, drop, locks

* **ATTACH PARTITION** может потребовать сканирования таблицы-партии для проверки её содержимого против границ; это сканирование может держать `ACCESS EXCLUSIVE` или другие сильные блокировки (в разных режимах/версиях поведение улучшалось — есть варианты `CONCURRENTLY` для DETACH и т.п.). Чтобы избежать длительного сканирования/блокировки, рекомендуется заранее добавлять `CHECK`-ограничение, соответствующее будущим границам, на прикрепляемую таблицу.

* **DROP PARTITION** — очень быстрый способ удалить данные (удаляется вся таблица-партиция), но сам DROP может требовать `ACCESS EXCLUSIVE` на родителя. Планируйте операции удаления/замены партиций аккуратно.

* **TRUNCATE ONLY parent\_table** — на партиционированной таблице это вызовет ошибку (parent не хранит данных), поэтому операции `TRUNCATE` следует применять к партициям или без `ONLY`.

---

### 6. Ограничения времён/сессий и временных таблиц

* **Нельзя смешивать временные и постоянные таблицы в одной и той же иерархии партиций**. Если parent — permanent, все партиции должны быть permanent; если parent — temporary, то все партиции должны быть temporary и принадлежат одной сессии.

---

### 7. Практические/эффективностные ограничения (сколько партиций можно иметь)

* **Нет «жёсткого» числа партиций в спецификации**, но на практике большое число партиций (тысячи и более) может привести к существенным накладным издержкам: планирование запроса становится дороже (planner просматривает список партиций), операции DDL (создание/удаление/обновление индексов, ATTACH/DETACH) становятся дорогими, может расти потребление метаданных в shared buffers/lock manager и т.д. В современных версиях Postgres масштабируемость партиций улучшена, но всё равно рекомендуется держать число партиций в разумных пределах и проектировать партиционирование осмысленно (например, комбинировать hash/range, использовать subpartitioning лишь при необходимости).

---

### 8. Итог — практические рекомендации и обходы

1. **Проектируйте схему с учётом ограничений ключей.** Если вам нужна уникальность/PK и FK, подумайте о включении partition key в PK/уникальный индекс (композиция `(partition_key, id)`), или реализуйте логику уникальности на уровне приложения/ETL, если включение partition key неприемлемо.

2. **Индексы:** если вам нужно создавать индекс без блокировок — создавайте индексы `CONCURRENTLY` на партициях и затем `ATTACH` к родителю (см. выше). Это стандартный рабочий паттерн.

3. **ON CONFLICT / UPSERT:** протестируйте UPSERT-сценарии в вашей версии Postgres; не пытайтесь в `DO UPDATE` менять partition key (это не поддерживается).

4. **Автоматизация и инструменты:** для управления жизненным циклом партиций (создание/удаление новых партиций, ретеншн) используйте проверенные инструменты и расширения (pg_partman и т.п.) или собственные скрипты, и по возможности автоматизируйте процедуру валидации/ATTACH.

5. **Мониторинг количества партиций:** если у вас сотни/тысячи партиций, измерьте влияние на планирование и DDL; при необходимости пересмотрите стратегию (более крупные диапазоны, hash-partition, субпартиционирование с осторожностью или шардинг).

---

## Чем отличаются материализованное и нематериализованное представления?

* **Нематериализованное представление (VIEW)** — это **виртуальная таблица**: определение представления — это просто именованный SQL-запрос. При обращении к view PostgreSQL подставляет и выполняет исходный запрос над актуальными данными таблиц-источников. View не занимает места для результатов и всегда показывает текущее состояние данных.
* **Материализованное представление (MATERIALIZED VIEW)** — это **физически сохранённый результат** запроса: результат выполнения запроса сохраняется в отдельной таблице на диске. Запрос к материализованному представлению читает сохранённые данные (быстро), но они могут устаревать — обновление (REFRESH) должно выполняться явно.

---

### Поведение и свойства

#### 1) Актуальность данных

* **VIEW**: всегда актуально — результат строится на лету из текущих данных источников.
* **MATERIALIZED VIEW**: может быть устаревшим (stale). Требуется `REFRESH MATERIALIZED VIEW`, чтобы актуализировать содержимое.

#### 2) Производительность чтения

* **VIEW**: каждая выборка выполняет базовый запрос → при сложных вычислениях/агрегациях может быть медленно.
* **MATERIALIZED VIEW**: чтение очень быстрое (как чтение из таблицы). Особенно выгодно, если запрос дорогой и результаты не требуется обновлять мгновенно.

#### 3) Место на диске и IO

* **VIEW**: не хранит данные, дополнительного дискового пространства не требует.
* **MATERIALIZED VIEW**: хранит результат на диске — требует места и IO при создании/обновлении.

#### 4) Индексы

* **VIEW**: нельзя создать индекс на самом view; индексы создаются на базовых таблицах и используются планировщиком при раскрытии view.
* **MATERIALIZED VIEW**: это полноценная таблица — на неё можно создавать индексы (включая уникальные), что позволяет ускорять конкретные запросы по матвью.

#### 5) Блокировки и совпадение работы / Refresh

* **VIEW**: нет операции refresh.
* **MATERIALIZED VIEW**: `REFRESH MATERIALIZED VIEW` пересоздаёт/обновляет содержимое.

  * По умолчанию `REFRESH` берёт эксклюзивные блокировки на материализованную view (чтения могут быть заблокированы на время).
  * В PostgreSQL есть опция `CONCURRENTLY` (`REFRESH MATERIALIZED VIEW CONCURRENTLY ...`), позволяющая обновлять матвью без длительной блокировки чтений, но она имеет ограничения: требуется уникальный индекс, и обновление «конкурентно» потребляет больше ресурсов (создаётся новая копия и затем выполняется atomic swap). (Перед использованием CONCURRENTLY проверьте требования версии и условия.)

#### 6) Updatable / INSERT/UPDATE/DELETE

* **VIEW**: некоторые простые view могут быть обновляемыми (INSERT/UPDATE/DELETE) — существует набор правил, когда view поддерживает DML (простые проекции над одной таблицей и т.д.). Можно использовать `INSTEAD OF` триггеры для реализации логики записи.
* **MATERIALIZED VIEW**: обычно **не поддерживает DML** (нельзя напрямую вставлять/обновлять строки в matview — это просто таблица, но обычно не предполагается поддерживать её вручную; можно теоретически делать `TRUNCATE`/`INSERT` если нужно, но это нестандартно). Рекомендуемый способ поддержания — `REFRESH` или поддержка вручную (ETL).

#### 7) Планирование и статистика

* **VIEW**: оптимизатор «инлайнит» запрос и строит план по базовым таблицам, учитывая актуальные статистики.
* **MATERIALIZED VIEW**: после `REFRESH` желательно `ANALYZE` (или VACUUM ANALYZE), чтобы сбор статистики был свежим; индексы на matview помогают планировщику.

---

### Примеры (PostgreSQL)

#### View (нематериализованное)

```sql
CREATE VIEW sales_summary AS
SELECT product_id, SUM(amount) AS total
FROM sales
GROUP BY product_id;

-- Запрос выполняет агрегирование при каждом вызове
SELECT * FROM sales_summary WHERE total > 1000;
```

#### Materialized view

```sql
CREATE MATERIALIZED VIEW sales_summary_mat AS
SELECT product_id, SUM(amount) AS total
FROM sales
GROUP BY product_id
WITH NO DATA;  -- можно создать пустое и заполнить позже

-- заполняем/обновляем
REFRESH MATERIALIZED VIEW sales_summary_mat;

-- можно индексировать
CREATE INDEX idx_sales_summary_mat_product ON sales_summary_mat(product_id);

-- затем быстрые выборки
SELECT * FROM sales_summary_mat WHERE total > 1000;
```

Для конкурентного обновления (при наличии уникального индекса):

```sql
REFRESH MATERIALIZED VIEW CONCURRENTLY sales_summary_mat;
```

---

### Ограничения и тонкости (важные детали)

* **REFRESH может быть дорогой** (полный пересчёт). Для больших matview это может занять значительное время и ресурсы.
* **REFRESH CONCURRENTLY**:

  * Требует уникального индекса над материализованным представлением.
  * Потребляет больше дискового пространства и может быть медленнее, но позволяет не блокировать чтение.
* **Автоматическое поддержание отсутствует**: PostgreSQL не обновляет matview автоматически при изменении исходных таблиц — нужно настроить расписание (cron, pg\_cron, CI/CD или ETL), либо реализовать свою логику обновления (триггеры + поддерживающие таблицы, но это сложнее).
* **Размеры и IO**: matview хранит копию данных — резервирование диска и резервные копии увеличиваются.
* **Также**: нельзя создать индекс на view; нельзя напрямую управлять планировщиком для inline view; но view удобны для безопасности (GRANT), абстракции и упрощения запросов.

---

### Когда использовать что (рекомендации)

#### Используйте **VIEW**, если:

* Нужно всегда получать **актуальные** данные.
* Запросы относительно просты или источники индексированы так, что вычисление «на лету» приемлемо по скорости.
* Нужна лёгкая абстракция/инкапсуляция логики/права доступа (например, row-level security, ограниченные проекции).
* Вы не хотите дополнительных расходов на хранение и обслуживание.

#### Используйте **MATERIALIZED VIEW**, если:

* Исходный запрос **дорогой** (сложные агрегаты, джоины по большим таблицам) и часто повторяется.
* Допустима некоторая **задержка актуализации** (staleness), и выгода от быстрого чтения перевешивает стоимость обновления.
* Нужны индексы по результатам агрегации для ускорения специфичных запросов.
* Вы готовы управлять жизненным циклом matview (планировать REFRESH, мониторить размеры и время обновления).

---

### Практические best practices

1. **Планируйте политику обновления**: как часто вы будете `REFRESH` (каждые N минут/часов, по расписанию, по событию)? Автоматизируйте (pg\_cron, cron + psql, фоновые worker).
2. **Индексируйте matview** под реальные запросы; для `REFRESH CONCURRENTLY` создайте уникальный индекс.
3. **Мониторьте время REFRESH** и IO/CPU; проверяйте влияние на систему.
4. **Если нужен near-real-time** — рассмотрите summary tables с incremental поддержанием или Kafka/streaming ETL.
5. **Анализируйте планы**: иногда обычный view вместе с правильными индексами на базовых таблицах даёт приемлемую производительность, и matview не требуется.

---

## Можно ли читать данные из материализованного представления, когда выполняется команда REFRESH?

Короткий ответ: **можно — но не всегда**. Поведение зависит от того, как вы запускаете `REFRESH MATERIALIZED VIEW`.

Детально.

---

### 1) `REFRESH MATERIALIZED VIEW` (без `CONCURRENTLY`)

По умолчанию команда берёт жесткую блокировку на материализованное представление и **блокирует чтение** до завершения обновления.

Что именно происходит:

* `REFRESH` пытается взять блокировку уровня, достаточную для изменения содержимого (в терминах PostgreSQL — это конфликт с обычными SELECT-блокировками).
* Если в данный момент выполняются SELECT’ы, которые уже захватили свои блокировки, `REFRESH` будет ждать, пока они не завершатся. Если же `REFRESH` уже захватил блокировку, новые SELECT’ы будут ждать, пока `REFRESH` не закончится. Иными словами, в период удержания блокировки чтение таблицы будет недоступно (блокировано) для других сессий.
* Практический эффект: пока идёт обычный `REFRESH`, другие сессии не смогут безопасно выполнять SELECT над матвью — либо их запросы будут ждать, либо, если реализована очередь блокировок, они будут заблокированы до окончания `REFRESH`.

Пример:

```sql
REFRESH MATERIALIZED VIEW sales_summary;
-- в это время SELECT * FROM sales_summary; будет ждать до завершения REFRESH
```

---

### 2) `REFRESH MATERIALIZED VIEW CONCURRENTLY` — можно читать во время обновления

Если вы выполните `REFRESH ... CONCURRENTLY`, то материализованное представление **читаемо в процессе обновления**: текущие SELECT’ы увидят старое содержимое, пока идёт обновление, а после успешного завершения `REFRESH CONCURRENTLY` новые SELECT’ы начнут видеть обновлённые данные.

Особенности и ограничения `CONCURRENTLY`:

* **Не блокирует чтения** в течение фактической работы (занимает только очень короткие «swap»–блокировки в конце). Это делает его подходящим для систем с высокими требованиями к доступности.
* **Требует уникального индекса** на материализованном представлении. Без уникального индекса `REFRESH ... CONCURRENTLY` выдаст ошибку. (Причина — алгоритм конкурентного обновления использует уникальный индекс для корректного объединения/сопоставления строк.)
* **Нельзя выполнять внутри явной транзакции**: `REFRESH MATERIALIZED VIEW CONCURRENTLY` запрещён в блоке `BEGIN ... COMMIT`.
* **Медленнее и дороже по ресурсам**: concurrent-refresh обычно расходует больше IO/времени и временного дискового пространства, потому что строит новую копию данных и затем атомарно переключает её с существующей.
* **Короткая блокировка при переключении**: в конце есть небольшая критическая секция, где новый результат «вставляется» в место старого — это кратковременная операция, но она очень короткая по времени по сравнению с полным `REFRESH`-блокированием.

Пример:

```sql
-- требуется уникальный индекс, например:
CREATE UNIQUE INDEX idx_sales_summary_product ON sales_summary(product_id);

-- затем:
REFRESH MATERIALIZED VIEW CONCURRENTLY sales_summary;
-- в это время SELECT * FROM sales_summary; будут возвращать старые данные до завершения REFRESH
```

---

### 3) Что происходит с уже запущенными SELECT’ами?

* Если SELECT начался *до* того, как `REFRESH` попытался взять блокировку, этот SELECT продолжит работать в своём транзакционном «снимке» и вернёт данные, которые были видимы для него на момент начала транзакции. `REFRESH` будет ждать освобождения необходимых блокировок.
* Если SELECT начнётся *после* того, как обычный `REFRESH` (без CONCURRENTLY) уже взял блокировку — SELECT будет заблокирован до окончания `REFRESH`.
* При `REFRESH CONCURRENTLY` новые SELECT’ы увидят старые данные пока идёт обновление; после успешного завершения они увидят новые данные.

---

### 4) Ограничения и практические нюансы

* `REFRESH ... CONCURRENTLY` требует уникального индекса и не поддерживается внутри транзакций; также он может быть значительно медленнее и требовать больше временного места.
* Обычный `REFRESH` проще и может быть быстрее (в некоторых случаях), но он **делает матвью недоступным для чтения на всё время обновления**.
* Если у вас потоковые/долгоживущие чтения, обычный `REFRESH` может вызывать блокировки и задержки; в таких системах предпочтителен `CONCURRENTLY` или иной подход к поддержанию актуальности данных.

---

### 5) Альтернативы и шаблоны, если `CONCURRENTLY` непригоден

Если `CONCURRENTLY` нельзя использовать (нет уникального индекса, или вы по каким-то причинам не можете), можно рассмотреть несколько подходов:

* **Двойной буфер / атомарный swap**: вычислять новый результат в отдельную таблицу (CREATE TABLE AS или CREATE MATERIALIZED VIEW temp), создать на ней индексы, и затем в короткой транзакции переименовать таблицы/матрицы (atomic `ALTER TABLE RENAME`) — это даёт короткое окно блокировки, но обеспечивает почти-непрерывный доступ. Учтите, что и переименование требует кратковременных блокировок.
* **Инкрементальное поддержание** (triggers, логика ETL): поддерживать агрегаты/результаты при вставках/обновлениях в исходных таблицах — это сложнее в реализации, но даёт всегда-свежие данные без массовых пересчётов.
* **Материальные таблицы и регулярный swap через скрипт**: скрипт строит новую таблицу, создает индексы, затем в короткой операции переименовывает (swap) — схоже с двойным буфером.
* **Планирование REFRESH в «окна» низкой нагрузки** — если согласны прерывания, запускать обычный `REFRESH` ночью.

---

## Как удалить дубликаты из таблицы?

### 1. Подготовка — сначала **найти** дубликаты и решить, какую строку оставить

Перед удалением обязательно определите критерий "дубликата" (по каким столбцам считаем строки одинаковыми) и правило, какую из одинаковых строк оставить (минимальный `id`, последняя по `ts`, произвольную и т. п.).

Пример: найти группы с дубликатами по `(col1, col2)`:

```sql
SELECT col1, col2, COUNT(*) AS cnt
FROM mytable
GROUP BY col1, col2
HAVING COUNT(*) > 1;
```

Посмотреть сами «дубли»:

```sql
SELECT *
FROM mytable t
WHERE (t.col1, t.col2) IN (
  SELECT col1, col2
  FROM mytable
  GROUP BY col1, col2
  HAVING COUNT(*) > 1
)
ORDER BY col1, col2;
```

---

### 2. Удаление дубликатов — безопасные варианты (PostgreSQL)

#### Вариант A — `DELETE` через CTE + `ROW_NUMBER()` (универсальный и понятный)

Оставляем одну строку (rn = 1), удаляем остальные:

```sql
WITH duplicates AS (
  SELECT id,
         ROW_NUMBER() OVER (
           PARTITION BY col1, col2           -- ключи дублирования
           ORDER BY id                       -- критерий, какую строку держать
         ) AS rn
  FROM mytable
)
DELETE FROM mytable
USING duplicates
WHERE mytable.id = duplicates.id
  AND duplicates.rn > 1;
```

Плюсы: гибкий (можно менять `ORDER BY` — keep newest/oldest). Минусы: требует уникального `id` (PK).

#### Вариант B — self-join `DELETE` (классический)

Удаляем строки с `id` больше, оставляя минимальный `id`:

```sql
DELETE FROM mytable a
USING mytable b
WHERE a.id > b.id
  AND a.col1 IS NOT DISTINCT FROM b.col1
  AND a.col2 IS NOT DISTINCT FROM b.col2;
```

Замечание: `IS NOT DISTINCT FROM` корректно сравнивает `NULL` как равные; если NULL-значений нет, можно `a.col1 = b.col1` и т.д.

#### Вариант C — `DELETE` с `GROUP BY` и `MIN(id)` (альтернатива)

Собираем `keep_id` и удаляем остальные:

```sql
DELETE FROM mytable t
USING (
  SELECT col1, col2, MIN(id) AS keep_id
  FROM mytable
  GROUP BY col1, col2
) s
WHERE t.col1 IS NOT DISTINCT FROM s.col1
  AND t.col2 IS NOT DISTINCT FROM s.col2
  AND t.id <> s.keep_id;
```

#### Вариант D — нет PK: использовать `ctid`

Если у таблицы нет уникального `id`, можно оперировать `ctid` (временная физическая позиция строки):

```sql
WITH to_delete AS (
  SELECT ctid
  FROM (
    SELECT ctid,
           ROW_NUMBER() OVER (PARTITION BY col1, col2 ORDER BY ctid) AS rn
    FROM mytable
  ) x
  WHERE rn > 1
)
DELETE FROM mytable
WHERE ctid IN (SELECT ctid FROM to_delete);
```

Важно: `ctid` меняется при `VACUUM FULL` или UPDATE, но для одной операции удаления годится.

---

### 3. Удаление в больших таблицах — эффективные варианты

#### Подход 1 — delete батчами (чтобы не держать длинную транзакцию)

Большие `DELETE` создают много WAL и могут заблокировать таблицу. Выполняйте в цикле небольшими партиями:

```sql
-- пример: удаляем до 10k дублей за итерацию
WITH duplicates AS (
  SELECT id
  FROM (
    SELECT id,
           ROW_NUMBER() OVER (PARTITION BY col1, col2 ORDER BY id) AS rn
    FROM mytable
  ) t WHERE rn > 1
  LIMIT 10000
)
DELETE FROM mytable WHERE id IN (SELECT id FROM duplicates);
```

Запускать цикл до тех пор, пока подзапрос возвращает строки.

#### Подход 2 — rebuild (копировать уникальные строки в новую таблицу и swap)

Если таблица очень большая и удаляется большой процент строк — часто быстрее создать новую таблицу без дублей, индексировать её, затем переименовать (атомарный swap) — это убирает проблему bloat и быстрый:

```sql
-- 1. создать таблицу с не дубликатами. Пример: оставить последнюю по ts
CREATE TABLE mytable_clean AS
SELECT DISTINCT ON (col1, col2) *
FROM mytable
ORDER BY col1, col2, ts DESC;  -- сохраняем последнюю по ts

-- 2. создать индексы и ограничения на mytable_clean
CREATE INDEX ... ON mytable_clean (...);
-- 3. в транзакции переименовать таблицы
BEGIN;
ALTER TABLE mytable RENAME TO mytable_old;
ALTER TABLE mytable_clean RENAME TO mytable;
COMMIT;
-- 4. удалить старую таблицу, после проверки
DROP TABLE mytable_old;
```

Важные предупреждения:

* Если на таблице есть внешние ключи ссылающиеся на неё — swap ломает FK (имена объектов меняются, но внешние ссылки указывают на старую таблицу). Обмен таблицами безопасен только если на таблице нет внешних ссылок или вы готовы обновить их.
* Нужно корректно восстановить последовательности (`setval`) для serial/identity.
* Этот метод экономит время и уменьшает фрагментацию, но требует дополнительного дискового пространства на время операции.

---

### 4. Создать уникальный индекс после удаления — предотвращение повторного возникновения

После удаления дубликатов создайте уникальный индекс, чтобы запретить появление новых дубликатов:

```sql
CREATE UNIQUE INDEX CONCURRENTLY ux_mytable_col1_col2 ON mytable (col1, col2);
```

`CONCURRENTLY` создаёт индекс без долгой блокировки вставок (но требует, чтобы не было existing duplicates; иначе команда провалится).

---

### 5. Особые моменты и подводные камни

* **NULL-поля:** сравнение `=` не равнозначно `IS NOT DISTINCT FROM` для `NULL`-значений. При наличии `NULL` используйте `IS NOT DISTINCT FROM` в условиях соединения/удаления или обработайте через `COALESCE`.
* **Внешние ключи:** нельзя просто удалить строки, на которые ссылаются другие таблицы; нужно либо удалить/обновить ссылки, либо использовать `ON DELETE CASCADE` (если это допустимо). При swap-методе следите за внешними ссылками — они останутся ссылаться на старую таблицу.
* **Триггеры:** `DELETE` вызовет триггеры; если они дорогие — это повлияет на скорость.
* **Долгие транзакции и bloat:** большие удаления приводят к «разрастанию» таблицы (bloat). После массового удаления выполняйте `VACUUM`/`VACUUM FULL` или используйте `pg_repack` для освобождения места.
* **WAL и репликация:** массовые удаления генерируют много WAL; учтите нагрузку на репликацию/логирование.
* **Бэкап:** перед массовыми DELETE всегда делайте бэкап или работайте на тестовом окружении сначала.
* **Проверка планов:** используйте `EXPLAIN (ANALYZE, BUFFERS) DELETE ...` или предварительно `EXPLAIN` SELECT-части, чтобы понять стоимость.

---

### 6. Примеры «по задаче»

#### Оставить самую новую запись по `created_at` для каждой пары `(user_id, event_type)`:

```sql
WITH dup AS (
  SELECT id,
         ROW_NUMBER() OVER (
           PARTITION BY user_id, event_type
           ORDER BY created_at DESC, id
         ) rn
  FROM events
)
DELETE FROM events e
USING dup
WHERE e.id = dup.id AND dup.rn > 1;
```

#### Удалить строки, если у нескольких строк одинаковые все колонки кроме `id`:

```sql
WITH cte AS (
  SELECT id,
         ROW_NUMBER() OVER (PARTITION BY col1, col2, col3 ORDER BY id) rn
  FROM mytable
)
DELETE FROM mytable USING cte WHERE mytable.id = cte.id AND cte.rn > 1;
```

#### Быстрое (но менее рекомендованное) удаление с `NOT IN`:

```sql
DELETE FROM mytable
WHERE id NOT IN (
  SELECT MIN(id)
  FROM mytable
  GROUP BY col1, col2
);
```

Минусы: `NOT IN` может иметь проблемы с `NULL` и производительностью; предпочтительнее `USING`-варианты или `ROW_NUMBER()`.

---

## Какой объявить СТЕ? Можно ли в одной таблице применить несколько СТЕ?

СТЕ — это временный результат запроса, определённый в пределах одного SQL-запроса, который можно переиспользовать несколько раз внутри этого запроса. В SQL выражается с помощью конструкции `WITH`.

**Назначение СТЕ:**

* Сделать запросы более читаемыми и структурированными.
* Избежать дублирования кода при повторном использовании одного и того же подзапроса.
* Упростить написание рекурсивных запросов (рекурсивные СТЕ).
* Иногда помогает оптимизатору понять логику запроса.

---

### Как объявить СТЕ?

Пример базового синтаксиса:

```sql
WITH cte_name AS (
    -- Подзапрос, возвращающий набор строк
    SELECT column1, column2
    FROM some_table
    WHERE condition
)
SELECT *
FROM cte_name
WHERE column1 > 100;
```

Объяснение:

* `WITH cte_name AS` — объявляет CTE с именем `cte_name`.
* В скобках — запрос, который формирует временный результат.
* После объявления `WITH` следует основной запрос, который может использовать `cte_name` как временную таблицу.

---

### Можно ли в одной таблице применить несколько СТЕ?

Термин «применить несколько СТЕ в одной таблице» в классическом понимании немного некорректен, так как СТЕ — это не объект таблицы, а часть запроса. Тем не менее, можно:

* **Объявить несколько СТЕ в одном SQL-запросе**. В этом случае СТЕ объявляются через запятую, по цепочке.
* Эти СТЕ можно использовать во внутреннем основном запросе или между собой (последовательное использование).

Пример с несколькими СТЕ:

```sql
WITH
cte1 AS (
    SELECT id, name FROM users WHERE active = true
),
cte2 AS (
    SELECT user_id, COUNT(*) AS order_count FROM orders GROUP BY user_id
)
SELECT cte1.id, cte1.name, cte2.order_count
FROM cte1
LEFT JOIN cte2 ON cte1.id = cte2.user_id;
```

Здесь два СТЕ объявлены (`cte1` и `cte2`) и затем используются в основном запросе.

---

### Можно ли использовать несколько СТЕ с рекурсией?

Да, можно объявить несколько рекурсивных и нерекурсивных СТЕ вместе, например:

```sql
WITH RECURSIVE cte_recursive AS (
    -- рекурсивный запрос
),
cte_non_recursive AS (
    -- нерекурсивный запрос
)
SELECT ...
```

---

### Можно ли использовать СТЕ в DML (INSERT, UPDATE, DELETE)?

Да, во многих СУБД, включая PostgreSQL, можно использовать СТЕ в запросах типа `UPDATE`, `DELETE`, `INSERT`, чтобы упростить логику и избежать повторного написания подзапросов.

Пример:

```sql
WITH old_data AS (
    SELECT id FROM mytable WHERE status = 'obsolete'
)
DELETE FROM mytable
USING old_data
WHERE mytable.id = old_data.id;
```

---

## Как оптимизируется запрос?

Оптимизация — это процесс преобразования запроса в такую форму, которая позволяет выполнить его максимально эффективно, используя минимальное количество ресурсов (времени процессора, памяти, ввода-вывода).

---

### 2. Компоненты оптимизации

#### 2.1. Парсинг и анализ

На этом этапе СУБД проверяет синтаксис запроса, анализирует его семантику и преобразует SQL в внутреннее представление (например, дерево операций).

#### 2.2. Генерация плана выполнения

Оптимизатор строит один или несколько планов выполнения — последовательностей операций, которые выполнят запрос. Например, какие индексы использовать, как объединять таблицы (join), как фильтровать данные.

#### 2.3. Выбор оптимального плана

Оптимизатор оценивает стоимость каждого плана (время CPU, количество операций чтения диска и т.д.) и выбирает наиболее эффективный.

---

### 3. Основные методы оптимизации запросов

#### 3.1. Использование индексов

* Индексы позволяют быстро находить нужные строки без полного сканирования таблицы (full table scan).
* Оптимизатор выбирает подходящий индекс для условия `WHERE`, соединений (`JOIN`), сортировки (`ORDER BY`).

#### 3.2. Фильтрация на ранних этапах

* Чем раньше данные отфильтровать (например, применить условие `WHERE`), тем меньше объём последующих операций.
* Оптимизатор старается "перенести" фильтры как можно ближе к источникам данных.

#### 3.3. Выбор эффективных алгоритмов соединения

* СУБД выбирает подходящий алгоритм `JOIN` — вложенные циклы (Nested Loop), сортировочные объединения (Sort Merge Join), хеш-объединения (Hash Join), исходя из объёма данных и наличия индексов.
* Например, для маленькой таблицы и большой — Nested Loop, для больших — Hash Join.

#### 3.4. Упрощение и преобразование выражений

* Оптимизатор упрощает арифметические и логические выражения, предвычисляет константы.
* Убирает избыточные операции, дублирующиеся подзапросы.

#### 3.5. Преобразование подзапросов

* Подзапросы могут быть переписаны в более эффективные JOIN’ы или объединения.
* Иногда используется «дешевое» материализованное промежуточное хранение.

#### 3.6. Использование статистики

* Оптимизатор опирается на статистические данные о таблицах: количество строк, распределение значений, наличие NULL и т.д.
* Статистика обновляется командами `ANALYZE` и влияет на выбор плана.

#### 3.7. Параллелизм

* В современных СУБД (например, PostgreSQL, Greenplum, Spark) запрос может выполняться параллельно — разбиваться на несколько потоков или процессов, чтобы ускорить обработку больших объёмов.

---

### 4. Роль индексов в оптимизации

* Индексы — ключ к быстрой выборке.
* Оптимизатор оценивает селективность условия и решает, стоит ли использовать индекс или проще сделать полный скан.
* Сложные запросы могут использовать несколько индексов и комбинировать результаты.

---

### 5. Как работает оптимизатор?

* Оптимизатор может быть **костным (rule-based)** — применяет жёсткие правила, или **стоимостным (cost-based)** — выбирает план с минимальной оценочной стоимостью.
* Современные СУБД используют преимущественно стоимостные оптимизаторы.

---

### 6. Влияние конструкции запроса

* Запросы с явными JOIN’ами, правильно оформленными условиями и фильтрами позволяют оптимизатору выбирать более эффективные планы.
* Использование функций, подзапросов и выражений может затруднять оптимизацию.

---

### 7. Способы улучшить оптимизацию запросов вручную

* Добавлять и поддерживать статистику актуальной (`ANALYZE`).
* Создавать и использовать индексы для часто используемых столбцов.
* Переписывать сложные запросы на более простые или использовать материализованные представления.
* Использовать явные подсказки (hints), если СУБД это поддерживает, чтобы направить оптимизатор.
* Проверять планы выполнения (`EXPLAIN`, `EXPLAIN ANALYZE`) и искать «узкие места» (например, полный скан, Nested Loop на больших таблицах).

---

### 8. Пример: оптимизация запроса с JOIN

Исходный запрос:

```sql
SELECT *
FROM orders o
JOIN customers c ON o.customer_id = c.id
WHERE c.country = 'USA';
```

Оптимизатор может:

* Использовать индекс по `customers.country` для быстрого поиска клиентов из США.
* Для найденных клиентов использовать индекс по `orders.customer_id` для быстрого доступа к заказам.
* Выбрать эффективный тип JOIN (Hash Join, если обе таблицы большие).

---

## Что будете делать, если в плане запроса увидели Nested Loop?

Nested Loop — это один из алгоритмов соединения (JOIN) в реляционных базах данных. Его суть:

* Для каждой строки из внешней (внешней по порядку) таблицы выполняется поиск соответствующих строк во внутренней таблице.
* В простейшем случае это двойной цикл: внешний цикл перебирает строки первой таблицы, внутренний — ищет совпадения во второй.

---

### Когда Nested Loop эффективен?

* Когда внешняя таблица содержит небольшое количество строк (например, ограничение `WHERE` сильно сузило выборку).
* Когда для внутренней таблицы есть подходящий индекс по полю соединения, что позволяет быстро находить нужные строки.
* Когда ожидаемый размер результата небольшой.

---

### Когда Nested Loop может быть проблемой?

* Если внешняя таблица большая и нет индекса по ключу соединения во внутренней таблице — происходит полное сканирование второй таблицы для каждой строки первой.
* При большом объёме данных Nested Loop может быть очень медленным, вызывая большие накладные расходы по времени и ресурсам.

---

### Какие действия предпринять, если увидели Nested Loop в плане?

#### 1. Проанализировать объёмы данных

* Узнать, сколько строк обрабатывается во внешней и внутренней таблицах.
* Если количество строк большое — Nested Loop может быть неэффективен.

#### 2. Проверить наличие индексов

* Убедиться, что для внутренней таблицы существует индекс по столбцу, по которому происходит соединение.
* Если индекса нет — создать его, что может значительно ускорить Nested Loop.

#### 3. Рассмотреть альтернативные алгоритмы соединения

* В зависимости от СУБД, можно попытаться заставить оптимизатор использовать **Hash Join** или **Merge Join**, которые эффективнее при больших объёмах.
* В PostgreSQL это можно сделать с помощью настройки параметров (`enable_nestloop`, `enable_hashjoin`, `enable_mergejoin`), временно отключив Nested Loop, чтобы посмотреть, будет ли план лучше.

#### 4. Переписать запрос

* Иногда перестроение запроса, добавление фильтров, сокращение количества данных на раннем этапе помогает оптимизатору выбрать другой план.
* Например, использовать подзапросы, CTE, ограничить выборку по условиям.

#### 5. Обновить статистику таблиц

* Убедитесь, что статистика актуальна (`ANALYZE`), чтобы оптимизатор имел правильные данные для оценки стоимости операторов.

#### 6. Проверить селективность условий

* Если соединение происходит по столбцам с низкой селективностью (много одинаковых значений), Hash Join может быть предпочтительнее.

---

### Когда Nested Loop оставлять как есть?

* Если запрос быстрый, и накладные расходы минимальны, нет необходимости менять план.
* Если таблицы маленькие, или данные сильно отфильтрованы.
* Если есть бизнес-ограничения или оптимизатор работает хорошо с текущим планом.

---









---

**9. Python: функции, декораторы, ООП и др.**

---

## Лямбда функция (что это, зачем, где использовать)

**Лямбда-функция** — это **анонимная функция**, то есть функция без имени, которая создаётся с помощью ключевого слова `lambda`.

Синтаксис:

```python
lambda аргументы: выражение
```

Пример:

```python
lambda x, y: x + y
```

Этот код создаёт функцию, которая складывает два аргумента `x` и `y`, но не присваивает ей имя. Чтобы использовать её, её можно вызвать напрямую или присвоить переменной:

```python
add = lambda x, y: x + y
print(add(2, 3))  # 5
```

---

### Зачем нужны лямбда-функции?

Лямбда-функции полезны, когда нужно:

1. **Определить простую функцию на месте**, без необходимости выносить её в отдельную именованную функцию.
2. **Сделать код короче и лаконичнее**, особенно когда функция передаётся как аргумент.
3. **Упростить работу с функциями высшего порядка**, такими как `map`, `filter`, `sorted`, `reduce`.

---

### Где использовать лямбда-функции?

Наиболее частые области применения:

1. **В функции `map()`**

```python
numbers = [1, 2, 3, 4]
squares = list(map(lambda x: x ** 2, numbers))
# [1, 4, 9, 16]
```

2. **В функции `filter()`**

```python
numbers = [1, 2, 3, 4, 5]
evens = list(filter(lambda x: x % 2 == 0, numbers))
# [2, 4]
```

3. **В функции `sorted()` с ключом**

```python
data = [('apple', 2), ('banana', 1), ('cherry', 3)]
sorted_data = sorted(data, key=lambda x: x[1])
# [('banana', 1), ('apple', 2), ('cherry', 3)]
```

---

### Ограничения лямбда-функций

* **Только одно выражение** (нельзя писать несколько инструкций или использовать конструкции вроде `if-else` в виде блоков).
* **Сложность в отладке**, особенно при большом количестве вложенных лямбда-функций.
* **Не всегда читаемо**, особенно для менее опытных разработчиков.

---

### Когда **не стоит** использовать лямбда-функции

* Если функция сложная или многострочная — лучше использовать `def`, чтобы улучшить читаемость.
* Если требуется повторное использование — именованная функция будет понятнее.

---

## В чем разница "==" и "is"?

### `==` — оператор **сравнения значений**

Оператор `==` проверяет, **равны ли значения** двух объектов, то есть **имеют ли они одинаковое содержимое**.

**Пример**:

```python
a = [1, 2, 3]
b = [1, 2, 3]

print(a == b)  # True — списки имеют одинаковое содержимое
```

Здесь `a` и `b` — это **два разных объекта в памяти**, но их значения совпадают, поэтому `==` возвращает `True`.

---

### `is` — оператор **сравнения идентичности объектов**

Оператор `is` проверяет, **являются ли два объекта на самом деле одним и тем же объектом в памяти**, то есть указывают ли они на **одну и ту же ячейку памяти (один и тот же ID)**.

**Пример**:

```python
a = [1, 2, 3]
b = [1, 2, 3]

print(a is b)  # False — это два разных объекта

c = a
print(a is c)  # True — это один и тот же объект
```

---

### Сравнение в таблице

| Критерий                    | `==` (равенство значений)              | `is` (идентичность объектов)                       |
| --------------------------- | -------------------------------------- | -------------------------------------------------- |
| Что сравнивает              | Содержимое объектов                    | Адреса объектов в памяти (id)                      |
| Может вернуть `True` для... | Разных объектов с одинаковым значением | Только для одного и того же объекта                |
| Пример с числами            | `1000 == 1000 → True`                  | `1000 is 1000 → False` (может быть)                |
| Применение                  | Проверка логического равенства         | Проверка, указывает ли переменная на тот же объект |

---

### Особенности и исключения

1. **Интернирование (interning)**

Python оптимизирует хранение некоторых объектов, таких как **небольшие целые числа** и **строки**, создавая их один раз и повторно используя (интернирование). Поэтому `is` может вернуть `True` даже для на первый взгляд разных переменных.

```python
a = 10
b = 10
print(a is b)  # True — Python использует один и тот же объект

x = 1000
y = 1000
print(x is y)  # False — объекты могут быть разными
```

Поведение зависит от реализации интерпретатора и может отличаться.

2. **Сравнение с `None`**

Сравнение с `None` **всегда** следует делать через `is`, а не `==`.

```python
if value is None:
    ...
```

Потому что `None` — это **одиночный объект**, и `is` проверяет его идентичность корректно.

---

## В чем разница между `func` и `func()`?

Разница между `func` и `func()` в Python принципиальная, и она связана с тем, что в одном случае мы работаем с **ссылкой на функцию**, а в другом — **вызываем эту функцию**.

---

### `func` — это **объект функции** (ссылка на неё)

Когда пишем `func`, без скобок, мы **не вызываем** функцию. Вместо этого мы **ссылаемся на сам объект функции**. Это позволяет, например, передать её как аргумент в другую функцию, сохранить в переменной или вызвать позже.

**Пример**:

```python
def greet():
    return "Hello"

a = greet      # просто ссылка на функцию
print(a)       # <function greet at 0x...>
print(a())     # Hello — вызов через переменную
```

Здесь:

* `a = greet` — сохраняет ссылку на функцию.
* `a()` — вызывает функцию через переменную.

---

### `func()` — это **вызов функции**

Когда пишем `func()`, мы **вызываем** функцию `func`. То есть Python:

1. Выполняет код внутри этой функции,
2. Возвращает результат (если есть оператор `return`),
3. Выполняет побочные эффекты, если они есть (например, печать в консоль, запись в файл и т.д.).

**Пример**:

```python
def greet():
    print("Hello")

greet      # ничего не происходит
greet()    # выводит "Hello"
```

---

### Сравнение:

| Выражение | Что означает      | Что делает                          |
| --------- | ----------------- | ----------------------------------- |
| `func`    | Ссылка на функцию | Ничего не вызывает, можно сохранить |
| `func()`  | Вызов функции     | Запускает функцию                   |

---

### Где важно различие:

1. **Передача функции как аргумента**

```python
def executor(callback):
    return callback()

def say_hi():
    return "Hi"

executor(say_hi)    # передаём функцию — правильно
executor(say_hi())  # передаём результат вызова — ошибка, если результат не функция
```

2. **Создание отложенных вычислений**

```python
actions = [lambda: 2 + 2, lambda: 3 * 3]

for action in actions:
    print(action())  # вызываем каждый элемент списка
```

---

## Назовите изменяемые и неизменяемые объекты (типы).

### Изменяемые объекты (mutable)

Изменяемые объекты — это такие, **состояние которых можно изменить после создания**, не меняя их идентификатор (`id` в памяти).

**Примеры** изменяемых объектов:

| Тип                                    | Пример              | Описание                                     |
| -------------------------------------- | ------------------- | -------------------------------------------- |
| `list`                                 | `[1, 2, 3]`         | Можно добавлять, удалять и изменять элементы |
| `dict`                                 | `{"a": 1}`          | Можно менять значения по ключу               |
| `set`                                  | `{1, 2, 3}`         | Можно добавлять и удалять элементы           |
| `bytearray`                            | `bytearray(b"abc")` | Побайтово изменяемая версия `bytes`          |
| Пользовательские классы (по умолчанию) | `class A: pass`     | Объекты можно менять через атрибуты          |

**Пример**:

```python
lst = [1, 2, 3]
lst[0] = 100
print(lst)  # [100, 2, 3]
```

---

### Неизменяемые объекты (immutable)

Неизменяемые объекты — это такие, **состояние которых нельзя изменить после создания**. Любая попытка изменить их приводит к созданию **нового объекта** в памяти.

**Примеры** неизменяемых объектов:

| Тип         | Пример              | Описание                                       |
| ----------- | ------------------- | ---------------------------------------------- |
| `int`       | `42`                | Любое изменение создаёт новый объект           |
| `float`     | `3.14`              | Аналогично с `int`                             |
| `str`       | `"hello"`           | Изменить символы строки нельзя                 |
| `tuple`     | `(1, 2, 3)`         | Но: может содержать изменяемые элементы внутри |
| `frozenset` | `frozenset([1, 2])` | Неизменяемая версия множества                  |
| `bytes`     | `b"abc"`            | Неизменяемая побайтовая строка                 |
| `bool`      | `True`, `False`     | Подвид `int`, тоже неизменяемый                |
| `NoneType`  | `None`              | Единственный экземпляр                         |

**Пример**:

```python
a = "hello"
print(id(a))          # допустим, 140730
a = a.upper()
print(id(a))          # другой id, т.к. создан новый объект
```

---

### Как проверить, изменяем объект или нет?

Попробуйте изменить его содержимое напрямую или использовать `id()`:

```python
x = (1, 2, 3)
print(id(x))
x = (1, 2, 3, 4)
print(id(x))  # id изменится — новый объект
```

---

### Сравнение изменяемых и неизменяемых:

| Свойство                             | Изменяемые объекты                     | Неизменяемые объекты    |
| ------------------------------------ | -------------------------------------- | ----------------------- |
| Можно менять после создания          | Да                                     | Нет                     |
| `id(obj)` при изменении              | Не меняется                            | Меняется (новый объект) |
| Можно использовать как ключ в `dict` | Нет (если объект сам по себе изменяем) | Да                      |
| Безопасность при многопоточности     | Менее безопасны                        | Более безопасны         |

---

### Вложенные структуры

Некоторые **неизменяемые объекты могут содержать изменяемые внутри**:

```python
t = (1, [2, 3])
t[1][0] = 99
print(t)  # (1, [99, 3]) — tuple сам неизменяем, но содержит изменяемый список
```

---

## Декораторы (что, зачем нужно, как влияет на структуру) + написать свой пример


### Что такое декоратор?

**Декоратор** — это функция, которая **принимает другую функцию как аргумент и возвращает новую функцию**, которая обычно расширяет или изменяет поведение исходной.

Это **структурный паттерн проектирования**, позволяющий оборачивать поведение функций или методов **без изменения их кода**.

На практике это синтаксический сахар для работы с функциями высшего порядка.

---

### Зачем нужны декораторы?

Декораторы позволяют:

* Повторно использовать логику (например, логирование, кеширование, проверка прав).
* Разделять бизнес-логику и инфраструктурный код.
* Следовать принципам чистой архитектуры и **DRY (Don't Repeat Yourself)**.
* Создавать **чистую, читаемую и расширяемую** структуру.

---

### Как работают декораторы?

Если есть функция `@decorator`, это равнозначно:

```python
@decorator
def my_func():
    pass
```

Это то же самое, что:

```python
def my_func():
    pass

my_func = decorator(my_func)
```

То есть декоратор вызывается один раз при определении функции и возвращает новый объект-функцию.

---

### Как влияет на структуру?

1. **Упрощает архитектуру** — например, можно писать обёртки для логирования, измерения времени выполнения, контроля доступа, и применять их к любой функции.

2. **Уменьшает дублирование** — общее поведение (например, валидация, обработка ошибок) пишется один раз и применяется везде.

3. **Изменяет поведение, не трогая код функции** — декоратор работает *снаружи*, не требуя изменить тело декорируемой функции.

---

### Пример собственного декоратора

Допустим, мы хотим логировать вызовы функций:

```python
def log_call(func):
    def wrapper(*args, **kwargs):
        print(f"Calling {func.__name__} with args={args}, kwargs={kwargs}")
        result = func(*args, **kwargs)
        print(f"{func.__name__} returned {result}")
        return result
    return wrapper
```

Используем декоратор:

```python
@log_call
def add(a, b):
    return a + b

@log_call
def greet(name):
    return f"Hello, {name}!"

# Примеры вызовов
add(2, 3)
greet("Alice")
```

**Результат:**

```
Calling add with args=(2, 3), kwargs={}
add returned 5
Calling greet with args=('Alice',), kwargs={}
greet returned Hello, Alice!
```

---

### Пример с параметрами (декоратор-декоратор)

Иногда нужен декоратор, который сам принимает аргументы:

```python
def repeat(n):
    def decorator(func):
        def wrapper(*args, **kwargs):
            for _ in range(n):
                result = func(*args, **kwargs)
            return result
        return wrapper
    return decorator

@repeat(3)
def say_hi():
    print("Hi!")

say_hi()  # Выведет "Hi!" трижды
```

---

## Можно ли на одну функцию нацепить несколько декораторов и как они будут считываться?

Да, **в Python можно применять несколько декораторов к одной функции**. Это называется **композиция декораторов**, и она широко используется на практике — например, при логировании, проверке прав доступа, обёртке в кэш и так далее.

---

### Как это работает?

Когда вы пишете несколько декораторов над функцией, они **применяются сверху вниз, но выполняются снизу вверх**.

### Синтаксис:

```python
@decorator1
@decorator2
@decorator3
def my_func():
    pass
```

Это эквивалентно:

```python
def my_func():
    pass

my_func = decorator1(decorator2(decorator3(my_func)))
```

---

### Порядок применения

* **Сначала** применяется `decorator3`, затем его результат оборачивается в `decorator2`, и результат этой обёртки — в `decorator1`.
* **При вызове** функции `my_func()` будет сначала исполняться логика `decorator1`, затем — `decorator2`, и так далее.

---

**Пример**

Допустим, у нас есть три декоратора:

```python
def deco1(func):
    def wrapper(*args, **kwargs):
        print("Before deco1")
        result = func(*args, **kwargs)
        print("After deco1")
        return result
    return wrapper

def deco2(func):
    def wrapper(*args, **kwargs):
        print("Before deco2")
        result = func(*args, **kwargs)
        print("After deco2")
        return result
    return wrapper

def deco3(func):
    def wrapper(*args, **kwargs):
        print("Before deco3")
        result = func(*args, **kwargs)
        print("After deco3")
        return result
    return wrapper
```

Применим их к функции:

```python
@deco1
@deco2
@deco3
def say_hello():
    print("Hello")
```

### Вызов:

```python
say_hello()
```

### Результат:

```
Before deco1
Before deco2
Before deco3
Hello
After deco3
After deco2
After deco1
```

---

## Что такое декоратор Шредингера?

Это **функция, которая может быть вызвана как с параметрами, так и без них**, и корректно работает в обоих случаях.

То есть:

```python
@my_decorator
def f():
    pass
```

и

```python
@my_decorator(param=True)
def f():
    pass
```

оба варианта работают. Такая универсальность требует особого способа реализации.

---

### Пример «декоратора Шрёдингера»

```python
import functools

def my_decorator(_func=None, *, param=False):
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            print(f"param={param}")
            return func(*args, **kwargs)
        return wrapper

    if _func is None:
        # декоратор вызван с параметрами
        return decorator
    else:
        # декоратор вызван без параметров
        return decorator(_func)
```

### Использование:

```python
@my_decorator
def hello():
    print("Hello")

@my_decorator(param=True)
def world():
    print("World")

hello()
# param=False
# Hello

world()
# param=True
# World
```

---

### Почему это называют «декоратором Шрёдингера»?

Потому что при чтении кода **невозможно заранее точно сказать, является ли `@my_decorator` декоратором с параметрами или нет**, пока не произойдёт «наблюдение» — то есть выполнение кода. Он находится в **суперпозиции состояний**:

* Может быть просто функцией-декоратором;
* Может быть фабрикой декоратора с параметрами.

---

### Для чего используется?

Такие декораторы полезны, когда вы хотите сделать свой декоратор **гибким** — чтобы его можно было применять как:

* `@my_decorator`
* `@my_decorator(param=value)`

И при этом не требовать от пользователя всегда указывать скобки.

---

## Генератор (что, зачем нужно) + написать свой пример

**Генератор** — это специальный тип функции (или выражения) в Python, который **лениво** (по мере запроса) возвращает значения с помощью ключевого слова `yield` вместо `return`.

Вместо того чтобы возвращать весь список сразу, генератор **выдаёт по одному элементу за раз**, сохраняя своё внутреннее состояние между вызовами. Это делает генераторы **эффективными по памяти** и полезными при работе с большими объёмами данных.

---

### Зачем нужны генераторы

1. **Экономия памяти**
   Генераторы не загружают в память всю последовательность — они возвращают элементы по мере запроса.

2. **Быстродействие на больших данных**
   Подход "ленивой" итерации позволяет начать обработку данных до того, как они полностью сформированы.

3. **Читаемость и компактность кода**
   Генераторы позволяют писать более понятный код, особенно для последовательных вычислений.

4. **Удобство для потоковой обработки**
   Используются в пайплайнах обработки данных, логов, стримов и т. д.

---

### Пример: генератор-функция

```python
def count_up_to(max_value):
    count = 1
    while count <= max_value:
        yield count
        count += 1
```

Использование:

```python
for number in count_up_to(5):
    print(number)
```

Результат:

```
1
2
3
4
5
```

---

### Как это работает

Каждый вызов `yield` приостанавливает выполнение функции, и при следующей итерации `for` она **возобновляется с того места**, где остановилась.

---

### Пример: генератор-выражение

Генератор можно записать с помощью выражения, аналогично list comprehension, но в круглых скобках:

```python
squares = (x*x for x in range(5))

for s in squares:
    print(s)
```

Результат:

```
0
1
4
9
16
```

---

### Отличие от обычных функций и списков

| Особенность                   | Обычная функция | Генератор-функция |
| ----------------------------- | --------------- | ----------------- |
| Использует `return`           | Да              | Нет               |
| Использует `yield`            | Нет             | Да                |
| Возвращает сразу все значения | Да              | Нет               |
| Потребляет всю память         | Да              | Нет (ленивый)     |

---

## Как рассчитывается сложность алгоритма? на примере list, tuple

**Асимптотическая сложность алгоритма** (или операции) — это способ описания, **как изменяется время выполнения или потребление памяти при увеличении размера входных данных**. Используется обозначение **"Big O"** — например, `O(1)`, `O(n)`, `O(log n)` и т.д.

---

### Основные обозначения сложности

* **O(1)** — постоянное время, не зависит от размера данных.
* **O(n)** — линейное время, растёт пропорционально размеру входа.
* **O(log n)** — логарифмическое время.
* **O(n²)** — квадратичное время (вложенные циклы).

---

### Сложность операций с `list` и `tuple`

Python `list` и `tuple` — это **последовательности**, но работают по-разному:

* `list` — изменяемая структура, поддерживает добавление, удаление, изменение элементов.
* `tuple` — неизменяемая структура, не поддерживает изменение после создания.

---

### Операции с `list`

| Операция                 | Сложность  | Пояснение                                                     |
| ------------------------ | ---------- | ------------------------------------------------------------- |
| `lst[i]`                 | O(1)       | Доступ по индексу реализован через массив.                    |
| `lst.append(x)`          | O(1)\*     | Амортизированная — иногда требуется перераспределение памяти. |
| `lst.insert(i, x)`       | O(n)       | Сдвиг всех элементов справа от `i`.                           |
| `lst.pop()`              | O(1)       | Удаление последнего элемента.                                 |
| `lst.pop(i)`             | O(n)       | Удаление по индексу требует сдвига остальных.                 |
| `lst.remove(x)`          | O(n)       | Поиск элемента и сдвиг.                                       |
| `lst.index(x)`           | O(n)       | Линейный поиск по элементам.                                  |
| `lst.sort()`             | O(n log n) | Быстрая встроенная сортировка (Timsort).                      |
| `lst.extend([x, y])`     | O(k)       | Добавление `k` элементов.                                     |
| Перебор: `for x in lst:` | O(n)       | Линейная итерация по элементам.                               |

---

### Операции с `tuple`

| Операция                         | Сложность | Пояснение                        |
| -------------------------------- | --------- | -------------------------------- |
| `tpl[i]`                         | O(1)      | Доступ по индексу, как в списке. |
| `tpl.index(x)`                   | O(n)      | Поиск значения.                  |
| `tpl.count(x)`                   | O(n)      | Подсчёт количества вхождений.    |
| Создание новой `tuple` из `list` | O(n)      | Копирование данных из списка.    |
| Перебор: `for x in tpl:`         | O(n)      | Линейная итерация.               |

---

### Почему `tuple` быстрее?

Так как `tuple` **неизменяемы**, они:

* имеют **меньший overhead в памяти**;
* **кэшируются** интерпретатором (в некоторых случаях);
* **быстрее создаются и итерируются**, чем списки.

---

## Как передаются аргументы в функцию?

---

### Виды передачи аргументов в функцию

1. **Позиционные аргументы (positional arguments)**
   Аргументы передаются в том порядке, в котором они объявлены в функции.

2. **Именованные аргументы (keyword arguments)**
   Аргументы передаются по имени параметра, порядок не важен.

3. **Аргументы по умолчанию (default arguments)**
   Параметры функции могут иметь значения по умолчанию, если при вызове они не передаются, используется это значение.

4. **Произвольное количество позиционных аргументов (`*args`)**
   Позволяет передавать функции произвольное число позиционных аргументов, которые собираются в кортеж.

5. **Произвольное количество именованных аргументов (`**kwargs`)**
   Позволяет передавать произвольное число именованных аргументов, которые собираются в словарь.

---

**Подробно о каждом способе**

### 1. Позиционные аргументы

```python
def greet(name, age):
    print(f"Hello, {name}. You are {age} years old.")

greet("Alice", 30)
```

* Значения передаются в том порядке, в котором объявлены параметры.
* Ошибка, если передать меньше или больше аргументов.

---

### 2. Именованные аргументы

```python
greet(age=30, name="Alice")
```

* Параметры явно указываются по имени.
* Позволяет менять порядок аргументов.

---

### 3. Аргументы по умолчанию

```python
def greet(name, age=25):
    print(f"Hello, {name}. You are {age} years old.")

greet("Bob")        # Используется age=25
greet("Bob", 40)    # age=40
```

* Значение параметра используется по умолчанию, если не передано.
* Параметры с дефолтными значениями должны идти после обязательных.

---

### 4. `*args` — произвольное число позиционных аргументов

```python
def sum_all(*args):
    return sum(args)

print(sum_all(1, 2, 3))    # 6
print(sum_all(5))          # 5
```

* Все переданные позиционные аргументы собираются в кортеж `args`.
* Удобно для функций с переменным числом аргументов.

---

### 5. `**kwargs` — произвольное число именованных аргументов

```python
def print_info(**kwargs):
    for key, value in kwargs.items():
        print(f"{key}: {value}")

print_info(name="Alice", age=30)
```

* Все именованные аргументы собираются в словарь `kwargs`.
* Позволяет передавать дополнительные параметры без их явного объявления.

---

### Как Python передаёт аргументы внутри функции?

* В Python аргументы передаются **по ссылке** на объект, а не копированием значения.
* Для **изменяемых объектов** (например, списков, словарей) это значит, что изменения внутри функции могут повлиять на объект вне её.
* Для **неизменяемых объектов** (например, чисел, строк, кортежей) — изменения внутри функции не влияют на оригинал.

Пример:

```python
def modify_list(lst):
    lst.append(100)

my_list = [1, 2, 3]
modify_list(my_list)
print(my_list)  # [1, 2, 3, 100]
```

---

## Функция, которая используется в качестве аргумента, может использовать свои аргументы?

Да, функция, которая передаётся в качестве аргумента другой функции, **может использовать свои собственные аргументы** при вызове. В Python функции являются объектами первого класса, то есть их можно передавать как аргументы, возвращать из других функций и сохранять в переменные.

---

### Как это работает на практике

Рассмотрим пример:

```python
def apply_function(func, x, y):
    return func(x, y)

def multiply(a, b):
    return a * b

result = apply_function(multiply, 3, 4)
print(result)  # Выведет 12
```

Здесь:

* `multiply` — это функция, принимающая два аргумента `a` и `b`.
* Она передаётся как аргумент в функцию `apply_function`.
* Внутри `apply_function` вызывается `func(x, y)`, то есть `multiply(3, 4)`.
* Таким образом, **функция, переданная как аргумент, принимает свои собственные параметры**, которые ей передаются во время вызова.

---

### Возможность передачи аргументов функции-аргументу

Функция-аргумент может иметь любые параметры — позиционные, именованные, с умолчаниями и т. п. Важно, чтобы вызывающая функция передавала нужные аргументы при вызове.

Например:

```python
def executor(func, *args, **kwargs):
    return func(*args, **kwargs)

def greet(name, greeting="Hello"):
    return f"{greeting}, {name}!"

print(executor(greet, "Alice"))               # Hello, Alice!
print(executor(greet, "Bob", greeting="Hi"))  # Hi, Bob!
```

---

### Почему это полезно

* Позволяет писать **универсальные и обобщённые функции**, которые могут принимать разные операции.
* Обеспечивает высокий уровень **гибкости и переиспользования** кода.
* Активно используется в функциональном программировании, коллбэках, обработчиках событий и т. п.

---

## Зачем прописывать тип входящих или выходящих данных в функцию?

Прописание типов входящих и выходящих данных в функции, известное как **аннотация типов** (type hinting), не является обязательным в Python, но имеет важные преимущества и служит нескольким целям.

---

### 1. Улучшение читаемости и понимания кода

Когда в определении функции явно указаны типы параметров и возвращаемого значения, становится сразу понятно, какие данные ожидаются и что функция возвращает. Это облегчает понимание кода другим разработчикам (или самому себе через некоторое время).

Пример:

```python
def add(a: int, b: int) -> int:
    return a + b
```

Такой код сразу показывает, что функция принимает два целых числа и возвращает целое число.

---

### 2. Поддержка статической проверки типов

Инструменты статической типизации (например, **mypy**, **Pyright**) могут анализировать код, находить ошибки типов до запуска программы. Это позволяет выявлять:

* Неправильное использование функций (например, передача строки вместо числа).
* Несоответствие возвращаемых значений заявленному типу.
* Потенциальные ошибки и баги на ранней стадии.

---

## 3. Документирование функции

Типы служат как своего рода **живая документация**, уменьшая необходимость писать дополнительные комментарии и отдельную документацию, особенно если имена параметров недостаточно информативны.

---

## 4. Облегчение рефакторинга и поддержки кода

При изменении кода наличие типовых аннотаций помогает понять, как изменять функции и что можно менять без нарушения логики. Это особенно важно в больших проектах с командной разработкой.

---

## 5. Влияние на выполнение программы

В стандартном Python **аннотации типов никак не влияют на выполнение программы** — они игнорируются интерпретатором. Однако их можно использовать с внешними инструментами для анализа и тестирования.

---

## Пример аннотации типов

```python
from typing import List, Optional

def process_items(items: List[int], flag: Optional[bool] = None) -> int:
    if flag:
        return sum(items)
    else:
        return len(items)
```

---

## Что представляет из себя тип данных Int в Python?

В Python тип данных `int` используется для хранения целых чисел — как положительных, так и отрицательных, включая ноль. Это один из базовых и широко используемых примитивных типов в языке.

---

### Особенности `int` в Python

#### 1. **Произвольная точность**

В отличие от многих языков программирования, где целочисленные типы имеют фиксированный размер (например, 32 или 64 бита), в Python `int` имеет **произвольную точность**. Это значит, что целое число может быть сколь угодно большим (ограничено только объемом доступной памяти).

Пример:

```python
a = 10**100  # Очень большое число
print(a)
```

Python корректно работает с таким числом, не переполняется и не теряет точность.

---

#### 2. **Неизменяемость**

Объекты типа `int` в Python являются **неизменяемыми** (immutable). Это означает, что после создания объекта его значение нельзя изменить. Любые операции с числами создают новый объект.

---

#### 3. **Поддержка всех стандартных арифметических операций**

`int` поддерживает:

* Сложение (`+`)
* Вычитание (`-`)
* Умножение (`*`)
* Целочисленное деление (`//`)
* Деление с плавающей точкой (`/`) — результатом будет `float`
* Остаток от деления (`%`)
* Возведение в степень (`**`)
* Побитовые операции (`&`, `|`, `^`, `~`, `<<`, `>>`)

---

#### 4. **Автоматическое преобразование между типами**

При арифметических операциях с другими типами Python автоматически преобразует `int` в подходящий тип — например, при операции с `float` результат будет `float`.

---

#### 5. **Хранение и производительность**

* Малые числа (обычно от -5 до 256) в Python кэшируются и переиспользуются, что улучшает производительность.
* Большие числа хранятся в виде объектов с массивом цифр, что влияет на скорость операций по сравнению с фиксированными типами в других языках.

---

#### 6. **Создание и преобразование**

* Создать `int` можно с помощью литералов, например `123`, `-456`.
* Можно преобразовать другие типы с помощью функции `int()`, например `int("42")` или `int(3.14)`.

---

### Пример использования `int`

```python
x = 10
y = 3

print(x + y)   # 13
print(x // y)  # 3
print(x / y)   # 3.3333333333333335
print(x ** y)  # 1000

big_num = 10**50
print(big_num)
```

---

## Можно ли в функции Python в качестве аргумента использовать функцию? Если да, то как называется такая функция?

Да, **в Python можно использовать функцию в качестве аргумента другой функции**. Более того, это обычная практика, особенно в функциональном стиле программирования, который Python частично поддерживает.

---

### Как это работает?

В Python **функции являются объектами первого класса (first-class objects)**. Это означает, что их можно:

* Передавать как аргументы в другие функции
* Возвращать из других функций
* Присваивать переменным
* Хранить в структурах данных (например, в списках или словарях)

---

### Как называется такая функция?

Функция, которая **принимает другую функцию как аргумент**, называется **функцией высшего порядка (higher-order function)**.

Также, если функция возвращает другую функцию — она тоже считается функцией высшего порядка.

---

### Пример: передача функции как аргумента

```python
def greet(name):
    return f"Hello, {name}!"

def apply_func(func, value):
    return func(value)

result = apply_func(greet, "Alice")
print(result)  # Hello, Alice!
```

Здесь:

* `greet` — обычная функция, принимающая строку.
* `apply_func` — функция высшего порядка, так как она принимает другую функцию (`func`) и применяет её к значению (`value`).

---

### Пример с использованием встроенной функции `map`

```python
def square(x):
    return x * x

numbers = [1, 2, 3, 4]
squared = list(map(square, numbers))
print(squared)  # [1, 4, 9, 16]
```

`map` — это функция высшего порядка, которая принимает:

* функцию (`square`)
* итерируемый объект (`numbers`)

И применяет функцию к каждому элементу.

---

### Когда это используется?

1. **Передача логики**: позволяет передавать поведение (например, сравнение, фильтрацию, форматирование) как параметр.
2. **Колбэки**: используется во многих библиотеках, где вы передаёте функцию, которая должна выполниться позже (например, обработчики событий).
3. **Функции `map`, `filter`, `sorted`, `reduce` и т. д.**
4. **Декораторы**: принимают функцию, модифицируют её поведение и возвращают новую.

---

## Назовите парадигмы ООП?

В объектно-ориентированном программировании (ООП) существует **несколько ключевых парадигм (или принципов)**, которые определяют его структуру, поведение и подход к организации кода. Эти парадигмы позволяют проектировать программное обеспечение, ориентируясь на **объекты**, которые объединяют **данные** и **поведение**.

Основные парадигмы ООП:

---

### 1. **Абстракция**

**Суть**:
Абстракция позволяет **выделить только значимую информацию** об объекте, скрывая при этом внутреннюю реализацию и ненужные детали.

**Пример**:
Когда вы используете объект `Car`, вы оперируете такими методами, как `start()` или `drive()`, не зная, как устроен двигатель или коробка передач.

**Зачем нужна**:

* Упрощает взаимодействие с объектами
* Делает код более читаемым и понятным
* Снижает связанность

---

### 2. **Инкапсуляция**

**Суть**:
Инкапсуляция — это механизм **ограничения доступа** к внутренним данным объекта и реализации его поведения. Обычно это достигается с помощью **модификаторов доступа** и **свойств**.

**Пример**:
В Python можно обозначить приватные переменные с помощью подчеркивания: `_balance`, и предоставить доступ через методы `get_balance()` и `set_balance()`.

**Зачем нужна**:

* Защищает данные от неконтролируемого изменения
* Позволяет управлять доступом к атрибутам
* Повышает устойчивость и предсказуемость кода

---

### 3. **Наследование**

**Суть**:
Наследование позволяет **создавать новый класс на основе существующего**, унаследовав его свойства и поведение, и при необходимости переопределить или расширить его.

**Пример**:

```python
class Animal:
    def speak(self):
        print("Some sound")

class Dog(Animal):
    def speak(self):
        print("Woof")
```

**Зачем нужно**:

* Позволяет повторно использовать код
* Обеспечивает иерархическую структуру классов
* Упрощает расширение системы

---

### 4. **Полиморфизм**

**Суть**:
Полиморфизм означает возможность **использовать один и тот же интерфейс для разных типов объектов**. Это может быть как **переопределение метода**, так и **использование интерфейсов/абстрактных классов**.

**Пример**:

```python
animals = [Dog(), Cat(), Bird()]
for animal in animals:
    animal.speak()  # У каждого speak будет вести себя по-своему
```

**Зачем нужен**:

* Повышает гибкость и расширяемость кода
* Позволяет писать универсальный код, работающий с разными типами объектов

---

### Заключение

Основные парадигмы объектно-ориентированного программирования:

1. **Абстракция** — скрытие деталей реализации
2. **Инкапсуляция** — защита данных и контроль доступа
3. **Наследование** — повторное использование и расширение поведения
4. **Полиморфизм** — универсальный интерфейс для разных реализаций

---

## Self (что это, для чего нужен, как и где использовать)

`self` — это **ссылка на текущий экземпляр класса** в объектно-ориентированном программировании на Python. Он используется в методах класса для доступа к **свойствам (атрибутам)** и **другим методам** этого же объекта.

---

### 1. Что такое `self`

* `self` — это **не ключевое слово**, а **принятый по соглашению** (convention) параметр первого аргумента в методах экземпляра класса.
* Он указывает на конкретный **объект**, для которого был вызван метод.

---

### 2. Для чего нужен `self`

`self` нужен, чтобы:

* Обращаться к **атрибутам объекта** (например, `self.name`)
* Вызывать **другие методы этого же объекта**
* Хранить **состояние экземпляра класса**
* Разграничивать **локальные переменные** и **данные объекта**

---

### 3. Как и где использовать

`self` используется только в **методах экземпляра** (не в статических и не в методах класса). Вот простой пример:

```python
class Person:
    def __init__(self, name):
        self.name = name  # сохраняем имя в атрибуте объекта

    def say_hello(self):
        print(f"Hello, my name is {self.name}")
```

Пример использования:

```python
p = Person("Alice")
p.say_hello()  # Hello, my name is Alice
```

Здесь:

* `__init__` — это конструктор, в него первым аргументом передаётся `self`
* Через `self.name = name` мы сохраняем значение в объекте
* В `say_hello` мы обращаемся к этому значению через `self.name`

---

### 4. Почему нужно явно писать `self`

В Python нужно **явно указывать `self`**, потому что:

* Это делает связь между методами и атрибутами объекта **прозрачной**
* Python не скрывает механику обращения к объекту (в отличие от некоторых других языков, где `this` встроено неявно)
* Это часть синтаксиса языка

---

### 5. Чем `self` отличается от `this` в других языках

* В Python вы **должны явно указать** `self` в качестве первого параметра в методе.
* В языках вроде Java или C++ `this` не передаётся как параметр — это встроенная переменная, доступная внутри методов по умолчанию.

---

### 6. Пример неправильного использования (без `self`)

```python
class Car:
    def set_color(color):
        self.color = color  # Ошибка: self не определён
```

Такой код вызовет ошибку, потому что `self` не передаётся в метод. Правильно так:

```python
class Car:
    def set_color(self, color):
        self.color = color
```

---

## Что такое super() и зачем нужен?

В Python `super()` — это встроенная функция, которая возвращает **проксирующий объект**, позволяющий **вызывать методы родительского (базового) класса** без необходимости явно указывать его имя.

Это особенно полезно при **наследовании**, когда требуется переопределить методы в дочернем классе, но при этом **сохранить вызов родительской реализации**.

---

### Зачем нужен `super()`

`super()` используется для:

1. **Повторного использования кода базового класса** — вместо дублирования.
2. **Поддержки множественного наследования** — безопасный и корректный способ вызывать методы родителя.
3. **Расширения функциональности метода родителя** — добавление логики до или после вызова метода базового класса.

---

### Пример: вызов конструктора родителя

```python
class Animal:
    def __init__(self, name):
        self.name = name

class Dog(Animal):
    def __init__(self, name, breed):
        super().__init__(name)  # вызываем __init__ базового класса
        self.breed = breed
```

Здесь:

* `super().__init__(name)` вызывает конструктор `Animal`, чтобы инициализировать `name`.
* Это безопаснее, чем `Animal.__init__(self, name)`, особенно при сложной иерархии классов.

---

### Пример: переопределение метода

```python
class Logger:
    def log(self, message):
        print(f"LOG: {message}")

class TimestampLogger(Logger):
    def log(self, message):
        from datetime import datetime
        timestamped = f"{datetime.now()}: {message}"
        super().log(timestamped)  # вызываем метод из родительского класса
```

---

### Поведение `super()` при множественном наследовании

Когда класс наследует от нескольких родителей, `super()` работает в соответствии с **алгоритмом разрешения порядка методов** (MRO — Method Resolution Order). Это позволяет избегать дублирования вызовов и конфликтов.

Пример:

```python
class A:
    def show(self):
        print("A")

class B(A):
    def show(self):
        print("B")
        super().show()

class C(A):
    def show(self):
        print("C")
        super().show()

class D(B, C):
    def show(self):
        print("D")
        super().show()

d = D()
d.show()
```

Результат:

```
D
B
C
A
```

MRO определяет, в каком порядке вызываются методы родительских классов. Его можно посмотреть так:

```python
print(D.mro())
```

---

### Почему не просто `ParentClass.method(self)`?

Хотя можно вызвать метод родителя явно:

```python
ParentClass.method(self, ...)
```

Но `super()`:

* Работает правильно при **множественном наследовании**
* Следует MRO
* Автоматически адаптируется, если базовый класс изменится

---

### Когда `super()` использовать обязательно

* При наследовании и переопределении методов, особенно `__init__`
* В случаях, когда нужно дополнить поведение родительского метода, а не полностью заменить

---

## Расскажи порядок разрешения методов?

Порядок разрешения методов (Method Resolution Order, **MRO**) — это правило, по которому Python определяет, **в каком порядке искать метод или атрибут** в иерархии наследования классов, когда он вызывается через экземпляр.

Это особенно важно в контексте **множественного наследования**, когда класс наследует от нескольких родительских классов, и один и тот же метод может быть определён в нескольких из них.

---

### Как работает MRO в Python

Python использует алгоритм **C3 Linearization**, который:

1. Учитывает **порядок наследования** (слева направо).
2. Гарантирует, что **дочерний класс всегда проверяется раньше родительского**.
3. Учитывает **иерархию** и не нарушает порядок наследования.

---

### Простой пример (одиночное наследование)

```python
class A:
    def say(self):
        print("A")

class B(A):
    pass

b = B()
b.say()  # Будет искать метод say в B, потом в A
```

MRO: `B → A → object`

---

### Пример с множественным наследованием

```python
class A:
    def say(self):
        print("A")

class B(A):
    def say(self):
        print("B")

class C(A):
    def say(self):
        print("C")

class D(B, C):
    pass

d = D()
d.say()
```

Вывод: `B`

---

#### Почему `B`, а не `C`?

Потому что MRO для класса `D`:

```python
print(D.__mro__)
```

Результат:

```
(<class '__main__.D'>, <class '__main__.B'>, <class '__main__.C'>, <class '__main__.A'>, <class 'object'>)
```

Python идёт по порядку:

* Сначала проверяет `D`
* Затем `B`
* Потом `C`
* Затем `A`
* И только в конце — `object`

---

### Как Python вычисляет MRO (C3 Linearization)

Для класса `D(B, C)`:

* MRO(B) = `[B, A, object]`
* MRO(C) = `[C, A, object]`
* Объединение по алгоритму C3: `[D, B, C, A, object]`

Алгоритм выбирает такой порядок, чтобы:

* Уважать порядок в объявлении (`class D(B, C)`)
* Убедиться, что родительский класс вызывается **после всех его дочерних классов**

---

### Использование `super()` и MRO

Когда вызывается `super()`, Python следует именно **MRO**. Поэтому `super()` корректно работает даже в сложной иерархии классов:

```python
class A:
    def hello(self):
        print("A")

class B(A):
    def hello(self):
        print("B")
        super().hello()

class C(A):
    def hello(self):
        print("C")
        super().hello()

class D(B, C):
    def hello(self):
        print("D")
        super().hello()

D().hello()
```

Результат:

```
D
B
C
A
```

MRO: `[D, B, C, A, object]`

---

### Просмотр порядка разрешения методов

MRO можно посмотреть двумя способами:

```python
print(ClassName.__mro__)
# или
help(ClassName)
```

---

## Что такое class methods / static methods?

В Python, методы класса (`class methods`) и статические методы (`static methods`) — это два способа определить поведение, связанное с классом, но не обязательно с конкретным экземпляром. Они задаются с помощью декораторов `@classmethod` и `@staticmethod`.

---

### 1. **Методы класса (`@classmethod`)**

#### Что это?

Метод класса — это метод, который получает **не экземпляр класса (`self`)**, а **сам класс (`cls`)** в качестве первого аргумента. Это позволяет работать с атрибутами и методами самого класса (например, создавать новые экземпляры).

#### Синтаксис:

```python
class MyClass:
    class_variable = 0

    @classmethod
    def set_class_variable(cls, value):
        cls.class_variable = value
```

#### Особенности:

* Получает доступ к **самому классу** (`cls`), а не к объекту.
* Может изменять **атрибуты класса**, но не конкретного объекта.
* Часто используется как **альтернативный конструктор**.

#### Пример:

```python
class Person:
    def __init__(self, name, age):
        self.name = name
        self.age = age

    @classmethod
    def from_string(cls, data_str):
        name, age = data_str.split(',')
        return cls(name, int(age))

p = Person.from_string("Alice,30")
```

В данном случае `from_string` — альтернативный способ создать объект `Person` из строки.

---

### 2. **Статические методы (`@staticmethod`)**

#### Что это?

Статический метод **не принимает ни `self`, ни `cls`**. Это обычная функция, которая просто находится внутри класса **для логической группировки**, но она никак не зависит от самого класса или его экземпляра.

#### Синтаксис:

```python
class MyClass:
    @staticmethod
    def utility(x, y):
        return x + y
```

#### Особенности:

* **Не имеет доступа** ни к экземпляру, ни к классу.
* Используется для **вспомогательных функций**, которые логически связаны с классом.
* Поведение полностью **независимо от состояния** класса и экземпляров.

#### Пример:

```python
class MathUtils:
    @staticmethod
    def add(a, b):
        return a + b

result = MathUtils.add(5, 3)
```

Здесь метод `add` не зависит ни от данных класса, ни от его объектов.

---

### Заключение

* Используй **обычные методы**, когда нужно работать с данными конкретного объекта.
* Используй **`@classmethod`**, когда нужно работать с самим классом, а не объектом (например, фабрики).
* Используй **`@staticmethod`**, когда метод никак не зависит от класса и объекта, но логически относится к классу.

---

## Что такое итерация?

**Итерация** — это процесс **последовательного перебора элементов** коллекции (например, списка, строки, множества, словаря) один за другим. В программировании под итерацией чаще всего понимается выполнение набора операций **над каждым элементом** итерируемого объекта.

---

### Что такое итерируемый объект?

Это объект, который **может быть пройден в цикле**, например:

* список (`list`)
* строка (`str`)
* кортеж (`tuple`)
* множество (`set`)
* словарь (`dict`)
* генераторы
* файлы

Такие объекты реализуют специальный метод `__iter__()`, возвращающий **итератор**.

---

### Итератор

Итератор — это объект, который поддерживает метод `__next__()`, возвращающий следующий элемент последовательности. Когда элементов больше нет, возбуждается исключение `StopIteration`.

**Пример "вручную"**:

```python
numbers = [1, 2, 3]
iterator = iter(numbers)

print(next(iterator))  # 1
print(next(iterator))  # 2
print(next(iterator))  # 3
# next(iterator) вызовет StopIteration
```

---

### Итерация в цикле `for`

В Python итерация почти всегда выполняется с помощью цикла `for`, который автоматически вызывает `iter()` и `next()`:

```python
fruits = ["apple", "banana", "cherry"]

for fruit in fruits:
    print(fruit)
```

#### Что происходит "под капотом":

1. Вызов `iter(fruits)` — создаётся итератор.
2. Вызов `next(iterator)` — получаем следующий элемент.
3. Повторяется до исключения `StopIteration`.

---

### Итерация в других контекстах

Итерации можно использовать в:

* генераторах списков:

  ```python
  squares = [x*x for x in range(5)]
  ```
* функциях `map`, `filter`, `zip`, `enumerate`
* написании собственных итераторов через классы
* создании генераторов через `yield`

---

### Зачем нужна итерация?

* Позволяет **обходить** элементы коллекций.
* Используется для **поиска**, **обработки**, **фильтрации** данных.
* Упрощает работу с последовательностями без необходимости ручного доступа по индексам.
* Является основой многих концепций: генераторы, потоки данных, ленивые вычисления.

---

## Какие типы данных могут быть ключами словаря?

В Python **ключами словаря (dict)** могут быть **только неизменяемые (immutable)** и **хешируемые (hashable)** объекты. Это связано с тем, что словарь реализован как хеш-таблица, и ключ должен иметь **фиксированное хеш-значение**, которое не меняется на протяжении его "жизни" в словаре.

---

### Требования к ключу словаря:

1. **Неизменяемость**: ключ должен быть immutable — его содержимое не должно меняться после создания.
2. **Хешируемость**: объект должен реализовывать метод `__hash__()` и быть сравним (`__eq__()`), чтобы словарь мог правильно находить и различать ключи.

---

### Что можно использовать в качестве ключа?

#### Разрешённые типы:

| Тип данных  | Пример                         | Объяснение                           |
| ----------- | ------------------------------ | ------------------------------------ |
| `int`       | `d = {1: "one"}`               | Целые числа неизменяемы и хешируемы  |
| `float`     | `d = {3.14: "pi"}`             | Хешируемы (если не NaN)              |
| `str`       | `d = {"a": 1}`                 | Строки — неизменяемые                |
| `bool`      | `d = {True: "yes"}`            | Логические значения — хешируемы      |
| `tuple`     | `d = {(1, 2): "pair"}`         | Если внутри кортежа только immutable |
| `frozenset` | `d = {frozenset({1,2}): "fs"}` | Неизменяемая версия множества        |
| `bytes`     | `d = {b"key": 123}`            | Неизменяемый тип                     |
| `None`      | `d = {None: "null"}`           | `None` — хешируемый объект           |

---

#### Нельзя использовать:

| Тип данных                          | Причина отказа                                                               |
| ----------------------------------- | ---------------------------------------------------------------------------- |
| `list`                              | Списки изменяемы, нет `__hash__()`                                           |
| `set`                               | Множества изменяемы                                                          |
| `dict`                              | Словари изменяемы                                                            |
| `bytearray`                         | Изменяемый тип                                                               |
| `custom class` (если не хешируемый) | По умолчанию можно, но поведение зависит от реализации `__hash__` и `__eq__` |

Пример ошибки:

```python
my_dict = {[1, 2]: "invalid"}  # TypeError: unhashable type: 'list'
```

---

### Пример использования допустимого ключа:

```python
d = {
    "name": "Alice",
    42: "number",
    (1, 2): "tuple",
    frozenset([1, 2]): "frozen"
}
```

---

### Пользовательские типы в качестве ключей

Если вы создаёте свой класс и хотите использовать его в качестве ключа, он должен реализовать методы:

* `__hash__(self)`
* `__eq__(self, other)`

Пример:

```python
class Key:
    def __init__(self, value):
        self.value = value

    def __hash__(self):
        return hash(self.value)

    def __eq__(self, other):
        return isinstance(other, Key) and self.value == other.value

d = {Key("abc"): "custom object"}
```

---

## Может ли изменяться порядок ключей в словаре?

В современных версиях Python (начиная с **Python 3.7**) **порядок ключей в словаре сохраняется** в том порядке, в котором они были добавлены. Это означает, что если вы вставляете ключи в определённой последовательности, при итерировании по словарю они будут возвращены в этом же порядке.

---

### Пример в Python 3.7+:

```python
data = {}
data["a"] = 1
data["b"] = 2
data["c"] = 3

for key in data:
    print(key)
```

**Результат:**

```
a
b
c
```

Порядок ключей соответствует порядку вставки.

---

### Что влияет на порядок:

* **Добавление новых ключей**: они добавляются в конец словаря.
* **Удаление и повторное добавление ключей**: удалённый ключ теряет своё положение. Повторно добавленный ключ будет в конце.
* **Изменение значения существующего ключа**: не влияет на порядок.

---

## Какая алгоритмическая сложность у получения значения по ключу из словаря?

Получение значения по ключу из словаря в Python имеет **амортизированную временную сложность O(1)** (постоянное время).

---

### Почему O(1)?

Словарь в Python реализован как **хеш-таблица**, где:

* Ключ преобразуется в хеш с помощью встроенной функции `hash()`.
* Хеш используется для определения позиции (индекса) в массиве (внутренней структуре словаря).
* Если по этому индексу сразу находится нужный ключ, значение возвращается немедленно — это и даёт сложность O(1).

---

### Возможные исключения:

Хотя в среднем доступ по ключу выполняется за O(1), в **крайних случаях** (при коллизиях) доступ может деградировать до **O(n)**, где `n` — количество элементов в словаре. Это происходит, если много разных ключей получают одинаковый хеш и попадают в одну "ячейку", образуя цепочку проверок.

Однако:

* Python использует **открытую адресацию** и **разрешение коллизий**, чтобы свести такие случаи к минимуму.
* Система динамически **перехеширует и расширяет** таблицу по мере роста, чтобы сохранить доступ эффективным.

---

## Кортеж может быть ключом словаря?

Да, **кортеж может быть ключом словаря в Python**, но при одном важном условии: **все элементы кортежа должны быть неизменяемыми (immutable)**.

---

### Почему это важно?

Словарь в Python реализован как **хеш-таблица**, а ключи словаря должны быть **хешируемыми** (то есть иметь стабильный `__hash__` и не изменяться после создания). Кортеж сам по себе является **неизменяемым типом данных**, но если он содержит внутри **изменяемые объекты**, то сам кортеж уже становится **нехешируемым**, и использовать его как ключ нельзя.

---

### Примеры

#### Разрешённый случай (все элементы неизменяемы):

```python
d = {
    (1, 2): "a",
    ("x", "y"): "b"
}
print(d[(1, 2)])  # вывод: "a"
```

#### Ошибка: кортеж содержит изменяемый элемент (например, список)

```python
key = (1, [2, 3])  # список внутри
d = {key: "value"}  # TypeError: unhashable type: 'list'
```

---

### Проверка:

```python
hash((1, 2, 3))           # работает
hash((1, [2, 3]))         # вызовет ошибку
```

---

## Какие магические методы должны быть реализованы в в классе, чтоб его можно было использовать в качестве ключа словаря?

Чтобы объект класса можно было использовать в качестве ключа словаря, он должен быть **хешируемым**. Для этого в классе должны быть корректно реализованы два магических метода:

---

### 1. `__hash__(self)`

Этот метод возвращает целое число — **хеш объекта**, который словарь использует для определения позиции в хеш-таблице.
Если `__hash__` не реализован (или возвращает `None`), объект считается **нехешируемым**, и попытка использовать его как ключ вызовет ошибку `TypeError`.

---

### 2. `__eq__(self, other)`

Этот метод определяет, **равны ли два объекта**. Он используется для проверки "а не совпадает ли ключ, который мы ищем, с уже существующим ключом".
Если два объекта имеют одинаковый хеш (`__hash__`), Python вызывает `__eq__`, чтобы сравнить их содержимое.

---

### Важно: согласованность `__hash__` и `__eq__`

Они должны быть **согласованы**, то есть:

* Если `a == b`, то должно быть `hash(a) == hash(b)`
* В противном случае словарь может работать неправильно: например, не найдет значение по ключу или перезапишет "не те" данные.

---

### Пример корректной реализации:

```python
class Person:
    def __init__(self, name, age):
        self.name = name
        self.age = age

    def __eq__(self, other):
        if not isinstance(other, Person):
            return False
        return self.name == other.name and self.age == other.age

    def __hash__(self):
        return hash((self.name, self.age))


p1 = Person("Alice", 30)
p2 = Person("Alice", 30)

d = {p1: "developer"}
print(d[p2])  # работает, потому что p1 == p2 и hash(p1) == hash(p2)
```

---

**Что произойдёт, если не реализовать `__hash__`?**

Если в классе определён `__eq__`, но **не определён `__hash__`**, то Python автоматически сделает `__hash__ = None`, и объект **станет нехешируемым**, даже если он технически неизменяем. Это делается для предотвращения потенциально некорректного поведения.

---

## Что такое контекстный менеджер?

**Контекстный менеджер** — это объект, который управляет **входом в контекст** и **выходом из него**, обеспечивая автоматическое выполнение некоторого кода **до** и **после** блока операций. Он используется, например, для **открытия и закрытия файлов**, **блокировок**, **подключений**, **транзакций**, **временных ресурсов** и т.д.

Контекстные менеджеры используются через конструкцию `with`:

```python
with open("file.txt", "r") as f:
    data = f.read()
```

В этом примере файл автоматически закрывается после выхода из блока `with`, даже если внутри произошла ошибка.

---

### Основные цели:

* Управление ресурсами (файлы, соединения, блокировки);
* Автоматическая очистка или закрытие;
* Обработка исключений в рамках блока;
* Повышение читаемости и надежности кода.

---

### Как работает под капотом?

Контекстный менеджер должен реализовать два магических метода:

#### 1. `__enter__(self)`

* Вызывается **при входе** в блок `with`.
* Возвращаемое значение передаётся в переменную после `as`.

#### 2. `__exit__(self, exc_type, exc_val, exc_tb)`

* Вызывается **при выходе** из блока.
* Аргументы описывают исключение, если оно произошло:

  * `exc_type` — тип исключения;
  * `exc_val` — значение исключения;
  * `exc_tb` — трассировка.

Если `__exit__` возвращает `True`, исключение подавляется, иначе — пробрасывается дальше.

---

### Пример собственного контекстного менеджера:

```python
class FileManager:
    def __init__(self, filename, mode):
        self.filename = filename
        self.mode = mode
        self.file = None

    def __enter__(self):
        print("Открытие файла")
        self.file = open(self.filename, self.mode)
        return self.file

    def __exit__(self, exc_type, exc_val, exc_tb):
        print("Закрытие файла")
        if self.file:
            self.file.close()

# Использование:
with FileManager("example.txt", "w") as f:
    f.write("Пример")
```

---

### Альтернатива: `contextlib`

Python предоставляет модуль `contextlib`, где можно создавать контекстные менеджеры с использованием декоратора `@contextmanager`:

```python
from contextlib import contextmanager

@contextmanager
def custom_context():
    print("Вход")
    yield "контекст"
    print("Выход")

with custom_context() as val:
    print(f"Работаем с {val}")
```

---

### Примеры встроенных контекстных менеджеров:

* `open(...)` — для файлов;
* `threading.Lock()` — блокировки;
* `decimal.localcontext()` — временные настройки округления;
* `sqlite3.connect(...)` — подключения к БД;
* `contextlib.suppress(Exception)` — подавление исключений.

---

## Что такое GIL?

**GIL (Global Interpreter Lock)** — это глобальная блокировка интерпретатора в реализации Python CPython, которая ограничивает выполнение байт-кода Python в один момент времени только одним потоком.

---

### Что это значит?

* В многопоточных программах на Python, даже если у вас несколько потоков (`threading.Thread`), **выполнение байт-кода Python происходит только в одном потоке одновременно**.
* Другими словами, GIL **сделан для упрощения управления памятью и внутренними структурами интерпретатора**, но при этом ограничивает параллелизм в многопоточных программах.

---

### Почему появился GIL?

* Python использует **счётчик ссылок** для управления памятью.
* Поддерживать правильное обновление этого счётчика в многопоточном окружении сложно и накладно.
* GIL обеспечивает **атомарность операций с памятью**, предотвращая одновременный доступ к внутренним структурам и снижая риск ошибок и повреждения данных.
* Это упрощает реализацию интерпретатора, снижая количество ошибок, связанных с конкуренцией потоков.

---

### Последствия GIL

* На многоядерных системах Python-потоки **не могут эффективно использовать все ядра для выполнения байт-кода одновременно**.
* CPU-интенсивные задачи не получают прироста производительности от многопоточности в CPython.
* Для операций, интенсивно использующих процессор, часто используют:

  * **Многопроцессность** (`multiprocessing`), где каждый процесс имеет свой интерпретатор и GIL.
  * Расширения на C или другие библиотеки, которые **освобождают GIL** при выполнении тяжёлых вычислений (например, NumPy).
* Ввод-вывод (I/O) операции не так сильно страдают из-за GIL, так как в периоды ожидания I/O GIL освобождается, позволяя другим потокам выполняться.

---

### Пример поведения

```python
import threading

def cpu_bound():
    count = 0
    for _ in range(10**7):
        count += 1

threads = []
for i in range(4):
    t = threading.Thread(target=cpu_bound)
    threads.append(t)
    t.start()

for t in threads:
    t.join()
```

Хотя создаётся 4 потока, из-за GIL ускорения по сравнению с однопоточным выполнением будет мало или не будет вовсе.

---

### Как обойти ограничения GIL?

* Использовать **многопроцессность**, где каждый процесс имеет свой независимый интерпретатор без общего GIL.
* Использовать библиотеки, которые **освобождают GIL** при выполнении тяжелых вычислений (C-расширения).
* Рассмотреть альтернативные реализации Python, которые не имеют GIL, например:

  * **Jython** (на базе Java)
  * **IronPython** (на базе .NET)
  * **PyPy STM** (экспериментальная поддержка)

Но у них свои ограничения и несовместимости.

---

## Чем модуль отличается от пакета?

В Python понятия **модуль** и **пакет** относятся к организации и структуре кода, но они имеют разные уровни и назначение.

---

### Модуль

* **Модуль** — это **отдельный файл с расширением `.py`**, содержащий код на Python: функции, классы, переменные, а также исполняемые инструкции.
* По сути, модуль — это **единица организации кода**, которую можно импортировать и использовать в других частях программы.
* Имя модуля соответствует имени файла без расширения.

#### Пример модуля

Файл `math_utils.py`:

```python
def add(a, b):
    return a + b

PI = 3.14159
```

Этот модуль можно импортировать в другом файле:

```python
import math_utils

print(math_utils.add(2, 3))
print(math_utils.PI)
```

---

### Пакет

* **Пакет** — это **специальная директория (папка)**, которая содержит один или несколько модулей или вложенных пакетов.
* Папка считается пакетом, если в ней есть файл `__init__.py` (в Python 3.3+ наличие этого файла не обязательно, но часто используется для совместимости).
* Пакеты позволяют **структурировать проект, организуя модули по иерархии папок**.
* Имя пакета используется при импорте для группировки модулей.

#### Пример структуры пакета

```
my_package/
    __init__.py
    module1.py
    module2.py
    sub_package/
        __init__.py
        module3.py
```

Импорт из пакета:

```python
from my_package import module1
from my_package.sub_package import module3
```

---

### Основные отличия

| Характеристика        | Модуль                         | Пакет                                       |
| --------------------- | ------------------------------ | ------------------------------------------- |
| Что это?              | Один файл `.py`                | Директория с модулями и/или пакетами        |
| Наличие `__init__.py` | Не требуется                   | Требуется (рекомендуется для совместимости) |
| Структура             | Плоская                        | Иерархическая                               |
| Использование         | Импортируется напрямую         | Импортируется с указанием пути              |
| Назначение            | Организация кода в одном файле | Организация и группировка множества модулей |

---









---

## Источники

https://t.me/data_interviews
